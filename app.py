# -*- coding: utf-8 -*-

# ================== ìë™ Table & Cox ë¶„ì„ê¸° (penalizer Auto-CV í¬í•¨) ==================

# í•„ìš” íŒ¨í‚¤ì§€: pandas, numpy, scipy, lifelines, openpyxl, xlrd, streamlit

# ì„¤ì¹˜: pip install -U pandas numpy scipy lifelines openpyxl xlrd streamlit



import streamlit as st

import pandas as pd

import numpy as np

from scipy import stats

import io

from lifelines import CoxPHFitter

from lifelines.exceptions import ConvergenceError

import matplotlib.pyplot as plt



# ----- í˜ì´ì§€ ì„¤ì • -----

st.set_page_config(page_title="ìë™ ë…¼ë¬¸ Table", layout="wide")



# ----- ê°„ë‹¨ ë¹„ë°€ë²ˆí˜¸ ë³´í˜¸ (ê¸°ë³¸: CRCR ë˜ëŠ” st.secrets['APP_PASSWORD']) -----

def _check_password():

Â  Â  def _password_entered():

Â  Â  Â  Â  target = st.secrets.get("APP_PASSWORD", "CRCR")

Â  Â  Â  Â  if st.session_state.get("_password_input", "") == str(target):

Â  Â  Â  Â  Â  Â  st.session_state["_pw_ok"] = True

Â  Â  Â  Â  Â  Â  st.session_state.pop("_password_input", None)

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  st.session_state["_pw_ok"] = False



Â  Â  if st.session_state.get("_pw_ok", False):

Â  Â  Â  Â  return True



Â  Â  st.sidebar.subheader("ğŸ” Access")

Â  Â  st.sidebar.text_input("Password", type="password", key="_password_input", on_change=_password_entered)

Â  Â  if st.session_state.get("_pw_ok") is False:

Â  Â  Â  Â  st.sidebar.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

Â  Â  st.stop()



# ë¹„ë°€ë²ˆí˜¸ ì²´í¬

_check_password()



st.title("ìë™ Table ìƒì„±ê¸°")



# -------------------- [ê³µí†µ ìœ í‹¸] --------------------

def format_p(p):

Â  Â  if p is None or (isinstance(p, float) and np.isnan(p)):

Â  Â  Â  Â  return "NA"

Â  Â  if p >= 0.999:

Â  Â  Â  Â  return "p > 0.99"

Â  Â  if p < 0.001:

Â  Â  Â  Â  return "<0.001"

Â  Â  return f"{p:.3f}"



def is_continuous(series, threshold=20):

Â  Â  try:

Â  Â  Â  Â  return (series.dtype.kind in "fi") and (series.nunique(dropna=True) > threshold)

Â  Â  except Exception:

Â  Â  Â  Â  return False



def ordered_levels(series):

Â  Â  vals = pd.Series(series.dropna().unique()).tolist()

Â  Â  numeric = []

Â  Â  non = []

Â  Â  for v in vals:

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  numeric.append((float(str(v)), v))

Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  non.append(str(v))

Â  Â  if len(numeric) == len(vals) and len(vals) > 0:

Â  Â  Â  Â  numeric.sort(key=lambda x: x[0])

Â  Â  Â  Â  return [v for _, v in numeric]

Â  Â  return sorted([str(v) for v in vals], key=lambda x: x)



def make_dummies(df_in, var, levels):

Â  Â  # "ë³€ìˆ˜=ìˆ˜ì¤€" ì´ë¦„ìœ¼ë¡œ ë”ë¯¸ ìƒì„± (drop_first â†’ ì²« ë ˆë²¨ì´ Reference)

Â  Â  cat = pd.Categorical(df_in[var].astype(str),

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â categories=[str(x) for x in levels],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â ordered=True)

Â  Â  dmy = pd.get_dummies(cat, prefix=var, prefix_sep="=", drop_first=True, dtype=float)

Â  Â  dmy.index = df_in.index

Â  Â  return dmy



def dummy_colname(var, level):

Â  Â  return f"{var}={str(level)}"



def drop_constant_cols(X):

Â  Â  keep = [c for c in X.columns if X[c].nunique(dropna=True) > 1]

Â  Â  return X[keep]



def drop_constant_predictors(X, time_col, event_col):Â  # === NEW: CVìš© (time/eventëŠ” í•­ìƒ ìœ ì§€)

Â  Â  pred_cols = [c for c in X.columns if c not in [time_col, event_col]]

Â  Â  keep = [c for c in pred_cols if X[c].nunique(dropna=True) > 1]

Â  Â  return X[[time_col, event_col] + keep]



def clean_time(s):

Â  Â  s = pd.to_numeric(s, errors="coerce")

Â  Â  s = s.replace([np.inf, -np.inf], np.nan)

Â  Â  return s



def ensure_binary_event(col, events, censored):

Â  Â  def _map(x):

Â  Â  Â  Â  if x in events: return 1

Â  Â  Â  Â  if x in censored: return 0

Â  Â  Â  Â  return np.nan

Â  Â  return col.apply(_map).astype(float)



# === NEW: penalizerë¥¼ CVë¡œ ì„ íƒ (C-index ìµœëŒ€í™”) ===

def select_penalizer_by_cv(X_all, time_col, event_col,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â grid=(0.0, 0.01, 0.05, 0.1, 0.2, 0.5),

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â k=5, seed=42):

Â  Â  """

Â  Â  X_all: duration, event, predictorsë¥¼ ëª¨ë‘ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„ (dropna/ìƒìˆ˜ì—´ ì œê±°ëœ ìƒíƒœ ê¶Œì¥)

Â  Â  ë°˜í™˜: best_penalizer(or None), {penalizer: mean_cindex}

Â  Â  """

Â  Â  if X_all.shape[0] < k + 2 or X_all[event_col].sum() < k:Â  # ë„ˆë¬´ ì‘ì€ ê²½ìš° ë°©ì–´

Â  Â  Â  Â  return None, {}



Â  Â  idx = X_all.index.to_numpy()

Â  Â  rng = np.random.default_rng(seed)

Â  Â  rng.shuffle(idx)

Â  Â  folds = np.array_split(idx, k)



Â  Â  scores = {}

Â  Â  for pen in grid:

Â  Â  Â  Â  cv_scores = []

Â  Â  Â  Â  for i in range(k):

Â  Â  Â  Â  Â  Â  test_idx = folds[i]

Â  Â  Â  Â  Â  Â  train_idx = np.concatenate([folds[j] for j in range(k) if j != i])



Â  Â  Â  Â  Â  Â  train = X_all.loc[train_idx].copy()

Â  Â  Â  Â  Â  Â  testÂ  = X_all.loc[test_idx].copy()



Â  Â  Â  Â  Â  Â  # í•™ìŠµì…‹ì—ì„œ ìƒìˆ˜ predictor ì œê±°, ì—´ ì¼ì¹˜ ë§ì¶”ê¸°

Â  Â  Â  Â  Â  Â  train = drop_constant_predictors(train, time_col, event_col)

Â  Â  Â  Â  Â  Â  testÂ  = test[train.columns]Â  # ê°™ì€ ì—´ ìˆœì„œ/êµ¬ì„± ìœ ì§€



Â  Â  Â  Â  Â  Â  # ìœ íš¨ì„± ì²´í¬

Â  Â  Â  Â  Â  Â  if train[event_col].sum() < 2 or test[event_col].sum() < 1:

Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  if train.shape[1] <= 2 or train.shape[0] < 5:

Â  Â  Â  Â  Â  Â  Â  Â  continue



Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  cph = CoxPHFitter(penalizer=pen)

Â  Â  Â  Â  Â  Â  Â  Â  cph.fit(train, duration_col=time_col, event_col=event_col)

Â  Â  Â  Â  Â  Â  Â  Â  # lifelinesì˜ ì ìˆ˜: concordance_index

Â  Â  Â  Â  Â  Â  Â  Â  s = cph.score(test, scoring_method="concordance_index")

Â  Â  Â  Â  Â  Â  Â  Â  s = float(s)

Â  Â  Â  Â  Â  Â  Â  Â  if np.isfinite(s):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cv_scores.append(s)

Â  Â  Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  Â  Â  continue



Â  Â  Â  Â  if cv_scores:

Â  Â  Â  Â  Â  Â  scores[pen] = float(np.mean(cv_scores))



Â  Â  if not scores:

Â  Â  Â  Â  return None, {}



Â  Â  # ìµœê³  C-index, ë™ì ì´ë©´ ë” ì‘ì€ penalizer ì„ íƒ

Â  Â  best_pen = sorted(scores.items(), key=lambda x: (-x[1], x[0]))[0][0]

Â  Â  return best_pen, scores



# -------------------- [íŒŒì¼ ì—…ë¡œë“œ] --------------------

uploaded_file = st.file_uploader("ì—‘ì…€/CSV ì—…ë¡œë“œ", type=['xls', 'xlsx', 'csv'])



df = None

sheetname = None



if uploaded_file:

Â  Â  name = uploaded_file.name.lower()

Â  Â  if name.endswith(".csv"):

Â  Â  Â  Â  df = pd.read_csv(uploaded_file)

Â  Â  elif name.endswith(".xlsx"):

Â  Â  Â  Â  xls = pd.ExcelFile(uploaded_file, engine="openpyxl")

Â  Â  Â  Â  sheetname = xls.sheet_names[0] if len(xls.sheet_names) == 1 else st.selectbox("ì‹œíŠ¸ ì„ íƒ", xls.sheet_names, index=0)

Â  Â  Â  Â  df = pd.read_excel(xls, sheet_name=sheetname, engine="openpyxl")

Â  Â  elif name.endswith(".xls"):

Â  Â  Â  Â  xls = pd.ExcelFile(uploaded_file, engine="xlrd")

Â  Â  Â  Â  sheetname = xls.sheet_names[0] if len(xls.sheet_names) == 1 else st.selectbox("ì‹œíŠ¸ ì„ íƒ", xls.sheet_names, index=0)

Â  Â  Â  Â  df = pd.read_excel(xls, sheet_name=sheetname, engine="xlrd")

Â  Â  else:

Â  Â  Â  Â  st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. csv/xls/xlsxë§Œ ì—…ë¡œë“œí•˜ì„¸ìš”.")

Â  Â  Â  Â  st.stop()



Â  Â  # ì»¬ëŸ¼ ê³µë°±/ê°œí–‰ ì œê±° & ì¤‘ë³µ ë°©ì§€

Â  Â  df.columns = pd.Index([str(c).strip() for c in df.columns]).map(lambda x: x.replace("\\n", " ").strip())

Â  Â  st.success(f"ì‹œíŠ¸ëª…: {sheetname if sheetname else ''}, ë°ì´í„° shape: {df.shape}")

Â  Â  st.dataframe(df.head())

Â  Â  st.session_state['df'] = df



# -------------------- [Table1: ì„œë¸Œ í•¨ìˆ˜] --------------------

def calc_subgroup_p(valid, group_col, var, val, group_values):

Â  Â  table = []

Â  Â  for g in group_values:

Â  Â  Â  Â  sub = valid[valid[group_col] == g][var]

Â  Â  Â  Â  count_val = (sub == val).sum()

Â  Â  Â  Â  count_else = (sub != val).sum()

Â  Â  Â  Â  table.append([count_val, count_else])

Â  Â  table = np.array(table)

Â  Â  try:

Â  Â  Â  Â  if table.shape == (2, 2):

Â  Â  Â  Â  Â  Â  _, p = stats.fisher_exact(table)

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  _, p, _, _ = stats.chi2_contingency(table)

Â  Â  except Exception:

Â  Â  Â  Â  p = np.nan

Â  Â  return format_p(p)



def analyze_table1_display(df, group_col, value_map, threshold=20):

Â  Â  result_rows = []

Â  Â  group_values = list(value_map.keys())

Â  Â  group_names = list(value_map.values())

Â  Â  group_n = {g: (df[group_col] == g).sum() for g in group_values}



Â  Â  for var in df.columns:

Â  Â  Â  Â  if var == group_col:Â 

Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  valid = df[df[group_col].isin(group_values)]

Â  Â  Â  Â  if valid[var].dropna().empty:

Â  Â  Â  Â  Â  Â  continue



Â  Â  Â  Â  # ì—°ì†í˜•

Â  Â  Â  Â  if is_continuous(valid[var], threshold=threshold):

Â  Â  Â  Â  Â  Â  row = {'Characteristic': var}

Â  Â  Â  Â  Â  Â  for g, g_name in zip(group_values, group_names):

Â  Â  Â  Â  Â  Â  Â  Â  sub = valid[valid[group_col] == g][var].dropna()

Â  Â  Â  Â  Â  Â  Â  Â  n = sub.shape[0]

Â  Â  Â  Â  Â  Â  Â  Â  if n > 0:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  med, q1, q3 = sub.median(), sub.quantile(0.25), sub.quantile(0.75)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mean, std = sub.mean(), sub.std()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  row[f"{g_name} (n={group_n[g]})"] = f"{med:.1f} [{q1:.1f}-{q3:.1f}]; {mean:.1f}Â±{std:.1f}"

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  row[f"{g_name} (n={group_n[g]})"] = "NA"

Â  Â  Â  Â  Â  Â  # ê²€ì •

Â  Â  Â  Â  Â  Â  p = np.nan; test_str = ""

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  arr = [valid[valid[group_col] == g][var].dropna() for g in group_values]

Â  Â  Â  Â  Â  Â  Â  Â  normal_flags = []

Â  Â  Â  Â  Â  Â  Â  Â  for vals in arr:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if len(vals) >= 3:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p_norm = stats.shapiro(vals)[1]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p_norm = 0

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  normal_flags.append(p_norm > 0.05)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  normal_flags.append(False)

Â  Â  Â  Â  Â  Â  Â  Â  if all(normal_flags):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if len(arr) == 2:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  _, p = stats.ttest_ind(arr[0], arr[1], nan_policy='omit'); test_str = "t-test"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif len(arr) > 2:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  _, p = stats.f_oneway(*arr); test_str = "ANOVA"

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if len(arr) == 2:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  _, p = stats.mannwhitneyu(arr[0], arr[1], alternative='two-sided'); test_str = "Mann-Whitney U"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif len(arr) > 2:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  _, p = stats.kruskal(*arr); test_str = "Kruskal-Wallis"

Â  Â  Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  Â  Â  p = np.nan; test_str = "NA"

Â  Â  Â  Â  Â  Â  row['Test'] = test_str

Â  Â  Â  Â  Â  Â  row['p value'] = format_p(p)

Â  Â  Â  Â  Â  Â  row['sub_p'] = ""

Â  Â  Â  Â  Â  Â  result_rows.append(row)

Â  Â  Â  Â  Â  Â  continue



Â  Â  Â  Â  # ë²”ì£¼í˜•

Â  Â  Â  Â  vlist = pd.Series(valid[var].dropna().unique())

Â  Â  Â  Â  if len(vlist) >= threshold:

Â  Â  Â  Â  Â  Â  row = {'Characteristic': var}

Â  Â  Â  Â  Â  Â  for g, g_name in zip(group_values, group_names):

Â  Â  Â  Â  Â  Â  Â  Â  sub = valid[valid[group_col] == g][var]

Â  Â  Â  Â  Â  Â  Â  Â  vc = sub.value_counts()

Â  Â  Â  Â  Â  Â  Â  Â  valstr = "; ".join([f"{idx}={cnt}({(cnt/group_n[g]*100):.0f}%)" for idx, cnt in vc.items()]) if group_n[g] > 0 else "NA"

Â  Â  Â  Â  Â  Â  Â  Â  row[f"{g_name} (n={group_n[g]})"] = valstr if len(vc) > 0 else "NA"

Â  Â  Â  Â  Â  Â  # ì „ì²´ p

Â  Â  Â  Â  Â  Â  p = np.nan; test_str = ""

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  ct = pd.crosstab(valid[group_col], valid[var])

Â  Â  Â  Â  Â  Â  Â  Â  if ct.shape == (2, 2):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  _, p = stats.fisher_exact(ct); test_str = "Fisher"

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  _, p, _, _ = stats.chi2_contingency(ct); test_str = "Chi-square"

Â  Â  Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  Â  Â  p = np.nan; test_str = "NA"

Â  Â  Â  Â  Â  Â  row['Test'] = test_str

Â  Â  Â  Â  Â  Â  row['p value'] = format_p(p)

Â  Â  Â  Â  Â  Â  row['sub_p'] = ""

Â  Â  Â  Â  Â  Â  result_rows.append(row)

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  # í—¤ë” í–‰

Â  Â  Â  Â  Â  Â  p = np.nan; test_str = ""

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  ct = pd.crosstab(valid[group_col], valid[var])

Â  Â  Â  Â  Â  Â  Â  Â  if ct.shape == (2, 2):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  _, p = stats.fisher_exact(ct); test_str = "Fisher"

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  _, p, _, _ = stats.chi2_contingency(ct); test_str = "Chi-square"

Â  Â  Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  Â  Â  p = np.nan; test_str = "NA"

Â  Â  Â  Â  Â  Â  first_row = {'Characteristic': var}

Â  Â  Â  Â  Â  Â  for g, g_name in zip(group_values, group_names):

Â  Â  Â  Â  Â  Â  Â  Â  first_row[f"{g_name} (n={group_n[g]})"] = ""

Â  Â  Â  Â  Â  Â  first_row['Test'] = test_str

Â  Â  Â  Â  Â  Â  first_row['p value'] = format_p(p)

Â  Â  Â  Â  Â  Â  first_row['sub_p'] = ""

Â  Â  Â  Â  Â  Â  result_rows.append(first_row)



Â  Â  Â  Â  Â  Â  for val in vlist:

Â  Â  Â  Â  Â  Â  Â  Â  row = {'Characteristic': f"Â  {val}"}

Â  Â  Â  Â  Â  Â  Â  Â  for g, g_name in zip(group_values, group_names):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cnt = valid[(valid[group_col] == g) & (valid[var] == val)].shape[0]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  percent = (cnt/group_n[g]*100) if group_n[g] > 0 else 0

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  row[f"{g_name} (n={group_n[g]})"] = f"{cnt}({percent:.0f}%)"

Â  Â  Â  Â  Â  Â  Â  Â  row['Test'] = ""

Â  Â  Â  Â  Â  Â  Â  Â  row['p value'] = ""

Â  Â  Â  Â  Â  Â  Â  Â  row['sub_p'] = calc_subgroup_p(valid, group_col, var, val, group_values)

Â  Â  Â  Â  Â  Â  Â  Â  result_rows.append(row)



Â  Â  return pd.DataFrame(result_rows)



# -------------------- [UI: Tab êµ¬ì„±] --------------------

if 'df' in st.session_state:

Â  Â  df = st.session_state['df']



if 'df' not in locals():

Â  Â  df = None



if df is not None:

Â  Â  tab1, tab2, tab3 = st.tabs(["ğŸ“Š Table1 ìë™í™”", "ğŸŸ¦ Cox íšŒê·€ë¶„ì„ (Univariate/Multivariate)", "ğŸŸ§ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ (Risk Factors)"])



Â  Â  # ===== TAB1 =====

Â  Â  with tab1:

Â  Â  Â  Â  st.header("2ë‹¨ê³„: Table ìë™í™” (ê°’ ì§ì ‘ ì„ íƒ/ë¼ë²¨/í–‰ë¶„ë¦¬/ìš”ì•½ ì§€ì›)")

Â  Â  Â  Â  st.info(

Â  Â  Â  Â  Â  Â  "ğŸ“Œ ì—°ì†í˜•/ë²”ì£¼í˜• ìë™ ë¶„ë¥˜: ê³ ìœ ê°’ 20ê°œ ì´ˆê³¼ â†’ ì—°ì†í˜•, ì´í•˜ëŠ” ë²”ì£¼í˜•.\\n"

Â  Â  Â  Â  Â  Â  "ê²°ê³¼ê°€ ìƒì‹ê³¼ ë‹¤ë¥´ë©´ ì§ì ‘ ë³€ìˆ˜ íƒ€ì…ì„ í™•ì¸í•˜ì„¸ìš”."

Â  Â  Â  Â  )



Â  Â  Â  Â  candidate_cols = list(df.columns)

Â  Â  Â  Â  group_col = st.selectbox("ë¶„ì„í•  ê·¸ë£¹ ë³€ìˆ˜ëª…ì„ ì„ íƒí•˜ì„¸ìš”", options=candidate_cols, key='group_col')

Â  Â  Â  Â  value_map = {}



Â  Â  Â  Â  if group_col and group_col in df.columns:

Â  Â  Â  Â  Â  Â  unique_vals = list(df[group_col].dropna().unique())

Â  Â  Â  Â  Â  Â  selected_vals = st.multiselect("ë¶„ì„í•  ê°’ì„ ì„ íƒí•˜ì„¸ìš”", unique_vals, default=unique_vals[:2], key='group_values')

Â  Â  Â  Â  Â  Â  if selected_vals:

Â  Â  Â  Â  Â  Â  Â  Â  col1, col2 = st.columns([2,6])

Â  Â  Â  Â  Â  Â  Â  Â  for val in selected_vals:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with col1:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"ê°’: {val}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with col2:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  label = st.text_input(f"í•´ë‹¹ ê°’ì˜ í‘œì‹œ ë¼ë²¨", value=str(val), key=f'label_{val}')

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  value_map[val] = label



Â  Â  Â  Â  Â  Â  Â  Â  if st.button("ë…¼ë¬¸ Table1 ìƒì„±", key='table1_generate'):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  target_df = df.dropna(subset=[group_col])

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = analyze_table1_display(target_df, group_col, value_map, threshold=20)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(result)



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  output = io.BytesIO()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with pd.ExcelWriter(output) as writer:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result.to_excel(writer, index=False)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.download_button(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  label="Table1 ì—‘ì…€ë¡œ ì €ì¥",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data=output.getvalue(),

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_name="ë…¼ë¬¸ìš©_Table1.xlsx",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  st.warning("ë¶„ì„í•  ê°’ì„ ìµœì†Œ 1ê°œ ì´ìƒ ì„ íƒí•´ ì£¼ì„¸ìš”.")

Â  Â  Â  Â  elif group_col:

Â  Â  Â  Â  Â  Â  st.error("í•´ë‹¹ ë³€ìˆ˜ëª…ì´ ë°ì´í„°ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")



Â  Â  # ===== TAB2 =====

Â  Â  with tab2:

Â  Â  Â  Â  st.header("ë…¼ë¬¸ Table: Factor / Subgroup / HR(95%CI) / p-value (Univariate & Multivariate)")



Â  Â  Â  Â  time_col = st.selectbox("ìƒì¡´ê¸°ê°„ ë³€ìˆ˜(time)", df.columns, key="cox_time_col")

Â  Â  Â  Â  event_col = st.selectbox("Event ë³€ìˆ˜(ì˜ˆ: 0=ìƒì¡´, 1=ì‚¬ë§ ë“±)", df.columns, key="cox_event_col")



Â  Â  Â  Â  temp_df = df.copy()

Â  Â  Â  Â  if event_col:

Â  Â  Â  Â  Â  Â  unique_events = list(df[event_col].dropna().unique())

Â  Â  Â  Â  Â  Â  st.write(f"ì´ ë³€ìˆ˜ì˜ ì‹¤ì œ ê°’: {unique_events}")

Â  Â  Â  Â  Â  Â  selected_event = st.multiselect("ì´ë²¤íŠ¸(ì‚¬ê±´) ê°’", unique_events, key='selected_event_val')

Â  Â  Â  Â  Â  Â  selected_censored = st.multiselect("ìƒì¡´/ê´€ì°°ì¢…ê²°(censored) ê°’", unique_events, key='selected_censored_val')

Â  Â  Â  Â  Â  Â  st.caption("â€» ì‚¬ê±´ê°’ê³¼ ê²€ì—´ê°’ì€ ì„œë¡œ ê²¹ì¹˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.")

Â  Â  Â  Â  Â  Â  temp_df["__event_for_cox"] = ensure_binary_event(temp_df[event_col], set(selected_event), set(selected_censored))

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  temp_df["__event_for_cox"] = np.nan



Â  Â  Â  Â  candidate_vars = [c for c in df.columns if c not in [time_col, event_col]]

Â  Â  Â  Â  variables = st.multiselect("ë¶„ì„ í›„ë³´ ë³€ìˆ˜ ì„ íƒ", candidate_vars, key="cox_variables")



Â  Â  Â  Â  c1, c2, c3, c4 = st.columns([1,1,1,1])

Â  Â  Â  Â  with c1:

Â  Â  Â  Â  Â  Â  p_enter = st.number_input("ë‹¤ë³€ëŸ‰ í¬í•¨ ê¸°ì¤€ p-enter (â‰¤)", min_value=0.001, max_value=1.0, value=0.05, step=0.01)

Â  Â  Â  Â  with c2:

Â  Â  Â  Â  Â  Â  max_levels = st.number_input("ë²”ì£¼í˜• íŒì • ìµœëŒ€ ê³ ìœ ê°’", min_value=2, max_value=50, value=10, step=1)

Â  Â  Â  Â  with c3:

Â  Â  Â  Â  Â  Â  auto_penal = st.checkbox("penalizer ìë™ ì„ íƒ (CV, C-index)", value=False)Â  # === NEW

Â  Â  Â  Â  with c4:

Â  Â  Â  Â  Â  Â  cv_k = st.number_input("CV folds (K)", min_value=3, max_value=10, value=5, step=1, disabled=not auto_penal)Â  # === NEW



Â  Â  Â  Â  penal_col = st.columns(1)[0]

Â  Â  Â  Â  penalizer = penal_col.number_input("penalizer (ìˆ˜ë ´ ì•ˆì •í™”)", min_value=0.0, max_value=5.0, value=0.1, step=0.1, disabled=auto_penal)



Â  Â  Â  Â  def basic_clean(df_in, time_col):

Â  Â  Â  Â  Â  Â  out = df_in.copy()

Â  Â  Â  Â  Â  Â  out[time_col] = clean_time(out[time_col])

Â  Â  Â  Â  Â  Â  out = out[out[time_col] > 0]

Â  Â  Â  Â  Â  Â  out = out.replace([np.inf, -np.inf], np.nan)

Â  Â  Â  Â  Â  Â  return out



Â  Â  Â  Â  if st.button("ë¶„ì„ ì‹¤í–‰"):

Â  Â  Â  Â  Â  Â  # í•„ìˆ˜ ê²€ì¦

Â  Â  Â  Â  Â  Â  if not selected_event or not selected_censored:

Â  Â  Â  Â  Â  Â  Â  Â  st.error("ì‚¬ê±´ê°’ê³¼ ê²€ì—´ê°’ì„ ê°ê° ìµœì†Œ 1ê°œ ì´ìƒ ì„ íƒí•˜ì„¸ìš”.")

Â  Â  Â  Â  Â  Â  Â  Â  st.stop()

Â  Â  Â  Â  Â  Â  if set(selected_event) & set(selected_censored):

Â  Â  Â  Â  Â  Â  Â  Â  st.error("ì‚¬ê±´ê°’ê³¼ ê²€ì—´ê°’ì´ ê²¹ì¹©ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•˜ì„¸ìš”.")

Â  Â  Â  Â  Â  Â  Â  Â  st.stop()



Â  Â  Â  Â  Â  Â  temp_df2 = basic_clean(temp_df, time_col).dropna(subset=[time_col, "__event_for_cox"])

Â  Â  Â  Â  Â  Â  n_events = int(temp_df2["__event_for_cox"].sum())

Â  Â  Â  Â  Â  Â  n_total = temp_df2.shape[0]

Â  Â  Â  Â  Â  Â  st.info(f"ì´ ê´€ì¸¡ì¹˜: {n_total}, ì´ë²¤íŠ¸ ìˆ˜: {n_events}")

Â  Â  Â  Â  Â  Â  if n_events < 5:

Â  Â  Â  Â  Â  Â  Â  Â  st.warning("ì´ë²¤íŠ¸ ìˆ˜ê°€ <5ë¡œ ë§¤ìš° ì ìŠµë‹ˆë‹¤. ì¶”ì •ì´ ë¶ˆì•ˆì •í•˜ê±°ë‚˜ ëª¨ë¸ì´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")



Â  Â  Â  Â  Â  Â  # ---------- 1) Univariate ----------

Â  Â  Â  Â  Â  Â  uni_sum_dict = {}

Â  Â  Â  Â  Â  Â  uni_na_vars = []

Â  Â  Â  Â  Â  Â  cat_info = {}



Â  Â  Â  Â  Â  Â  for var in variables:

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dat_raw = temp_df2[[time_col, "__event_for_cox", var]].copy()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dat_raw = dat_raw.dropna(subset=[var])

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if dat_raw.empty:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  uni_na_vars.append(var); continue



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (dat_raw[var].dtype == "object") or (dat_raw[var].nunique(dropna=True) <= max_levels):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lvls = ordered_levels(dat_raw[var])

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cat_info[var] = {"levels": lvls, "ref": lvls[0]}

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dmy = make_dummies(dat_raw, var, lvls)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dat = pd.concat([dat_raw[[time_col, "__event_for_cox"]], dmy], axis=1)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cat_info[var] = {"levels": None, "ref": None}

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dat = dat_raw[[time_col, "__event_for_cox", var]].copy()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dat[var] = pd.to_numeric(dat[var], errors="coerce")



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dat = dat.dropna()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dat = drop_constant_cols(dat)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (dat.shape[0] < 3) or (dat["__event_for_cox"].sum() < 1) or (dat.shape[1] <= 2):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  uni_na_vars.append(var); continue



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cph = CoxPHFitter(penalizer=penalizer)Â  # UnivariateëŠ” ì…ë ¥ penalizer ì‚¬ìš©

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cph.fit(dat, duration_col=time_col, event_col="__event_for_cox")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  uni_sum_dict[var] = cph.summary.copy()

Â  Â  Â  Â  Â  Â  Â  Â  except ConvergenceError:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  uni_na_vars.append(var)

Â  Â  Â  Â  Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  uni_na_vars.append(var)



Â  Â  Â  Â  Â  Â  # ë³€ìˆ˜ì„ íƒ

Â  Â  Â  Â  Â  Â  univariate_pvals = {}

Â  Â  Â  Â  Â  Â  for var, summ in uni_sum_dict.items():

Â  Â  Â  Â  Â  Â  Â  Â  if cat_info[var]["levels"] is None:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if var in summ.index:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  univariate_pvals[var] = float(summ.loc[var, "p"])

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p_min = None

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for _, row in summ.iterrows():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p = float(row["p"])

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p_min = p if p_min is None else min(p_min, p)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if p_min is not None:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  univariate_pvals[var] = p_min



Â  Â  Â  Â  Â  Â  selected_vars = [v for v, p in univariate_pvals.items() if p <= p_enter]

Â  Â  Â  Â  Â  Â  st.write(f"ë‹¤ë³€ëŸ‰ í›„ë³´ ë³€ìˆ˜(â‰¤ {p_enter:.3f}): {selected_vars if selected_vars else 'ì—†ìŒ'}")



Â  Â  Â  Â  Â  Â  # ---------- 2) Multivariate ----------

Â  Â  Â  Â  Â  Â  multi_sum = None

Â  Â  Â  Â  Â  Â  multi_na_vars = []

Â  Â  Â  Â  Â  Â  chosen_penalizer = penalizerÂ  # ê¸°ë³¸ê°’



Â  Â  Â  Â  Â  Â  if len(selected_vars) >= 1:

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dat_base = temp_df2[[time_col, "__event_for_cox"]].copy()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  X_list = []

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for var in selected_vars:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if cat_info.get(var, {}).get("levels") is None:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  xi = pd.to_numeric(temp_df2[var], errors="coerce").to_frame(var)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lvls = cat_info[var]["levels"]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  xi = make_dummies(temp_df2[[var]], var, lvls)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  X_list.append(xi)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  X_all = pd.concat([dat_base] + X_list, axis=1).dropna()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  X_all = drop_constant_predictors(X_all, time_col, "__event_for_cox")



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # === NEW: Auto-CVë¡œ penalizer ì„ íƒ ===

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if auto_penal and X_all["__event_for_cox"].sum() >= cv_k:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pen_grid = (0.0, 0.01, 0.05, 0.1, 0.2, 0.5)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  best_pen, pen_scores = select_penalizer_by_cv(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  X_all, time_col, "__event_for_cox",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  grid=pen_grid, k=int(cv_k), seed=42

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if best_pen is not None:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chosen_penalizer = float(best_pen)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"Auto-CV ì„ íƒ penalizer = {chosen_penalizer} (í‰ê·  C-index ê¸°ì¤€)")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.caption(f"Grid ì„±ëŠ¥: { {k: round(v,4) for k,v in pen_scores.items()} }")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning("CVë¡œ penalizerë¥¼ ê²°ì •í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì…ë ¥ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (X_all.shape[0] >= 3) and (X_all["__event_for_cox"].sum() >= 1) and (X_all.shape[1] > 2):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cph_multi = CoxPHFitter(penalizer=chosen_penalizer)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cph_multi.fit(X_all, duration_col=time_col, event_col="__event_for_cox")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  multi_sum = cph_multi.summary.copy()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  multi_na_vars = selected_vars

Â  Â  Â  Â  Â  Â  Â  Â  except ConvergenceError:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  multi_na_vars = selected_vars

Â  Â  Â  Â  Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  multi_na_vars = selected_vars



Â  Â  Â  Â  Â  Â  # ---------- 3) ì¶œë ¥ í…Œì´ë¸” ----------

Â  Â  Â  Â  Â  Â  rows = []

Â  Â  Â  Â  Â  Â  for var in variables:

Â  Â  Â  Â  Â  Â  Â  Â  rows.append({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Factor": var, "Subgroup": "",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Univariate analysis HR (95% CI)": "", "Univariate analysis p-Value": "",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Multivariate analysis HR (95% CI)": "", "Multivariate analysis p-Value": ""

Â  Â  Â  Â  Â  Â  Â  Â  })



Â  Â  Â  Â  Â  Â  Â  Â  # ì™„ì „ ì‹¤íŒ¨

Â  Â  Â  Â  Â  Â  Â  Â  if (var in uni_na_vars) and ((multi_sum is None) or (var in multi_na_vars)):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rows.append({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Factor": "", "Subgroup": "(insufficient / skipped)",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Univariate analysis HR (95% CI)": "NA", "Univariate analysis p-Value": "NA",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Multivariate analysis HR (95% CI)": "NA", "Multivariate analysis p-Value": "NA"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue



Â  Â  Â  Â  Â  Â  Â  Â  # ë²”ì£¼í˜•

Â  Â  Â  Â  Â  Â  Â  Â  if cat_info.get(var, {}).get("levels") is not None:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lvls = cat_info[var]["levels"]; ref = cat_info[var]["ref"]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rows.append({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Factor": "", "Subgroup": f"{ref} (Reference)",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Univariate analysis HR (95% CI)": "Ref.", "Univariate analysis p-Value": "",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Multivariate analysis HR (95% CI)": "Ref.", "Multivariate analysis p-Value": ""

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for lv in lvls[1:]:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  colname = dummy_colname(var, lv)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Uni

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (var in uni_na_vars) or (var not in uni_sum_dict) or (colname not in uni_sum_dict[var].index):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hr_uni, p_uni = "NA", "NA"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  r = uni_sum_dict[var].loc[colname]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hr_uni = f"{r['exp(coef)']:.3f} ({r['exp(coef) lower 95%']:.3f}-{r['exp(coef) upper 95%']:.3f})"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p_uni = format_p(float(r['p']))

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Multi

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (multi_sum is None) or (var in multi_na_vars) or (colname not in (multi_sum.index if multi_sum is not None else [])):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hr_multi, p_multi = "NA", "NA"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  r = multi_sum.loc[colname]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hr_multi = f"{r['exp(coef)']:.3f} ({r['exp(coef) lower 95%']:.3f}-{r['exp(coef) upper 95%']:.3f})"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p_multi = format_p(float(r['p']))

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rows.append({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Factor": "", "Subgroup": str(lv),

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Univariate analysis HR (95% CI)": hr_uni, "Univariate analysis p-Value": p_uni,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Multivariate analysis HR (95% CI)": hr_multi, "Multivariate analysis p-Value": p_multi

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  })



Â  Â  Â  Â  Â  Â  Â  Â  # ì—°ì†í˜•

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (var not in uni_sum_dict) or (var in uni_na_vars) or (var not in uni_sum_dict[var].index if var in uni_sum_dict else True):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hr_uni, p_uni = "NA", "NA"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  r = uni_sum_dict[var].loc[var]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hr_uni = f"{r['exp(coef)']:.3f} ({r['exp(coef) lower 95%']:.3f}-{r['exp(coef) upper 95%']:.3f})"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p_uni = format_p(float(r['p']))



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (multi_sum is None) or (var in multi_na_vars) or (var not in (multi_sum.index if multi_sum is not None else [])):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hr_multi, p_multi = "NA", "NA"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  r = multi_sum.loc[var]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hr_multi = f"{r['exp(coef)']:.3f} ({r['exp(coef) lower 95%']:.3f}-{r['exp(coef) upper 95%']:.3f})"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p_multi = format_p(float(r['p']))



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rows.append({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Factor": "", "Subgroup": "",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Univariate analysis HR (95% CI)": hr_uni, "Univariate analysis p-Value": p_uni,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Multivariate analysis HR (95% CI)": hr_multi, "Multivariate analysis p-Value": p_multi

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  })



Â  Â  Â  Â  Â  Â  result_table = pd.DataFrame(rows)

Â  Â  Â  Â  Â  Â  st.write("**ë…¼ë¬¸ ì œì¶œìš© í…Œì´ë¸” (Univariate/Multivariate ë³‘ë ¬, Reference, Factor/ìˆ˜ì¤€êµ¬ì¡°)**")

Â  Â  Â  Â  Â  Â  if auto_penal and len(selected_vars) >= 1:

Â  Â  Â  Â  Â  Â  Â  Â  st.caption(f"*ë‹¤ë³€ëŸ‰ ìµœì¢… penalizer: {chosen_penalizer}*")

Â  Â  Â  Â  Â  Â  st.dataframe(result_table)



Â  Â  Â  Â  Â  Â  output = io.BytesIO()

Â  Â  Â  Â  Â  Â  with pd.ExcelWriter(output) as writer:

Â  Â  Â  Â  Â  Â  Â  Â  result_table.to_excel(writer, index=False)

Â  Â  Â  Â  Â  Â  st.download_button(

Â  Â  Â  Â  Â  Â  Â  Â  label="Cox ê²°ê³¼ ì—‘ì…€ë¡œ ì €ì¥",

Â  Â  Â  Â  Â  Â  Â  Â  data=output.getvalue(),

Â  Â  Â  Â  Â  Â  Â  Â  file_name="Cox_Regression_Results_Table.xlsx",

Â  Â  Â  Â  Â  Â  Â  Â  mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"

Â  Â  Â  Â  Â  Â  )
       # ===== TAB3: Logistic Regression (NEW) =====
    with tab3:
        # --- ì§€ì—­ ì„í¬íŠ¸: ìƒë‹¨ ì„í¬íŠ¸ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤ ---
        import statsmodels.api as sm
        from statsmodels.discrete.discrete_model import PerfectSeparationError
    
        st.header("ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ (Risk Factor Analysis)")
        st.info("ì¢…ì†ë³€ìˆ˜ì˜ íŠ¹ì • ê°’ì„ ì‚¬ê±´(1)ê³¼ ê¸°ì¤€(0)ìœ¼ë¡œ ì •ì˜í•˜ì—¬ ìœ„í—˜ì¸ìë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. Cox í…Œì´ë¸”ê³¼ ë™ì¼í•œ ë…¼ë¬¸ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë§Œë“­ë‹ˆë‹¤.")
    
        # ---- Hosmerâ€“Lemeshow ê²€ì • í•¨ìˆ˜ (íƒ­ ë‚´ë¶€ì— ì •ì˜; ì „ì—­ ì˜¤ì—¼ ë°©ì§€) ----
        def calculate_hosmer_lemeshow(y_true, y_prob, g=10):
            """Return (HL_stat, p_value, error_message). g=number of bins."""
            import numpy as np
            from scipy import stats as _stats
            try:
                y_true = np.asarray(y_true, dtype=float)
                y_prob = np.asarray(y_prob, dtype=float)
                mask = np.isfinite(y_true) & np.isfinite(y_prob)
                y_true, y_prob = y_true[mask], y_prob[mask]
                if y_true.size < 20:
                    return None, None, "í‘œë³¸ ìˆ˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤(<20)."
                # deciles by predicted probability
                try:
                    bins = pd.qcut(y_prob, q=min(g, max(2, np.unique(y_prob).size)), duplicates='drop')
                except Exception:
                    q = min(g, 5)
                    bins = pd.qcut(y_prob, q=q, duplicates='drop')
                df_hl = pd.DataFrame({'y': y_true, 'p': y_prob, 'bin': bins})
                grp = df_hl.groupby('bin', observed=True)
                O1 = grp['y'].sum()
                N = grp['y'].count()
                Pbar = grp['p'].mean()
                E1 = N * Pbar
                O0 = N - O1
                E0 = N - E1
                # add tiny epsilon to avoid zero division
                eps = 1e-12
                HL = (((O1 - E1)**2)/(E1 + eps) + ((O0 - E0)**2)/(E0 + eps)).sum()
                df = max(1, len(grp) - 2)
                p = 1 - _stats.chi2.cdf(HL, df)
                return float(HL), float(p), None
            except Exception as e:
                return None, None, f"HL ê³„ì‚° ì˜¤ë¥˜: {type(e).__name__}: {e}"
    
        # ---- UI ----
        dep_var = st.selectbox("ì¢…ì† ë³€ìˆ˜ (Y) ì„ íƒ", df.columns, key="logistic_dep_var")
    
        if dep_var:
            unique_outcomes = list(df[dep_var].dropna().unique())
            st.write(f"'{dep_var}' ë³€ìˆ˜ì˜ ê³ ìœ  ê°’: {unique_outcomes}")
            event_values = st.multiselect("ì‚¬ê±´(Event=1)ì— í•´ë‹¹í•˜ëŠ” ê°’ ì„ íƒ", unique_outcomes, key="logistic_event")
            control_values = st.multiselect("ê¸°ì¤€(Control=0)ì— í•´ë‹¹í•˜ëŠ” ê°’ ì„ íƒ", unique_outcomes, key="logistic_control")
            st.caption("â€» ì‚¬ê±´ ê°’ê³¼ ê¸°ì¤€ ê°’ì€ ì„œë¡œ ê²¹ì¹˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.")
        else:
            event_values, control_values = [], []
    
        indep_vars = st.multiselect(
            "ë…ë¦½ ë³€ìˆ˜ (X) ì„ íƒ (ìœ„í—˜ì¸ì í›„ë³´)", [c for c in df.columns if c != dep_var], key="logistic_indep_vars"
        )
    
        c1_log, c2_log = st.columns(2)
        p_enter_logistic = c1_log.number_input(
            "ë‹¤ë³€ëŸ‰ í¬í•¨ ê¸°ì¤€ p-enter (â‰¤)", min_value=0.001, max_value=1.0, value=0.05, step=0.01, key='logistic_p_enter'
        )
        max_levels_logistic = c2_log.number_input(
            "ë²”ì£¼í˜• íŒì • ìµœëŒ€ ê³ ìœ ê°’", min_value=2, max_value=50, value=10, step=1, key="logistic_max_levels"
        )
    
        if st.button("ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ ì‹¤í–‰", key="run_logistic"):
            # ---- í•„ìˆ˜ ê²€ì¦ ----
            if not dep_var or not event_values or not control_values or not indep_vars:
                st.error("ì¢…ì† ë³€ìˆ˜, ì‚¬ê±´ ê°’, ê¸°ì¤€ ê°’, ë…ë¦½ ë³€ìˆ˜ë¥¼ ëª¨ë‘ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
                st.stop()
            if set(event_values) & set(control_values):
                st.error("ì‚¬ê±´ ê°’ê³¼ ê¸°ì¤€ ê°’ì´ ê²¹ì¹©ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•˜ì„¸ìš”.")
                st.stop()
    
            try:
                with st.spinner('ë¶„ì„ì„ ìˆ˜í–‰ ì¤‘ì…ë‹ˆë‹¤...'):
                    # ---- ë°ì´í„° ì¤€ë¹„: Y ì´ì§„í™” ----
                    cols_to_use = [dep_var] + indep_vars
                    df_model = df[cols_to_use].copy()
                    df_model['__y_bin'] = ensure_binary_event(df_model[dep_var], set(event_values), set(control_values))
                    df_model.dropna(subset=['__y_bin'], inplace=True)
                    df_model['__y_bin'] = df_model['__y_bin'].astype(int)
    
                    # ---- X ì „ì²˜ë¦¬ (ë²”ì£¼í˜• ë”ë¯¸, ì—°ì†í˜• ìˆ«ìí™”) ----
                    X_list, cat_info_logistic = [], {}
                    for var in indep_vars:
                        # ë²”ì£¼í˜• íŒì •: ê³ ìœ ê°’ â‰¤ max_levels OR dtype=object â†’ ë²”ì£¼í˜•
                        if (df_model[var].dtype == 'object') or (df_model[var].nunique(dropna=True) <= max_levels_logistic):
                            levels = ordered_levels(df_model[var])
                            cat_info_logistic[var] = {"levels": levels, "ref": levels[0]}
                            X_list.append(make_dummies(df_model[[var]], var, levels))  # drop_first=True ë‚´ë¶€ êµ¬í˜„
                        else:
                            cat_info_logistic[var] = {"levels": None, "ref": None}
                            X_list.append(pd.to_numeric(df_model[var], errors='coerce').rename(var))
    
                    if not X_list:
                        st.error("ìœ íš¨í•œ ë…ë¦½ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        st.stop()
    
                    X_processed = pd.concat(X_list, axis=1)
                    # ìƒìˆ˜ ì—´ ì œê±°(ëª¨ë“  ê°’ ë™ì¼) í›„, ìƒìˆ˜ë¥¼ ì¶”ê°€ (constëŠ” ìœ ì§€)
                    X_processed = X_processed.loc[:, X_processed.nunique(dropna=True) > 1]
                    model_data = pd.concat([df_model['__y_bin'], X_processed], axis=1).dropna()
                    y_final = model_data['__y_bin']
                    X_final = model_data.drop(columns=['__y_bin'])
                    X_final = sm.add_constant(X_final, has_constant='add')  # const ì¶”ê°€
    
                    if X_final.shape[1] <= 1:
                        st.error("ë¶„ì„ì— ì‚¬ìš©í•  ìœ íš¨í•œ ë…ë¦½ ë³€ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                        st.stop()
    
                    st.info(f"ë¶„ì„ì— ì‚¬ìš©ëœ ì´ ê´€ì¸¡ì¹˜: {int(len(y_final))}, ì‚¬ê±´ ìˆ˜: {int(y_final.sum())}")
    
                    # ---------- 1) Univariate ----------
                    uni_results = {}           # {var: fitted_result}
                    univariate_pmins = {}      # {var: min p among its terms}
                    uni_fail_reasons = {}      # {var: reason}
    
                    for var in indep_vars:
                        try:
                            # í•´ë‹¹ ë³€ìˆ˜ì˜ ëª¨ë“  ì»¬ëŸ¼(ì—°ì†í˜• 1ê°œ ë˜ëŠ” ë”ë¯¸ë“¤)ë§Œ ì¶”ì¶œ
                            var_cols = [c for c in X_final.columns if (c == var) or c.startswith(f"{var}=")]
                            if not var_cols:
                                continue
                            X_uni = X_final[['const'] + var_cols].copy()
                            y_uni = y_final.loc[X_uni.index]
                            # ìœ íš¨ì„±
                            if X_uni.shape[1] <= 1 or len(np.unique(y_uni)) < 2:
                                uni_fail_reasons[var] = "ìœ íš¨í•˜ì§€ ì•Šì€ ì„¤ê³„í–‰ë ¬/ë°˜ì‘ê°’"
                                continue
                            # ì í•© (newton â†’ ì‹¤íŒ¨ ì‹œ lbfgs ì¬ì‹œë„)
                            try:
                                res = sm.Logit(y_uni, X_uni).fit(method='newton', disp=0)
                            except Exception:
                                res = sm.Logit(y_uni, X_uni).fit(method='lbfgs', disp=0)
                            uni_results[var] = res
                            # min p (const ì œì™¸)
                            pvals = [res.pvalues[c] for c in res.pvalues.index if c != 'const']
                            if pvals:
                                univariate_pmins[var] = float(np.nanmin(pvals))
                        except PerfectSeparationError:
                            uni_fail_reasons[var] = "ë°ì´í„° ì™„ì „ ë¶„ë¦¬(Perfect Separation)"
                        except np.linalg.LinAlgError:
                            uni_fail_reasons[var] = "ë‹¤ì¤‘ê³µì„ ì„±(Singular Matrix)"
                        except Exception as e:
                            uni_fail_reasons[var] = f"ì˜¤ë¥˜: {type(e).__name__}: {e}"
    
                    if uni_fail_reasons:
                        with st.expander("ë‹¨ë³€ëŸ‰ ì í•© ì‹¤íŒ¨ ë³€ìˆ˜/ì‚¬ìœ  ë³´ê¸°"):
                            for v, r in uni_fail_reasons.items():
                                st.caption(f"- **{v}** â†’ {r}")
    
                    # ---------- 2) ë³€ìˆ˜ ì„ íƒ (p-enter ê¸°ì¤€) ----------
                    selected_vars_for_multi = [v for v, p in univariate_pmins.items() if p <= p_enter_logistic]
                    st.write(f"**ë‹¤ë³€ëŸ‰ ë¶„ì„ í¬í•¨ ë³€ìˆ˜ (p â‰¤ {p_enter_logistic:.3f})**: {selected_vars_for_multi if selected_vars_for_multi else 'ì—†ìŒ'}")
    
                    # ---------- 3) Multivariate ----------
                    result_multi = None
                    X_multi, y_multi = None, None
                    if selected_vars_for_multi:
                        multi_cols = ['const']
                        for var in selected_vars_for_multi:
                            multi_cols += [c for c in X_final.columns if (c == var) or c.startswith(f"{var}=")]
                        # ì¤‘ë³µ ì œê±° ë° ìˆœì„œ ìœ ì§€
                        multi_cols = list(dict.fromkeys(multi_cols))
                        X_multi = X_final[multi_cols].copy()
                        y_multi = y_final.loc[X_multi.index]
                        if X_multi.shape[1] > 1:
                            try:
                                result_multi = sm.Logit(y_multi, X_multi).fit(method='newton', disp=0)
                            except Exception:
                                result_multi = sm.Logit(y_multi, X_multi).fit(method='lbfgs', disp=0)
    
                    # ---------- 4) ë…¼ë¬¸ìš© í‘œ ìƒì„± (Cox í˜•ì‹ê³¼ ìœ ì‚¬) ----------
                    rows = []
                    for var in indep_vars:
                        # ì„¹ì…˜ í—¤ë”
                        rows.append({
                            'Factor': var, 'Subgroup': '',
                            'Univariate OR (95% CI)': '', 'Univariate p-Value': '',
                            'Multivariate OR (95% CI)': '', 'Multivariate p-Value': ''
                        })
    
                        is_cat = (cat_info_logistic.get(var, {}).get('levels') is not None)
                        if is_cat:
                            levels = cat_info_logistic[var]['levels']
                            ref = cat_info_logistic[var]['ref']
                            # Reference ë¼ì¸
                            rows.append({
                                'Factor': '', 'Subgroup': f'{ref} (Reference)',
                                'Univariate OR (95% CI)': 'Ref.', 'Univariate p-Value': '',
                                'Multivariate OR (95% CI)': 'Ref.', 'Multivariate p-Value': ''
                            })
                            # ê° ë”ë¯¸ ë ˆë²¨
                            for lv in levels[1:]:
                                term = f"{var}={lv}"
                                # Univariate
                                if (var in uni_results) and (term in uni_results[var].params.index):
                                    r = uni_results[var]
                                    b = float(r.params[term]); p = float(r.pvalues[term])
                                    ci_low, ci_high = [float(x) for x in r.conf_int().loc[term].tolist()]
                                    or_txt = f"{np.exp(b):.3f} ({np.exp(ci_low):.3f}-{np.exp(ci_high):.3f})"
                                    p_txt = format_p(p)
                                else:
                                    or_txt, p_txt = 'NA', 'NA'
                                # Multivariate
                                if (result_multi is not None) and (term in result_multi.params.index):
                                    r = result_multi
                                    b = float(r.params[term]); p = float(r.pvalues[term])
                                    ci_low, ci_high = [float(x) for x in r.conf_int().loc[term].tolist()]
                                    or_multi = f"{np.exp(b):.3f} ({np.exp(ci_low):.3f}-{np.exp(ci_high):.3f})"
                                    p_multi = format_p(p)
                                else:
                                    or_multi, p_multi = 'NA', 'NA'
                                rows.append({
                                    'Factor': '', 'Subgroup': str(lv),
                                    'Univariate OR (95% CI)': or_txt, 'Univariate p-Value': p_txt,
                                    'Multivariate OR (95% CI)': or_multi, 'Multivariate p-Value': p_multi
                                })
                        else:
                            # ì—°ì†í˜• ë³€ìˆ˜ 1ì—´
                            if (var in uni_results) and (var in uni_results[var].params.index):
                                r = uni_results[var]
                                b = float(r.params[var]); p = float(r.pvalues[var])
                                ci_low, ci_high = [float(x) for x in r.conf_int().loc[var].tolist()]
                                or_txt = f"{np.exp(b):.3f} ({np.exp(ci_low):.3f}-{np.exp(ci_high):.3f})"
                                p_txt = format_p(p)
                            else:
                                or_txt, p_txt = 'NA', 'NA'
                            if (result_multi is not None) and (var in result_multi.params.index):
                                r = result_multi
                                b = float(r.params[var]); p = float(r.pvalues[var])
                                ci_low, ci_high = [float(x) for x in r.conf_int().loc[var].tolist()]
                                or_multi = f"{np.exp(b):.3f} ({np.exp(ci_low):.3f}-{np.exp(ci_high):.3f})"
                                p_multi = format_p(p)
                            else:
                                or_multi, p_multi = 'NA', 'NA'
                            rows.append({
                                'Factor': '', 'Subgroup': '',
                                'Univariate OR (95% CI)': or_txt, 'Univariate p-Value': p_txt,
                                'Multivariate OR (95% CI)': or_multi, 'Multivariate p-Value': p_multi
                            })
    
                    publication_df = pd.DataFrame(rows)
                    st.write("**ë…¼ë¬¸ ì œì¶œìš© í…Œì´ë¸” (Univariate/Multivariate ë³‘ë ¬, Reference, Factor/ìˆ˜ì¤€êµ¬ì¡°)**")
                    st.dataframe(publication_df)
    
                    # ---- HL ì í•©ë„ ----
                    if result_multi is not None:
                        st.write("---")
                        st.subheader("ëª¨ë¸ ì í•©ë„: Hosmerâ€“Lemeshow Test")
                        y_pred_prob = result_multi.predict(X_multi)
                        hl_stat, hl_p, hl_err = calculate_hosmer_lemeshow(y_multi, y_pred_prob)
                        if hl_err:
                            st.warning(f"í˜¸ìŠ¤ë¨¸â€“ë ˜ì‡¼ ê²€ì •ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {hl_err}")
                        else:
                            c1, c2 = st.columns(2)
                            c1.metric("Chi-squared statistic", f"{hl_stat:.3f}")
                            c2.metric("p-value", f"{hl_p:.3f}")
                            st.caption("â€» p-valueê°€ 0.05ë³´ë‹¤ í¬ë©´ ëª¨ë¸ì´ ë°ì´í„°ì— ì ì ˆíˆ ì í•©í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
                    # ---- ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ----
                    output_logistic = io.BytesIO()
                    with pd.ExcelWriter(output_logistic, engine='openpyxl') as writer:
                        publication_df.to_excel(writer, index=False, sheet_name='Logistic Regression Results')
                    st.download_button(
                        label="ë¡œì§€ìŠ¤í‹± ê²°ê³¼ ì—‘ì…€ë¡œ ì €ì¥",
                        data=output_logistic.getvalue(),
                        file_name="Logistic_Regression_Publication_Table.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        key='download_logistic_publication'
                    )
    
            except Exception as e:
                st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {type(e).__name__}: {e}")
     
# ================== ë ==============================
