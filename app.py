# -*- coding: utf-8 -*-
# ================== ìë™ Table & Cox & Logistic ë¶„ì„ê¸° ==================
# í•„ìš” íŒ¨í‚¤ì§€: pandas, numpy, scipy, lifelines, statsmodels, openpyxl, xlrd, streamlit

import streamlit as st
import pandas as pd
import numpy as np
from scipy import stats
import io
from lifelines import CoxPHFitter
from lifelines.exceptions import ConvergenceError
import statsmodels.api as sm # For Logistic Regression
from statsmodels.tools.sm_exceptions import PerfectSeparationError
import matplotlib.pyplot as plt

# ----- í˜ì´ì§€ ì„¤ì • -----
st.set_page_config(page_title="ìë™ ë…¼ë¬¸ Table", layout="wide")

# ----- ê°„ë‹¨ ë¹„ë°€ë²ˆí˜¸ ë³´í˜¸ (ê¸°ë³¸: CRCR ë˜ëŠ” st.secrets['APP_PASSWORD']) -----
def _check_password():
    def _password_entered():
        # Streamlit secretsì— ì €ì¥ëœ ë¹„ë°€ë²ˆí˜¸ ë˜ëŠ” ê¸°ë³¸ê°’ "CRCR" ì‚¬ìš©
        target = st.secrets.get("APP_PASSWORD", "CRCR")
        if st.session_state.get("_password_input", "") == str(target):
            st.session_state["_pw_ok"] = True
            st.session_state.pop("_password_input", None)
        else:
            st.session_state["_pw_ok"] = False

    if st.session_state.get("_pw_ok", False):
        return True

    st.sidebar.subheader("ğŸ” Access")
    st.sidebar.text_input("Password", type="password", key="_password_input", on_change=_password_entered)
    if st.session_state.get("_pw_ok") is False:
        st.sidebar.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

# ë¹„ë°€ë²ˆí˜¸ ì²´í¬ ì‹¤í–‰
_check_password()

st.title("ìë™ Table ìƒì„±ê¸°")

# -------------------- [ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜] --------------------
def format_p(p):
    """p-valueë¥¼ ë…¼ë¬¸ í˜•ì‹ì— ë§ê²Œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if p is None or (isinstance(p, float) and np.isnan(p)):
        return "NA"
    if p >= 0.999:
        return "p > 0.99"
    if p < 0.001:
        return "<0.001"
    return f"{p:.3f}"

def is_continuous(series, threshold=20):
    """ì‹œë¦¬ì¦ˆê°€ ì—°ì†í˜• ë³€ìˆ˜ì¸ì§€ íŒë³„í•©ë‹ˆë‹¤."""
    try:
        # ìˆ«ìí˜•(ì •ìˆ˜, ì‹¤ìˆ˜)ì´ê³  ê³ ìœ ê°’ ê°œìˆ˜ê°€ thresholdë¥¼ ì´ˆê³¼í•˜ë©´ ì—°ì†í˜•ìœ¼ë¡œ íŒë‹¨
        return (series.dtype.kind in "fi") and (series.nunique(dropna=True) > threshold)
    except Exception:
        return False

def ordered_levels(series):
    """ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ë ˆë²¨ì„ ì •ë ¬í•©ë‹ˆë‹¤. ìˆ«ìí˜•ì´ë©´ ìˆ«ì ìˆœ, ì•„ë‹ˆë©´ ë¬¸ì ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤."""
    # ê³ ìœ ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€ (ì˜ˆ: 1ê³¼ '1'ì„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬)
    unique_strings = pd.Series(series.dropna().unique()).astype(str).unique().tolist()
    
    try:
        # ìˆ«ì(float) ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ ì‹œë„
        unique_strings.sort(key=float)
    except ValueError:
        # í•˜ë‚˜ë¼ë„ float ë³€í™˜ ì‹¤íŒ¨ ì‹œ, ë¬¸ìì—´ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        unique_strings.sort()
    
    return unique_strings

def make_dummies(df_in, var, levels):
    """ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ë”ë¯¸ ë³€ìˆ˜ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë ˆë²¨ì„ ê¸°ì¤€ìœ¼ë¡œ í•©ë‹ˆë‹¤."""
    cat = pd.Categorical(df_in[var].astype(str),
                         categories=[str(x) for x in levels],
                         ordered=True)
    dmy = pd.get_dummies(cat, prefix=var, prefix_sep="=", drop_first=True, dtype=float)
    dmy.index = df_in.index
    return dmy

def dummy_colname(var, level):
    """ë”ë¯¸ ë³€ìˆ˜ì˜ ì»¬ëŸ¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    return f"{var}={str(level)}"

def drop_constant_cols(X):
    """ìƒìˆ˜ ì»¬ëŸ¼ì„ ì œê±°í•˜ë˜, íšŒê·€ë¶„ì„ì— í•„ìš”í•œ 'const' ì»¬ëŸ¼ì€ ìœ ì§€í•©ë‹ˆë‹¤."""
    cols_to_keep = []
    for col in X.columns:
        # 'const' ì»¬ëŸ¼ì´ê±°ë‚˜, ê³ ìœ ê°’ì´ 1ê°œ ì´ˆê³¼ì¸ ê²½ìš°ì—ë§Œ ìœ ì§€
        if col == 'const' or X[col].nunique(dropna=True) > 1:
            cols_to_keep.append(col)
    return X[cols_to_keep]

def drop_constant_predictors(X, time_col, event_col):
    """Cox ë¶„ì„ìš© ë°ì´í„°ì—ì„œ ìƒìˆ˜ì¸ ì˜ˆì¸¡ ë³€ìˆ˜ë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
    pred_cols = [c for c in X.columns if c not in [time_col, event_col]]
    keep = [c for c in pred_cols if X[c].nunique(dropna=True) > 1]
    return X[[time_col, event_col] + keep]

def clean_time(s):
    """ìƒì¡´ ë¶„ì„ì˜ ì‹œê°„ ë³€ìˆ˜ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤. (ìˆ«ìí˜• ë³€í™˜, inf/nan ì²˜ë¦¬)"""
    s = pd.to_numeric(s, errors="coerce")
    s = s.replace([np.inf, -np.inf], np.nan)
    return s

def ensure_binary_event(col, events, censored):
    """ì´ë²¤íŠ¸ ë³€ìˆ˜ë¥¼ 0(censored)ê³¼ 1(event)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    def _map(x):
        if x in events: return 1
        if x in censored: return 0
        return np.nan
    return col.apply(_map).astype(float)

def calculate_hosmer_lemeshow(y_true, y_pred_prob, n_groups=10):
    """ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ì˜ Hosmer-Lemeshow ì í•©ë„ ê²€ì •ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
    data = pd.DataFrame({'y_true': y_true, 'y_pred_prob': y_pred_prob})
    
    try:
        data['group'] = pd.qcut(data['y_pred_prob'], q=n_groups, duplicates='drop')
    except ValueError:
        # ì¤‘ë³µ ì˜ˆì¸¡ í™•ë¥ ê°’ìœ¼ë¡œ ì¸í•´ ê·¸ë£¹ ë‚˜ëˆ„ê¸° ì‹¤íŒ¨ ì‹œ, ê·¸ë£¹ ìˆ˜ë¥¼ ì¤„ì—¬ì„œ ì¬ì‹œë„
        n_groups = data['y_pred_prob'].nunique()
        if n_groups < 2: return np.nan, np.nan, "Not enough unique probabilities for the test."
        data['group'] = pd.qcut(data['y_pred_prob'], q=n_groups, duplicates='drop')
    
    summary = data.groupby('group', observed=False).agg(
        total_count=('y_true', 'count'),
        observed_events=('y_true', 'sum'),
        expected_events=('y_pred_prob', 'sum')
    )
    
    summary['observed_non_events'] = summary['total_count'] - summary['observed_events']
    summary['expected_non_events'] = summary['total_count'] - summary['expected_events']

    if (summary['expected_events'] < 1e-6).any() or (summary['expected_non_events'] < 1e-6).any():
        return np.nan, np.nan, "Test failed due to zero or near-zero expected frequencies in some groups."

    hl_stat = (
        ((summary['observed_events'] - summary['expected_events'])**2 / summary['expected_events']) +
        ((summary['observed_non_events'] - summary['expected_non_events'])**2 / summary['expected_non_events'])
    ).sum()

    df_hl = len(summary) - 2
    if df_hl <= 0:
        return np.nan, np.nan, "Not enough groups to calculate p-value (degrees of freedom <= 0)."
        
    p_value = stats.chi2.sf(hl_stat, df_hl)
    
    return hl_stat, p_value, None

def select_penalizer_by_cv(X_all, time_col, event_col,
                           grid=(0.0, 0.01, 0.05, 0.1, 0.2, 0.5),
                           k=5, seed=42):
    """êµì°¨ ê²€ì¦(Cross-Validation)ì„ í†µí•´ ìµœì ì˜ penalizer ê°’ì„ ì°¾ìŠµë‹ˆë‹¤."""
    if X_all.shape[0] < k + 2 or X_all[event_col].sum() < k:
        return None, {}
    idx = X_all.index.to_numpy()
    rng = np.random.default_rng(seed)
    rng.shuffle(idx)
    folds = np.array_split(idx, k)
    scores = {}
    for pen in grid:
        cv_scores = []
        for i in range(k):
            test_idx = folds[i]
            train_idx = np.concatenate([folds[j] for j in range(k) if j != i])
            train = X_all.loc[train_idx].copy()
            test  = X_all.loc[test_idx].copy()
            train = drop_constant_predictors(train, time_col, event_col)
            test  = test[train.columns]
            if train[event_col].sum() < 2 or test[event_col].sum() < 1: continue
            if train.shape[1] <= 2 or train.shape[0] < 5: continue
            try:
                cph = CoxPHFitter(penalizer=pen)
                cph.fit(train, duration_col=time_col, event_col=event_col)
                s = float(cph.score(test, scoring_method="concordance_index"))
                if np.isfinite(s): cv_scores.append(s)
            except Exception: continue
        if cv_scores: scores[pen] = float(np.mean(cv_scores))
    if not scores: return None, {}
    best_pen = sorted(scores.items(), key=lambda x: (-x[1], x[0]))[0][0]
    return best_pen, scores

# -------------------- [íŒŒì¼ ì—…ë¡œë“œ UI] --------------------
uploaded_file = st.file_uploader("ì—‘ì…€/CSV ì—…ë¡œë“œ", type=['xls', 'xlsx', 'csv'])
df = None
sheetname = None
if uploaded_file:
    name = uploaded_file.name.lower()
    try:
        if name.endswith(".csv"):
            df = pd.read_csv(uploaded_file)
        elif name.endswith((".xlsx", ".xls")):
            engine = "openpyxl" if name.endswith(".xlsx") else "xlrd"
            xls = pd.ExcelFile(uploaded_file, engine=engine)
            sheetname = xls.sheet_names[0] if len(xls.sheet_names) == 1 else st.selectbox("ì‹œíŠ¸ ì„ íƒ", xls.sheet_names, index=0)
            df = pd.read_excel(xls, sheet_name=sheetname, engine=engine)
        
        # ì»¬ëŸ¼ëª… ê³µë°± ë° ê°œí–‰ ë¬¸ì ì œê±°
        df.columns = pd.Index([str(c).strip().replace("\\n", " ") for c in df.columns])
        st.success(f"ì‹œíŠ¸ëª…: {sheetname if sheetname else ''}, ë°ì´í„° shape: {df.shape}")
        st.dataframe(df.head())
        st.session_state['df'] = df
    except Exception as e:
        st.error(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.stop()

# -------------------- [Table1: ë¶„ì„ í•¨ìˆ˜] --------------------
def analyze_table1_display(df, group_col, value_map, threshold=20):
    result_rows = []
    group_values = list(value_map.keys())
    group_names = list(value_map.values())
    group_n = {g: (df[group_col] == g).sum() for g in group_values}
    
    for var in df.columns:
        if var == group_col: continue
        valid = df[df[group_col].isin(group_values)]
        if valid[var].dropna().empty: continue
        
        if is_continuous(valid[var], threshold=threshold):
            # ì—°ì†í˜• ë³€ìˆ˜ ì²˜ë¦¬
            row = {'Characteristic': var}
            for g, g_name in zip(group_values, group_names):
                sub = valid[valid[group_col] == g][var].dropna()
                if sub.shape[0] > 0:
                    med, q1, q3 = sub.median(), sub.quantile(0.25), sub.quantile(0.75)
                    mean, std = sub.mean(), sub.std()
                    row[f"{g_name} (n={group_n[g]})"] = f"{med:.1f} [{q1:.1f}-{q3:.1f}]; {mean:.1f}Â±{std:.1f}"
                else:
                    row[f"{g_name} (n={group_n[g]})"] = "NA"
            
            p = np.nan; test_str = ""
            try:
                arr = [valid[valid[group_col] == g][var].dropna() for g in group_values]
                # ì •ê·œì„± ê²€ì • í›„ t-test/ANOVA ë˜ëŠ” ë¹„ëª¨ìˆ˜ ê²€ì • ì„ íƒ
                if all(stats.shapiro(s)[1] > 0.05 for s in arr if len(s) >= 3):
                    if len(arr) == 2: _, p = stats.ttest_ind(arr[0], arr[1], nan_policy='omit'); test_str = "t-test"
                    elif len(arr) > 2: _, p = stats.f_oneway(*arr); test_str = "ANOVA"
                else:
                    if len(arr) == 2: _, p = stats.mannwhitneyu(arr[0], arr[1], alternative='two-sided'); test_str = "Mann-Whitney U"
                    elif len(arr) > 2: _, p = stats.kruskal(*arr); test_str = "Kruskal-Wallis"
            except Exception: p = np.nan; test_str = "NA"
            row['Test'] = test_str; row['p value'] = format_p(p); row['sub_p'] = ""
            result_rows.append(row)
        else:
            # ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬
            p = np.nan; test_str = ""
            try:
                ct = pd.crosstab(valid[group_col], valid[var])
                if ct.shape == (2, 2): _, p = stats.fisher_exact(ct); test_str = "Fisher's Exact"
                else: _, p, _, _ = stats.chi2_contingency(ct); test_str = "Chi-square"
            except Exception: p = np.nan; test_str = "NA"
            
            result_rows.append({'Characteristic': var, **{f"{g_name} (n={group_n[g]})": "" for g_name in group_names}, 'Test': test_str, 'p value': format_p(p), 'sub_p': ""})
            
            for val in ordered_levels(valid[var]):
                row = {'Characteristic': f"  {val}"}
                for g, g_name in zip(group_values, group_names):
                    cnt = valid[(valid[group_col] == g) & (valid[var].astype(str) == str(val))].shape[0]
                    percent = (cnt/group_n[g]*100) if group_n[g] > 0 else 0
                    row[f"{g_name} (n={group_n[g]})"] = f"{cnt}({percent:.0f}%)"
                
                row['Test'] = ""
                row['p value'] = ""
                p_sub = np.nan
                try:
                    # í•˜ìœ„ ê·¸ë£¹ p-value ê³„ì‚°
                    table = np.array([[(valid[(valid[group_col] == g) & (valid[var].astype(str) == str(val))]).shape[0], (valid[(valid[group_col] == g) & (valid[var].astype(str) != str(val))]).shape[0]] for g in group_values])
                    if table.shape == (2, 2): _, p_sub = stats.fisher_exact(table)
                    else: _, p_sub, _, _ = stats.chi2_contingency(table)
                except: pass
                row['sub_p'] = format_p(p_sub)
                result_rows.append(row)
                
    return pd.DataFrame(result_rows)


# -------------------- [UI: íƒ­ êµ¬ì„±] --------------------
if 'df' in st.session_state:
    df = st.session_state['df']
else:
    df = None

if df is not None:
    tab_titles = ["ğŸ“Š Table1 ìë™í™”", "ğŸŸ¦ Cox íšŒê·€ë¶„ì„", "ğŸŸ§ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„"]
    tab1, tab2, tab3 = st.tabs(tab_titles)

    # ==================== TAB1: Table 1 ====================
    with tab1:
        st.header("Table 1 ìë™ ìƒì„±")
        group_col = st.selectbox("ë¶„ì„í•  ê·¸ë£¹ ë³€ìˆ˜ëª… ì„ íƒ", options=list(df.columns), key='group_col')
        if group_col:
            unique_vals = list(df[group_col].dropna().unique())
            selected_vals = st.multiselect("ë¶„ì„í•  ê·¸ë£¹ ê°’ ì„ íƒ", unique_vals, default=unique_vals[:2] if len(unique_vals) >= 2 else unique_vals, key='group_values')
            if selected_vals:
                value_map = {val: st.text_input(f"'{val}'ì˜ í‘œì‹œ ë¼ë²¨", value=str(val), key=f'label_{val}') for val in selected_vals}
                if st.button("ë…¼ë¬¸ Table1 ìƒì„±", key='table1_generate'):
                    with st.spinner('Table 1ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...'):
                        result = analyze_table1_display(df, group_col, value_map, threshold=20)
                        st.dataframe(result)
                        output = io.BytesIO()
                        with pd.ExcelWriter(output, engine='openpyxl') as writer:
                            result.to_excel(writer, index=False)
                        st.download_button("Table1 ì—‘ì…€ë¡œ ì €ì¥", output.getvalue(), "Table1_Results.xlsx")

    # ==================== TAB2: Cox Regression ====================
    with tab2:
        st.header("Cox ë¹„ë¡€ìœ„í—˜ íšŒê·€ë¶„ì„ (Univariate & Multivariate)")
        time_col = st.selectbox("ìƒì¡´ê¸°ê°„ ë³€ìˆ˜(time)", df.columns, key="cox_time_col")
        event_col = st.selectbox("Event ë³€ìˆ˜(ì˜ˆ: 0=ìƒì¡´, 1=ì‚¬ë§ ë“±)", df.columns, key="cox_event_col")
        
        if event_col:
            unique_events = list(df[event_col].dropna().unique())
            selected_event = st.multiselect("ì´ë²¤íŠ¸(ì‚¬ê±´) ê°’", unique_events, key='selected_event_val')
            selected_censored = st.multiselect("ìƒì¡´/ê´€ì°°ì¢…ê²°(censored) ê°’", unique_events, key='selected_censored_val')
            
        variables = st.multiselect("ë¶„ì„ í›„ë³´ ë³€ìˆ˜ ì„ íƒ", [c for c in df.columns if c not in [time_col, event_col]], key="cox_variables")
        
        c1, c2, c3, c4 = st.columns(4)
        p_enter = c1.number_input("ë‹¤ë³€ëŸ‰ í¬í•¨ ê¸°ì¤€ p-enter (â‰¤)", 0.001, 1.0, 0.05, 0.01)
        max_levels = c2.number_input("ë²”ì£¼í˜• íŒì • ìµœëŒ€ ê³ ìœ ê°’", 2, 50, 10, 1, key="cox_max_levels")
        auto_penal = c3.checkbox("penalizer ìë™ ì„ íƒ (CV)", value=False)
        cv_k = c4.number_input("CV folds (K)", 3, 10, 5, 1, disabled=not auto_penal)
        penalizer = st.number_input("penalizer (ìˆ˜ë ´ ì•ˆì •í™”)", 0.0, 5.0, 0.1, 0.01, disabled=auto_penal)

        if st.button("Cox íšŒê·€ë¶„ì„ ì‹¤í–‰"):
            with st.spinner('Cox íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰ ì¤‘ì…ë‹ˆë‹¤...'):
                if not selected_event or not selected_censored:
                    st.error("ì‚¬ê±´ê°’ê³¼ ê²€ì—´ê°’ì„ ê°ê° ìµœì†Œ 1ê°œ ì´ìƒ ì„ íƒí•˜ì„¸ìš”.")
                    st.stop()
                if set(selected_event) & set(selected_censored):
                    st.error("ì‚¬ê±´ê°’ê³¼ ê²€ì—´ê°’ì´ ê²¹ì¹©ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•˜ì„¸ìš”.")
                    st.stop()

                # ë°ì´í„° ì¤€ë¹„
                df_cox = df.copy()
                df_cox["__event_for_cox"] = ensure_binary_event(df_cox[event_col], set(selected_event), set(selected_censored))
                df_cox[time_col] = clean_time(df_cox[time_col])
                df_cox = df_cox.dropna(subset=[time_col, "__event_for_cox"])
                df_cox = df_cox[df_cox[time_col] > 0]
                
                n_events = int(df_cox["__event_for_cox"].sum())
                n_total = df_cox.shape[0]
                st.info(f"ì´ ê´€ì¸¡ì¹˜: {n_total}, ì´ë²¤íŠ¸ ìˆ˜: {n_events}")
                if n_events < 5:
                    st.warning("ì´ë²¤íŠ¸ ìˆ˜ê°€ 5ê°œ ë¯¸ë§Œìœ¼ë¡œ ë§¤ìš° ì ì–´ ë¶„ì„ì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                # 1. ë‹¨ë³€ëŸ‰ ë¶„ì„ (Univariate)
                uni_results = {}
                failed_uni_vars = {}
                cat_info = {}
                for var in variables:
                    try:
                        cols_to_use = [time_col, "__event_for_cox", var]
                        df_uni = df_cox[cols_to_use].dropna(subset=[var]).copy()
                        if df_uni.shape[0] < 3: continue
                        
                        if is_continuous(df_uni[var], threshold=max_levels):
                            cat_info[var] = {"levels": None, "ref": None}
                            df_uni[var] = pd.to_numeric(df_uni[var], errors='coerce')
                        else:
                            levels = ordered_levels(df_uni[var])
                            cat_info[var] = {"levels": levels, "ref": levels[0]}
                            dummies = make_dummies(df_uni, var, levels)
                            df_uni = pd.concat([df_uni[[time_col, "__event_for_cox"]], dummies], axis=1)
                        
                        df_uni = df_uni.dropna()
                        df_uni = drop_constant_predictors(df_uni, time_col, "__event_for_cox")

                        if df_uni.shape[1] > 2 and df_uni["__event_for_cox"].sum() > 0:
                            cph = CoxPHFitter(penalizer=penalizer)
                            cph.fit(df_uni, duration_col=time_col, event_col="__event_for_cox")
                            uni_results[var] = cph.summary
                    except Exception as e:
                        failed_uni_vars[var] = str(e)
                
                # 2. ë‹¤ë³€ëŸ‰ ë¶„ì„ì„ ìœ„í•œ ë³€ìˆ˜ ì„ íƒ
                univariate_pvals = {var: res['p'].min() for var, res in uni_results.items()}
                selected_vars = [v for v, p in univariate_pvals.items() if p <= p_enter]
                st.write(f"**ë‹¤ë³€ëŸ‰ ë¶„ì„ í¬í•¨ ë³€ìˆ˜ (p â‰¤ {p_enter})**: {selected_vars if selected_vars else 'ì—†ìŒ'}")

                # 3. ë‹¤ë³€ëŸ‰ ë¶„ì„ (Multivariate)
                multi_summary = None
                chosen_penalizer = penalizer
                if selected_vars:
                    try:
                        cols_for_multi = [time_col, "__event_for_cox"] + selected_vars
                        df_multi_raw = df_cox[cols_for_multi].copy()
                        
                        X_list = []
                        for var in selected_vars:
                            if cat_info.get(var, {}).get("levels"):
                                X_list.append(make_dummies(df_multi_raw[[var]], var, cat_info[var]['levels']))
                            else:
                                X_list.append(pd.to_numeric(df_multi_raw[var], errors='coerce').rename(var))
                        
                        X_processed = pd.concat(X_list, axis=1)
                        df_multi = pd.concat([df_multi_raw[[time_col, "__event_for_cox"]], X_processed], axis=1).dropna()
                        df_multi = drop_constant_predictors(df_multi, time_col, "__event_for_cox")

                        if auto_penal:
                            best_pen, scores = select_penalizer_by_cv(df_multi, time_col, "__event_for_cox", k=cv_k)
                            if best_pen is not None:
                                chosen_penalizer = best_pen
                                st.success(f"CVë¥¼ í†µí•´ ìµœì  Penalizer = {chosen_penalizer} ì„ íƒ (C-index ê¸°ì¤€)")
                                st.caption(f"Grid ì„±ëŠ¥: { {k: round(v,4) for k,v in scores.items()} }")
                            else:
                                st.warning("CVë¡œ Penalizerë¥¼ ê²°ì •í•˜ì§€ ëª»í•´ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        
                        if df_multi.shape[1] > 2 and df_multi["__event_for_cox"].sum() > 0:
                            cph_multi = CoxPHFitter(penalizer=chosen_penalizer)
                            cph_multi.fit(df_multi, duration_col=time_col, event_col="__event_for_cox")
                            multi_summary = cph_multi.summary
                    except Exception as e:
                        st.error(f"ë‹¤ë³€ëŸ‰ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

                # 4. ê²°ê³¼ í…Œì´ë¸” ìƒì„±
                output_rows = []
                for var in variables:
                    is_cat = cat_info.get(var, {}).get('levels') is not None
                    
                    if is_cat:
                        output_rows.append({'Factor': var, 'Subgroup': '', 'Univariate HR (95% CI)': '', 'p (Uni)': '', 'Multivariate HR (95% CI)': '', 'p (Multi)': ''})
                        levels = cat_info[var]['levels']
                        output_rows.append({'Factor': '', 'Subgroup': f"{levels[0]} (Reference)", 'Univariate HR (95% CI)': '1.0', 'p (Uni)': '', 'Multivariate HR (95% CI)': '1.0', 'p (Multi)': ''})
                        
                        for level in levels[1:]:
                            dummy_name = dummy_colname(var, level)
                            row = {'Factor': '', 'Subgroup': str(level)}
                            # Uni
                            if var in uni_results and dummy_name in uni_results[var].index:
                                r = uni_results[var].loc[dummy_name]
                                row['Univariate HR (95% CI)'] = f"{r['exp(coef)']:.3f} ({r['exp(coef) lower 95%']:.3f}-{r['exp(coef) upper 95%']:.3f})"
                                row['p (Uni)'] = format_p(r['p'])
                            # Multi
                            if multi_summary is not None and dummy_name in multi_summary.index:
                                r = multi_summary.loc[dummy_name]
                                row['Multivariate HR (95% CI)'] = f"{r['exp(coef)']:.3f} ({r['exp(coef) lower 95%']:.3f}-{r['exp(coef) upper 95%']:.3f})"
                                row['p (Multi)'] = format_p(r['p'])
                            output_rows.append(row)
                    else: # Continuous
                        row = {'Factor': var, 'Subgroup': ''}
                        # Uni
                        if var in uni_results and var in uni_results[var].index:
                            r = uni_results[var].loc[var]
                            row['Univariate HR (95% CI)'] = f"{r['exp(coef)']:.3f} ({r['exp(coef) lower 95%']:.3f}-{r['exp(coef) upper 95%']:.3f})"
                            row['p (Uni)'] = format_p(r['p'])
                        # Multi
                        if multi_summary is not None and var in multi_summary.index:
                            r = multi_summary.loc[var]
                            row['Multivariate HR (95% CI)'] = f"{r['exp(coef)']:.3f} ({r['exp(coef) lower 95%']:.3f}-{r['exp(coef) upper 95%']:.3f})"
                            row['p (Multi)'] = format_p(r['p'])
                        output_rows.append(row)
                
                publication_df = pd.DataFrame(output_rows).fillna('')
                st.dataframe(publication_df)
                
                output_cox = io.BytesIO()
                with pd.ExcelWriter(output_cox, engine='openpyxl') as writer:
                    publication_df.to_excel(writer, index=False)
                st.download_button("Cox ë¶„ì„ ê²°ê³¼ ì—‘ì…€ ì €ì¥", output_cox.getvalue(), "Cox_Regression_Results.xlsx")


    # ==================== TAB3: Logistic Regression ====================
    with tab3:
        st.header("ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ (Risk Factor Analysis)")
        st.info("ì¢…ì†ë³€ìˆ˜ì˜ íŠ¹ì • ê°’ì„ ì‚¬ê±´(1)ê³¼ ê¸°ì¤€(0)ìœ¼ë¡œ ì •ì˜í•˜ì—¬ ìœ„í—˜ì¸ìë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

        dep_var = st.selectbox("ì¢…ì† ë³€ìˆ˜ (Y) ì„ íƒ", df.columns, key="logistic_dep_var")
        
        if dep_var:
            unique_outcomes = list(df[dep_var].dropna().unique())
            event_values = st.multiselect("ì‚¬ê±´(Event=1)ì— í•´ë‹¹í•˜ëŠ” ê°’ ì„ íƒ", unique_outcomes, key="logistic_event")
            control_values = st.multiselect("ê¸°ì¤€(Control=0)ì— í•´ë‹¹í•˜ëŠ” ê°’ ì„ íƒ", unique_outcomes, key="logistic_control")
            st.caption("â€» ì‚¬ê±´ ê°’ê³¼ ê¸°ì¤€ ê°’ì€ ì„œë¡œ ê²¹ì¹˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.")

        indep_vars = st.multiselect("ë…ë¦½ ë³€ìˆ˜ (X) ì„ íƒ (ìœ„í—˜ì¸ì í›„ë³´)", [c for c in df.columns if c != dep_var], key="logistic_indep_vars")
        
        c1_log, c2_log = st.columns(2)
        p_enter_logistic = c1_log.number_input("ë‹¤ë³€ëŸ‰ í¬í•¨ ê¸°ì¤€ p-enter (â‰¤)", 0.001, 1.0, 0.05, 0.01, key='logistic_p_enter')
        max_levels_logistic = c2_log.number_input("ë²”ì£¼í˜• íŒì • ìµœëŒ€ ê³ ìœ ê°’", 2, 50, 10, 1, key="logistic_max_levels")

        if st.button("ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ ì‹¤í–‰", key="run_logistic"):
            if not dep_var or not event_values or not control_values or not indep_vars:
                st.error("ì¢…ì† ë³€ìˆ˜, ì‚¬ê±´ ê°’, ê¸°ì¤€ ê°’, ë…ë¦½ ë³€ìˆ˜ë¥¼ ëª¨ë‘ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
                st.stop()
            if set(event_values) & set(control_values):
                st.error("ì‚¬ê±´ ê°’ê³¼ ê¸°ì¤€ ê°’ì´ ê²¹ì¹©ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•˜ì„¸ìš”.")
                st.stop()

            try:
                with st.spinner('ë¶„ì„ì„ ìˆ˜í–‰ ì¤‘ì…ë‹ˆë‹¤...'):
                    # ë°ì´í„° ì¤€ë¹„
                    cols_to_use = [dep_var] + indep_vars
                    df_model = df[cols_to_use].copy()
                    df_model['__y_binary'] = ensure_binary_event(df_model[dep_var], set(event_values), set(control_values))
                    df_model.dropna(subset=['__y_binary'], inplace=True)
                    df_model['__y_binary'] = df_model['__y_binary'].astype(int)
                    y = df_model['__y_binary']

                    # ë…ë¦½ë³€ìˆ˜ ì²˜ë¦¬ (ë”ë¯¸ ë³€ìˆ˜í™”)
                    X_list, cat_info_logistic = [], {}
                    for var in indep_vars:
                        if not is_continuous(df_model[var], threshold=max_levels_logistic):
                            levels = ordered_levels(df_model[var])
                            cat_info_logistic[var] = {"levels": levels, "ref": levels[0]}
                            X_list.append(make_dummies(df_model[[var]], var, levels))
                        else:
                            cat_info_logistic[var] = {"levels": None, "ref": None}
                            X_list.append(pd.to_numeric(df_model[var], errors='coerce').rename(var))
                    
                    if not X_list: st.error("ìœ íš¨í•œ ë…ë¦½ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤."); st.stop()

                    X_processed = pd.concat(X_list, axis=1)
                    model_data = pd.concat([y, X_processed], axis=1).dropna()
                    y_final = model_data[y.name]
                    X_final_no_const = model_data.drop(columns=[y.name])
                    
                    # ìƒìˆ˜í•­ ì¶”ê°€ í›„ ìƒìˆ˜ ì˜ˆì¸¡ë³€ìˆ˜ ì œê±°
                    X_final_with_const = sm.add_constant(X_final_no_const, has_constant='add')
                    X_final = drop_constant_cols(X_final_with_const)

                    if X_final.shape[1] <= 1 or 'const' not in X_final.columns:
                        st.error("ë¶„ì„ì— ì‚¬ìš©í•  ìœ íš¨í•œ ë…ë¦½ ë³€ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."); st.stop()
                    st.info(f"ë¶„ì„ì— ì‚¬ìš©ëœ ì´ ê´€ì¸¡ì¹˜: {len(y_final)}, ì‚¬ê±´ ìˆ˜: {y_final.sum()}")

                    # 1. ë‹¨ë³€ëŸ‰ ë¶„ì„
                    uni_results, p_values_uni, failed_vars = {}, {}, {}
                    for var in indep_vars:
                        var_cols = [c for c in X_final.columns if c == var or c.startswith(f"{var}=")]
                        if not var_cols: continue
                        
                        X_uni = X_final[['const'] + var_cols]
                        try:
                            res = sm.Logit(y_final, X_uni).fit(method='newton', disp=0) 
                            uni_results[var] = res
                            p_values_uni[var] = res.pvalues.drop('const').min()
                        except Exception as e: 
                            failed_vars[var] = str(e)

                    # 2. ë‹¤ë³€ëŸ‰ ë¶„ì„
                    selected_vars_multi = [v for v, p in p_values_uni.items() if p <= p_enter_logistic]
                    st.write(f"**ë‹¤ë³€ëŸ‰ ë¶„ì„ í¬í•¨ ë³€ìˆ˜ (p â‰¤ {p_enter_logistic})**: {selected_vars_multi if selected_vars_multi else 'ì—†ìŒ'}")
                    
                    result_multi = None
                    if selected_vars_multi:
                        multi_cols = ['const'] + [c for var in selected_vars_multi for c in X_final.columns if c.startswith(f"{var}=") or c == var]
                        X_multi = X_final[multi_cols]
                        try:
                            result_multi = sm.Logit(y_final, X_multi).fit(method='newton', disp=0)
                        except Exception as e:
                            st.error(f"ë‹¤ë³€ëŸ‰ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

                    # 3. ê²°ê³¼ í…Œì´ë¸” ìƒì„±
                    output_rows = []
                    for var in indep_vars:
                        is_cat = cat_info_logistic[var]['levels'] is not None
                        if is_cat:
                            output_rows.append({'Factor': var, 'Subgroup': '', 'Univariate OR (95% CI)': '', 'p-value (Uni)': '', 'Multivariate OR (95% CI)': '', 'p-value (Multi)': ''})
                            levels = cat_info_logistic[var]['levels']
                            output_rows.append({'Factor': '', 'Subgroup': f"{levels[0]} (Reference)", 'Univariate OR (95% CI)': '1.0', 'p-value (Uni)': '', 'Multivariate OR (95% CI)': '1.0' if var in selected_vars_multi else '', 'p-value (Multi)': ''})
                            for level in levels[1:]:
                                d_name = dummy_colname(var, level)
                                row = {'Factor': '', 'Subgroup': str(level)}
                                if var in uni_results and d_name in uni_results[var].params:
                                    res, p, conf = uni_results[var].params[d_name], uni_results[var].pvalues[d_name], uni_results[var].conf_int().loc[d_name]
                                    row['Univariate OR (95% CI)'] = f"{np.exp(res):.3f} ({np.exp(conf[0]):.3f}-{np.exp(conf[1]):.3f})"
                                    row['p-value (Uni)'] = format_p(p)
                                if result_multi and d_name in result_multi.params:
                                    res, p, conf = result_multi.params[d_name], result_multi.pvalues[d_name], result_multi.conf_int().loc[d_name]
                                    row['Multivariate OR (95% CI)'] = f"{np.exp(res):.3f} ({np.exp(conf[0]):.3f}-{np.exp(conf[1]):.3f})"
                                    row['p-value (Multi)'] = format_p(p)
                                output_rows.append(row)
                        else: # Continuous
                            row = {'Factor': var, 'Subgroup': ''}
                            if var in uni_results and var in uni_results[var].params:
                                res, p, conf = uni_results[var].params[var], uni_results[var].pvalues[var], uni_results[var].conf_int().loc[var]
                                row['Univariate OR (95% CI)'] = f"{np.exp(res):.3f} ({np.exp(conf[0]):.3f}-{np.exp(conf[1]):.3f})"
                                row['p-value (Uni)'] = format_p(p)
                            if result_multi and var in result_multi.params:
                                res, p, conf = result_multi.params[var], result_multi.pvalues[var], result_multi.conf_int().loc[var]
                                row['Multivariate OR (95% CI)'] = f"{np.exp(res):.3f} ({np.exp(conf[0]):.3f}-{np.exp(conf[1]):.3f})"
                                row['p-value (Multi)'] = format_p(p)
                            output_rows.append(row)

                    publication_df = pd.DataFrame(output_rows).fillna('')
                    st.dataframe(publication_df)

                    if result_multi:
                        st.write("---")
                        st.write("### ëª¨ë¸ ì í•©ë„ ê²€ì • (Hosmer-Lemeshow Test)")
                        y_pred_prob = result_multi.predict(X_multi)
                        hl_stat, p_value_hl, hl_error = calculate_hosmer_lemeshow(y_final, y_pred_prob)
                        if hl_error:
                            st.warning(f"í˜¸ìŠ¤ë¨¸-ë ˜ì‡¼ ê²€ì •ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {hl_error}")
                        else:
                            col1, col2 = st.columns(2)
                            col1.metric("Chi-squared statistic", f"{hl_stat:.3f}")
                            col2.metric("p-value", f"{p_value_hl:.3f}")
                            st.caption("â€» p-valueê°€ 0.05ë³´ë‹¤ í¬ë©´ ëª¨ë¸ì´ ë°ì´í„°ì— ì˜ ì í•©í•œë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                    output_logistic = io.BytesIO()
                    with pd.ExcelWriter(output_logistic, engine='openpyxl') as writer:
                        publication_df.to_excel(writer, index=False)
                    st.download_button("ë¡œì§€ìŠ¤í‹± ë¶„ì„ ê²°ê³¼ ì—‘ì…€ ì €ì¥", output_logistic.getvalue(), "Logistic_Regression_Results.xlsx")

            except Exception as e:
                st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
