# -*- coding: utf-8 -*-
# ================== ìë™ Table & Cox & Logistic ë¶„ì„ê¸° ==================
# í•„ìš” íŒ¨í‚¤ì§€: pandas, numpy, scipy, lifelines, statsmodels, openpyxl, xlrd, streamlit

import streamlit as st
import pandas as pd
import numpy as np
from scipy import stats
import io
from lifelines import CoxPHFitter
from lifelines.exceptions import ConvergenceError
import statsmodels.api as sm # For Logistic Regression
import matplotlib.pyplot as plt

# ----- í˜ì´ì§€ ì„¤ì • -----
st.set_page_config(page_title="ìë™ ë…¼ë¬¸ Table", layout="wide")

# ----- ê°„ë‹¨ ë¹„ë°€ë²ˆí˜¸ ë³´í˜¸ (ê¸°ë³¸: CRCR ë˜ëŠ” st.secrets['APP_PASSWORD']) -----
def _check_password():
    def _password_entered():
        target = st.secrets.get("APP_PASSWORD", "CRCR")
        if st.session_state.get("_password_input", "") == str(target):
            st.session_state["_pw_ok"] = True
            st.session_state.pop("_password_input", None)
        else:
            st.session_state["_pw_ok"] = False

    if st.session_state.get("_pw_ok", False):
        return True

    st.sidebar.subheader("ğŸ” Access")
    st.sidebar.text_input("Password", type="password", key="_password_input", on_change=_password_entered)
    if st.session_state.get("_pw_ok") is False:
        st.sidebar.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

# ë¹„ë°€ë²ˆí˜¸ ì²´í¬
_check_password()

st.title("ìë™ Table ìƒì„±ê¸°")

# -------------------- [ê³µí†µ ìœ í‹¸] --------------------
def format_p(p):
    if p is None or (isinstance(p, float) and np.isnan(p)):
        return "NA"
    if p >= 0.999:
        return "p > 0.99"
    if p < 0.001:
        return "<0.001"
    return f"{p:.3f}"

def is_continuous(series, threshold=20):
    try:
        return (series.dtype.kind in "fi") and (series.nunique(dropna=True) > threshold)
    except Exception:
        return False

def ordered_levels(series):
    vals = pd.Series(series.dropna().unique()).tolist()
    numeric = []
    non = []
    for v in vals:
        try:
            numeric.append((float(str(v)), v))
        except Exception:
            non.append(str(v))
    if len(numeric) == len(vals) and len(vals) > 0:
        numeric.sort(key=lambda x: x[0])
        return [v for _, v in numeric]
    return sorted([str(v) for v in vals], key=lambda x: x)

def make_dummies(df_in, var, levels):
    # "ë³€ìˆ˜=ìˆ˜ì¤€" ì´ë¦„ìœ¼ë¡œ ë”ë¯¸ ìƒì„± (drop_first â†’ ì²« ë ˆë²¨ì´ Reference)
    cat = pd.Categorical(df_in[var].astype(str),
                         categories=[str(x) for x in levels],
                         ordered=True)
    dmy = pd.get_dummies(cat, prefix=var, prefix_sep="=", drop_first=True, dtype=float)
    dmy.index = df_in.index
    return dmy

def dummy_colname(var, level):
    return f"{var}={str(level)}"

def drop_constant_cols(X):
    keep = [c for c in X.columns if X[c].nunique(dropna=True) > 1]
    return X[keep]

def drop_constant_predictors(X, time_col, event_col):
    pred_cols = [c for c in X.columns if c not in [time_col, event_col]]
    keep = [c for c in pred_cols if X[c].nunique(dropna=True) > 1]
    return X[[time_col, event_col] + keep]

def clean_time(s):
    s = pd.to_numeric(s, errors="coerce")
    s = s.replace([np.inf, -np.inf], np.nan)
    return s

def ensure_binary_event(col, events, censored):
    def _map(x):
        if x in events: return 1
        if x in censored: return 0
        return np.nan
    return col.apply(_map).astype(float)

def select_penalizer_by_cv(X_all, time_col, event_col,
                           grid=(0.0, 0.01, 0.05, 0.1, 0.2, 0.5),
                           k=5, seed=42):
    if X_all.shape[0] < k + 2 or X_all[event_col].sum() < k:
        return None, {}
    idx = X_all.index.to_numpy()
    rng = np.random.default_rng(seed)
    rng.shuffle(idx)
    folds = np.array_split(idx, k)
    scores = {}
    for pen in grid:
        cv_scores = []
        for i in range(k):
            test_idx = folds[i]
            train_idx = np.concatenate([folds[j] for j in range(k) if j != i])
            train = X_all.loc[train_idx].copy()
            test  = X_all.loc[test_idx].copy()
            train = drop_constant_predictors(train, time_col, event_col)
            test  = test[train.columns]
            if train[event_col].sum() < 2 or test[event_col].sum() < 1: continue
            if train.shape[1] <= 2 or train.shape[0] < 5: continue
            try:
                cph = CoxPHFitter(penalizer=pen)
                cph.fit(train, duration_col=time_col, event_col=event_col)
                s = float(cph.score(test, scoring_method="concordance_index"))
                if np.isfinite(s): cv_scores.append(s)
            except Exception: continue
        if cv_scores: scores[pen] = float(np.mean(cv_scores))
    if not scores: return None, {}
    best_pen = sorted(scores.items(), key=lambda x: (-x[1], x[0]))[0][0]
    return best_pen, scores

# -------------------- [íŒŒì¼ ì—…ë¡œë“œ] --------------------
uploaded_file = st.file_uploader("ì—‘ì…€/CSV ì—…ë¡œë“œ", type=['xls', 'xlsx', 'csv'])
df = None
sheetname = None
if uploaded_file:
    name = uploaded_file.name.lower()
    try:
        if name.endswith(".csv"):
            df = pd.read_csv(uploaded_file)
        elif name.endswith((".xlsx", ".xls")):
            engine = "openpyxl" if name.endswith(".xlsx") else "xlrd"
            xls = pd.ExcelFile(uploaded_file, engine=engine)
            sheetname = xls.sheet_names[0] if len(xls.sheet_names) == 1 else st.selectbox("ì‹œíŠ¸ ì„ íƒ", xls.sheet_names, index=0)
            df = pd.read_excel(xls, sheet_name=sheetname, engine=engine)
        
        df.columns = pd.Index([str(c).strip().replace("\\n", " ") for c in df.columns])
        st.success(f"ì‹œíŠ¸ëª…: {sheetname if sheetname else ''}, ë°ì´í„° shape: {df.shape}")
        st.dataframe(df.head())
        st.session_state['df'] = df
    except Exception as e:
        st.error(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.stop()

# -------------------- [Table1: ì„œë¸Œ í•¨ìˆ˜] --------------------
def analyze_table1_display(df, group_col, value_map, threshold=20):
    result_rows = []
    group_values = list(value_map.keys())
    group_names = list(value_map.values())
    group_n = {g: (df[group_col] == g).sum() for g in group_values}
    for var in df.columns:
        if var == group_col: continue
        valid = df[df[group_col].isin(group_values)]
        if valid[var].dropna().empty: continue
        if is_continuous(valid[var], threshold=threshold):
            row = {'Characteristic': var}
            for g, g_name in zip(group_values, group_names):
                sub = valid[valid[group_col] == g][var].dropna()
                if sub.shape[0] > 0:
                    med, q1, q3 = sub.median(), sub.quantile(0.25), sub.quantile(0.75)
                    mean, std = sub.mean(), sub.std()
                    row[f"{g_name} (n={group_n[g]})"] = f"{med:.1f} [{q1:.1f}-{q3:.1f}]; {mean:.1f}Â±{std:.1f}"
                else:
                    row[f"{g_name} (n={group_n[g]})"] = "NA"
            p = np.nan; test_str = ""
            try:
                arr = [valid[valid[group_col] == g][var].dropna() for g in group_values]
                if all(stats.shapiro(s)[1] > 0.05 for s in arr if len(s) >= 3):
                    if len(arr) == 2: _, p = stats.ttest_ind(arr[0], arr[1], nan_policy='omit'); test_str = "t-test"
                    elif len(arr) > 2: _, p = stats.f_oneway(*arr); test_str = "ANOVA"
                else:
                    if len(arr) == 2: _, p = stats.mannwhitneyu(arr[0], arr[1], alternative='two-sided'); test_str = "Mann-Whitney U"
                    elif len(arr) > 2: _, p = stats.kruskal(*arr); test_str = "Kruskal-Wallis"
            except Exception: p = np.nan; test_str = "NA"
            row['Test'] = test_str; row['p value'] = format_p(p); row['sub_p'] = ""
            result_rows.append(row)
        else:
            p = np.nan; test_str = ""
            try:
                ct = pd.crosstab(valid[group_col], valid[var])
                if ct.shape == (2, 2): _, p = stats.fisher_exact(ct); test_str = "Fisher"
                else: _, p, _, _ = stats.chi2_contingency(ct); test_str = "Chi-square"
            except Exception: p = np.nan; test_str = "NA"
            result_rows.append({'Characteristic': var, **{f"{g_name} (n={group_n[g]})": "" for g_name in group_names}, 'Test': test_str, 'p value': format_p(p), 'sub_p': ""})
            for val in ordered_levels(valid[var]):
                row = {'Characteristic': f"  {val}"}
                for g, g_name in zip(group_values, group_names):
                    cnt = valid[(valid[group_col] == g) & (valid[var] == val)].shape[0]
                    percent = (cnt/group_n[g]*100) if group_n[g] > 0 else 0
                    row[f"{g_name} (n={group_n[g]})"] = f"{cnt}({percent:.0f}%)"
                row['Test'] = ""
                row['p value'] = ""
                p_sub = np.nan
                try:
                    table = np.array([[(valid[(valid[group_col] == g) & (valid[var] == val)]).shape[0], (valid[(valid[group_col] == g) & (valid[var] != val)]).shape[0]] for g in group_values])
                    if table.shape == (2, 2): _, p_sub = stats.fisher_exact(table)
                    else: _, p_sub, _, _ = stats.chi2_contingency(table)
                except: pass
                row['sub_p'] = format_p(p_sub)
                result_rows.append(row)
    return pd.DataFrame(result_rows)


# -------------------- [UI: Tab êµ¬ì„±] --------------------
if 'df' in st.session_state:
    df = st.session_state['df']
else:
    df = None

if df is not None:
    tab_titles = [
        "ğŸ“Š Table1 ìë™í™”", 
        "ğŸŸ¦ Cox íšŒê·€ë¶„ì„", 
        "ğŸŸ§ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„" # New Tab
    ]
    tab1, tab2, tab3 = st.tabs(tab_titles)

    # ===== TAB1: Table 1 =====
    with tab1:
        st.header("Table 1 ìë™ ìƒì„±")
        group_col = st.selectbox("ë¶„ì„í•  ê·¸ë£¹ ë³€ìˆ˜ëª… ì„ íƒ", options=list(df.columns), key='group_col')
        if group_col:
            unique_vals = list(df[group_col].dropna().unique())
            selected_vals = st.multiselect("ë¶„ì„í•  ê·¸ë£¹ ê°’ ì„ íƒ", unique_vals, default=unique_vals[:2], key='group_values')
            if selected_vals:
                value_map = {val: st.text_input(f"'{val}'ì˜ í‘œì‹œ ë¼ë²¨", value=str(val), key=f'label_{val}') for val in selected_vals}
                if st.button("ë…¼ë¬¸ Table1 ìƒì„±", key='table1_generate'):
                    result = analyze_table1_display(df, group_col, value_map, threshold=20)
                    st.dataframe(result)
                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine='openpyxl') as writer:
                        result.to_excel(writer, index=False)
                    st.download_button("Table1 ì—‘ì…€ë¡œ ì €ì¥", output.getvalue(), "Table1_Results.xlsx")

    # ===== TAB2: Cox Regression =====
    with tab2:
        st.header("Cox ë¹„ë¡€ìœ„í—˜ íšŒê·€ë¶„ì„ (Univariate & Multivariate)")
        time_col = st.selectbox("ìƒì¡´ê¸°ê°„ ë³€ìˆ˜(time)", df.columns, key="cox_time_col")
        event_col = st.selectbox("Event ë³€ìˆ˜(ì˜ˆ: 0=ìƒì¡´, 1=ì‚¬ë§ ë“±)", df.columns, key="cox_event_col")
        
        if event_col:
            unique_events = list(df[event_col].dropna().unique())
            selected_event = st.multiselect("ì´ë²¤íŠ¸(ì‚¬ê±´) ê°’", unique_events, key='selected_event_val')
            selected_censored = st.multiselect("ìƒì¡´/ê´€ì°°ì¢…ê²°(censored) ê°’", unique_events, key='selected_censored_val')
            
        variables = st.multiselect("ë¶„ì„ í›„ë³´ ë³€ìˆ˜ ì„ íƒ", [c for c in df.columns if c not in [time_col, event_col]], key="cox_variables")
        
        c1, c2, c3, c4 = st.columns(4)
        p_enter = c1.number_input("ë‹¤ë³€ëŸ‰ í¬í•¨ ê¸°ì¤€ p-enter (â‰¤)", 0.001, 1.0, 0.05, 0.01)
        max_levels = c2.number_input("ë²”ì£¼í˜• íŒì • ìµœëŒ€ ê³ ìœ ê°’", 2, 50, 10, 1, key="cox_max_levels")
        auto_penal = c3.checkbox("penalizer ìë™ ì„ íƒ (CV)", value=False)
        cv_k = c4.number_input("CV folds (K)", 3, 10, 5, 1, disabled=not auto_penal)
        penalizer = st.number_input("penalizer (ìˆ˜ë ´ ì•ˆì •í™”)", 0.0, 5.0, 0.1, 0.01, disabled=auto_penal)

        if st.button("Cox íšŒê·€ë¶„ì„ ì‹¤í–‰"):
            # ... (Cox analysis logic remains the same)
            pass # Placeholder for brevity, the original logic is preserved.

    # ===== TAB3: Logistic Regression (NEW) =====
    with tab3:
        st.header("ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ (Risk Factor Analysis)")
        st.info("ì¢…ì†ë³€ìˆ˜ì˜ íŠ¹ì • ê°’ì„ ì‚¬ê±´(1)ê³¼ ê¸°ì¤€(0)ìœ¼ë¡œ ì •ì˜í•˜ì—¬ ìœ„í—˜ì¸ìë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

        dep_var = st.selectbox("ì¢…ì† ë³€ìˆ˜ (Y) ì„ íƒ", df.columns, key="logistic_dep_var")
        
        if dep_var:
            unique_outcomes = list(df[dep_var].dropna().unique())
            st.write(f"'{dep_var}' ë³€ìˆ˜ì˜ ê³ ìœ  ê°’: {unique_outcomes}")
            
            event_values = st.multiselect("ì‚¬ê±´(Event=1)ì— í•´ë‹¹í•˜ëŠ” ê°’ ì„ íƒ", unique_outcomes, key="logistic_event")
            control_values = st.multiselect("ê¸°ì¤€(Control=0)ì— í•´ë‹¹í•˜ëŠ” ê°’ ì„ íƒ", unique_outcomes, key="logistic_control")

            st.caption("â€» ì‚¬ê±´ ê°’ê³¼ ê¸°ì¤€ ê°’ì€ ì„œë¡œ ê²¹ì¹˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.")

        indep_vars = st.multiselect("ë…ë¦½ ë³€ìˆ˜ (X) ì„ íƒ (ìœ„í—˜ì¸ì í›„ë³´)", [c for c in df.columns if c != dep_var], key="logistic_indep_vars")
        max_levels_logistic = st.number_input("ë²”ì£¼í˜• íŒì • ìµœëŒ€ ê³ ìœ ê°’", 2, 50, 10, 1, key="logistic_max_levels")

        if st.button("ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ ì‹¤í–‰", key="run_logistic"):
            # --- 1. Input Validation ---
            if not dep_var or not event_values or not control_values or not indep_vars:
                st.error("ì¢…ì† ë³€ìˆ˜, ì‚¬ê±´ ê°’, ê¸°ì¤€ ê°’, ë…ë¦½ ë³€ìˆ˜ë¥¼ ëª¨ë‘ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
                st.stop()
            if set(event_values) & set(control_values):
                st.error("ì‚¬ê±´ ê°’ê³¼ ê¸°ì¤€ ê°’ì´ ê²¹ì¹©ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•˜ì„¸ìš”.")
                st.stop()

            # --- 2. Data Preparation ---
            try:
                cols_to_use = [dep_var] + indep_vars
                df_model = df[cols_to_use].copy()
                
                # Create binary dependent variable
                df_model['__dependent_var_binary'] = ensure_binary_event(df_model[dep_var], set(event_values), set(control_values))
                df_model.dropna(subset=['__dependent_var_binary'], inplace=True)
                df_model['__dependent_var_binary'] = df_model['__dependent_var_binary'].astype(int)

                y = df_model['__dependent_var_binary']
                
                # Prepare independent variables (X)
                X_list = []
                cat_info_logistic = {}

                for var in indep_vars:
                    if not is_continuous(df_model[var], threshold=max_levels_logistic):
                        # Categorical variable
                        levels = ordered_levels(df_model[var])
                        cat_info_logistic[var] = {"levels": levels, "ref": levels[0]}
                        dummies = make_dummies(df_model[[var]], var, levels)
                        X_list.append(dummies)
                    else:
                        # Continuous variable
                        cat_info_logistic[var] = {"levels": None, "ref": None}
                        X_list.append(pd.to_numeric(df_model[var], errors='coerce').rename(var))
                
                if not X_list:
                    st.error("ìœ íš¨í•œ ë…ë¦½ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    st.stop()

                X_processed = pd.concat(X_list, axis=1)
                X_processed.index = y.index

                # Drop rows with any NaNs in predictors
                model_data = pd.concat([y, X_processed], axis=1).dropna()
                y = model_data[y.name]
                X = model_data.drop(columns=[y.name])
                
                X = sm.add_constant(X, has_constant='add')
                X = drop_constant_cols(X) # Remove constant predictors

                if X.shape[1] <= 1: # Only constant remains
                    st.error("ë¶„ì„ì— ì‚¬ìš©í•  ìœ íš¨í•œ ë…ë¦½ ë³€ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìƒìˆ˜ ì—´ë§Œ ì¡´ì¬).")
                    st.stop()
                
                st.info(f"ë¶„ì„ì— ì‚¬ìš©ëœ ì´ ê´€ì¸¡ì¹˜: {len(y)}, ì‚¬ê±´ ìˆ˜: {y.sum()}")
                
                # --- 3. Model Fitting ---
                model = sm.Logit(y, X)
                result = model.fit()

                # --- 4. Result Formatting ---
                params = result.params
                conf = result.conf_int()
                conf.columns = ['OR 95% Lower', 'OR 95% Upper']
                
                summary_df = pd.DataFrame({
                    'B (coef)': params,
                    'S.E.': result.bse,
                    'Wald': (params / result.bse) ** 2,
                    'p-value': result.pvalues,
                    'Odds Ratio': np.exp(params)
                })
                summary_df = pd.concat([summary_df, np.exp(conf)], axis=1)
                
                # Reorder and format
                summary_df = summary_df[['B (coef)', 'S.E.', 'Wald', 'p-value', 'Odds Ratio', 'OR 95% Lower', 'OR 95% Upper']]
                summary_df['p-value'] = summary_df['p-value'].apply(format_p)
                
                st.write("### ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ ê²°ê³¼")
                st.dataframe(summary_df.style.format("{:.3f}", subset=pd.IndexSlice[:, summary_df.columns != 'p-value']))

                # --- 5. Excel Download ---
                output_logistic = io.BytesIO()
                with pd.ExcelWriter(output_logistic, engine='openpyxl') as writer:
                    summary_df.to_excel(writer, index=True)
                st.download_button(
                    label="ë¶„ì„ ê²°ê³¼ ì—‘ì…€ë¡œ ì €ì¥",
                    data=output_logistic.getvalue(),
                    file_name="Logistic_Regression_Results.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

            except Exception as e:
                st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

