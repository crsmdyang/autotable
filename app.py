# -*- coding: utf-8 -*-

# ================== ìë™ Table & Cox ë¶„ì„ê¸° (penalizer Auto-CV í¬í•¨) ==================
# í•„ìš” íŒ¨í‚¤ì§€: pandas, numpy, scipy, lifelines, openpyxl, xlrd, streamlit, statsmodels
# ì„¤ì¹˜: pip install -U pandas numpy scipy lifelines openpyxl xlrd streamlit statsmodels

import io
import numpy as np
import pandas as pd
from scipy import stats
import streamlit as st
import matplotlib.pyplot as plt
from lifelines import CoxPHFitter
from lifelines.exceptions import ConvergenceError

# ----- í˜ì´ì§€ ì„¤ì • -----
st.set_page_config(page_title="ìë™ ë…¼ë¬¸ Table", layout="wide")

# ----- ê°„ë‹¨ ë¹„ë°€ë²ˆí˜¸ ë³´í˜¸ (ê¸°ë³¸: CRCR ë˜ëŠ” st.secrets['APP_PASSWORD']) -----
def _check_password():
    def _password_entered():
        target = st.secrets.get("APP_PASSWORD", "CRCR")
        if st.session_state.get("_password_input", "") == str(target):
            st.session_state["_pw_ok"] = True
            st.session_state.pop("_password_input", None)
        else:
            st.session_state["_pw_ok"] = False

    if st.session_state.get("_pw_ok", False):
        return True

    st.sidebar.subheader("ğŸ” Access")
    st.sidebar.text_input("Password", type="password", key="_password_input", on_change=_password_entered)
    if st.session_state.get("_pw_ok") is False:
        st.sidebar.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

# ë¹„ë°€ë²ˆí˜¸ ì²´í¬
_check_password()

st.title("ìë™ Table ìƒì„±ê¸°")


# -------------------- [ê³µí†µ ìœ í‹¸] --------------------
def format_p(p):
    if p is None or (isinstance(p, float) and np.isnan(p)):
        return "NA"
    if p >= 0.999:
        return "p > 0.99"
    if p < 0.001:
        return "<0.001"
    return f"{p:.3f}"

def is_continuous(series, threshold=20):
    try:
        return (series.dtype.kind in "fi") and (series.nunique(dropna=True) > threshold)
    except Exception:
        return False

def ordered_levels(series):
    vals = pd.Series(series.dropna().unique()).tolist()
    numeric, non = [], []
    for v in vals:
        try:
            numeric.append((float(str(v)), v))
        except Exception:
            non.append(str(v))
    if len(numeric) == len(vals) and len(vals) > 0:
        numeric.sort(key=lambda x: x[0])
        return [v for _, v in numeric]
    return sorted([str(v) for v in vals], key=lambda x: x)

def make_dummies(df_in, var, levels):
    # "ë³€ìˆ˜=ìˆ˜ì¤€" ì´ë¦„ìœ¼ë¡œ ë”ë¯¸ ìƒì„± (drop_first â†’ ì²« ë ˆë²¨ì´ Reference)
    cat = pd.Categorical(
        df_in[var].astype(str),
        categories=[str(x) for x in levels],
        ordered=True
    )
    dmy = pd.get_dummies(cat, prefix=var, prefix_sep="=", drop_first=True, dtype=float)
    dmy.index = df_in.index
    return dmy

def dummy_colname(var, level):
    return f"{var}={str(level)}"

def drop_constant_cols(X):
    keep = [c for c in X.columns if X[c].nunique(dropna=True) > 1]
    return X[keep]

def drop_constant_predictors(X, time_col, event_col):  # === CVìš© (time/eventëŠ” í•­ìƒ ìœ ì§€)
    pred_cols = [c for c in X.columns if c not in [time_col, event_col]]
    keep = [c for c in pred_cols if X[c].nunique(dropna=True) > 1]
    return X[[time_col, event_col] + keep]

def clean_time(s):
    s = pd.to_numeric(s, errors="coerce")
    s = s.replace([np.inf, -np.inf], np.nan)
    return s

def ensure_binary_event(col, events, censored):
    def _map(x):
        if x in events: return 1
        if x in censored: return 0
        return np.nan
    return col.apply(_map).astype(float)

# === NEW: penalizerë¥¼ CVë¡œ ì„ íƒ (C-index ìµœëŒ€í™”) ===
def select_penalizer_by_cv(
    X_all, time_col, event_col,
    grid=(0.0, 0.01, 0.05, 0.1, 0.2, 0.5),
    k=5, seed=42
):
    """
    X_all: duration, event, predictorsë¥¼ ëª¨ë‘ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„ (dropna/ìƒìˆ˜ì—´ ì œê±°ëœ ìƒíƒœ ê¶Œì¥)
    ë°˜í™˜: best_penalizer(or None), {penalizer: mean_cindex}
    """
    if X_all.shape[0] < k + 2 or X_all[event_col].sum() < k:
        return None, {}

    idx = X_all.index.to_numpy()
    rng = np.random.default_rng(seed)
    rng.shuffle(idx)
    folds = np.array_split(idx, k)

    scores = {}
    for pen in grid:
        cv_scores = []
        for i in range(k):
            test_idx = folds[i]
            train_idx = np.concatenate([folds[j] for j in range(k) if j != i])

            train = X_all.loc[train_idx].copy()
            test  = X_all.loc[test_idx].copy()

            # í•™ìŠµì…‹ì—ì„œ ìƒìˆ˜ predictor ì œê±°, ì—´ ì¼ì¹˜ ë§ì¶”ê¸°
            train = drop_constant_predictors(train, time_col, event_col)
            test  = test[train.columns]  # ê°™ì€ ì—´ ìˆœì„œ/êµ¬ì„± ìœ ì§€

            # ìœ íš¨ì„± ì²´í¬
            if train[event_col].sum() < 2 or test[event_col].sum() < 1:
                continue
            if train.shape[1] <= 2 or train.shape[0] < 5:
                continue

            try:
                cph = CoxPHFitter(penalizer=pen)
                cph.fit(train, duration_col=time_col, event_col=event_col)
                s = cph.score(test, scoring_method="concordance_index")
                s = float(s)
                if np.isfinite(s):
                    cv_scores.append(s)
            except Exception:
                continue

        if cv_scores:
            scores[pen] = float(np.mean(cv_scores))

    if not scores:
        return None, {}

    # ìµœê³  C-index, ë™ì ì´ë©´ ë” ì‘ì€ penalizer ì„ íƒ
    best_pen = sorted(scores.items(), key=lambda x: (-x[1], x[0]))[0][0]
    return best_pen, scores


# -------------------- [íŒŒì¼ ì—…ë¡œë“œ] --------------------
uploaded_file = st.file_uploader("ì—‘ì…€/CSV ì—…ë¡œë“œ", type=['xls', 'xlsx', 'csv'])

df = None
sheetname = None

if uploaded_file:
    name = uploaded_file.name.lower()
    if name.endswith(".csv"):
        df = pd.read_csv(uploaded_file)
    elif name.endswith(".xlsx"):
        xls = pd.ExcelFile(uploaded_file, engine="openpyxl")
        sheetname = xls.sheet_names[0] if len(xls.sheet_names) == 1 else st.selectbox("ì‹œíŠ¸ ì„ íƒ", xls.sheet_names, index=0)
        df = pd.read_excel(xls, sheet_name=sheetname, engine="openpyxl")
    elif name.endswith(".xls"):
        xls = pd.ExcelFile(uploaded_file, engine="xlrd")
        sheetname = xls.sheet_names[0] if len(xls.sheet_names) == 1 else st.selectbox("ì‹œíŠ¸ ì„ íƒ", xls.sheet_names, index=0)
        df = pd.read_excel(xls, sheet_name=sheetname, engine="xlrd")
    else:
        st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. csv/xls/xlsxë§Œ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        st.stop()

    # ì»¬ëŸ¼ ê³µë°±/ê°œí–‰ ì œê±° & ì¤‘ë³µ ë°©ì§€
    df.columns = pd.Index([str(c).strip() for c in df.columns]).map(lambda x: x.replace("\n", " ").strip())
    st.success(f"ì‹œíŠ¸ëª…: {sheetname if sheetname else ''}, ë°ì´í„° shape: {df.shape}")
    st.dataframe(df.head())
    st.session_state['df'] = df


# -------------------- [Table1: ì„œë¸Œ í•¨ìˆ˜] --------------------
def calc_subgroup_p(valid, group_col, var, val, group_values):
    table = []
    for g in group_values:
        sub = valid[valid[group_col] == g][var]
        count_val = (sub == val).sum()
        count_else = (sub != val).sum()
        table.append([count_val, count_else])
    table = np.array(table)
    try:
        if table.shape == (2, 2):
            _, p = stats.fisher_exact(table)
        else:
            _, p, _, _ = stats.chi2_contingency(table)
    except Exception:
        p = np.nan
    return format_p(p)

def analyze_table1_display(df, group_col, value_map, threshold=20):
    result_rows = []
    group_values = list(value_map.keys())
    group_names  = list(value_map.values())
    group_n = {g: (df[group_col] == g).sum() for g in group_values}

    for var in df.columns:
        if var == group_col:
            continue
        valid = df[df[group_col].isin(group_values)]
        if valid[var].dropna().empty:
            continue

        # ì—°ì†í˜•
        if is_continuous(valid[var], threshold=threshold):
            row = {'Characteristic': var}
            for g, g_name in zip(group_values, group_names):
                sub = valid[valid[group_col] == g][var].dropna()
                n = sub.shape[0]
                if n > 0:
                    med, q1, q3 = sub.median(), sub.quantile(0.25), sub.quantile(0.75)
                    mean, std = sub.mean(), sub.std()
                    row[f"{g_name} (n={group_n[g]})"] = f"{med:.1f} [{q1:.1f}-{q3:.1f}]; {mean:.1f}Â±{std:.1f}"
                else:
                    row[f"{g_name} (n={group_n[g]})"] = "NA"
            # ê²€ì •
            p = np.nan; test_str = ""
            try:
                arr = [valid[valid[group_col] == g][var].dropna() for g in group_values]
                normal_flags = []
                for vals in arr:
                    if len(vals) >= 3:
                        try:
                            p_norm = stats.shapiro(vals)[1]
                        except Exception:
                            p_norm = 0
                        normal_flags.append(p_norm > 0.05)
                    else:
                        normal_flags.append(False)
                if all(normal_flags):
                    if len(arr) == 2:
                        _, p = stats.ttest_ind(arr[0], arr[1], nan_policy='omit'); test_str = "t-test"
                    elif len(arr) > 2:
                        _, p = stats.f_oneway(*arr); test_str = "ANOVA"
                else:
                    if len(arr) == 2:
                        _, p = stats.mannwhitneyu(arr[0], arr[1], alternative='two-sided'); test_str = "Mann-Whitney U"
                    elif len(arr) > 2:
                        _, p = stats.kruskal(*arr); test_str = "Kruskal-Wallis"
            except Exception:
                p = np.nan; test_str = "NA"
            row['Test'] = test_str
            row['p value'] = format_p(p)
            row['sub_p'] = ""
            result_rows.append(row)
            continue

        # ë²”ì£¼í˜•
        vlist = pd.Series(valid[var].dropna().unique())
        if len(vlist) >= threshold:
            row = {'Characteristic': var}
            for g, g_name in zip(group_values, group_names):
                sub = valid[valid[group_col] == g][var]
                vc = sub.value_counts()
                valstr = "; ".join([f"{idx}={cnt}({(cnt/group_n[g]*100):.0f}%)" for idx, cnt in vc.items()]) if group_n[g] > 0 else "NA"
                row[f"{g_name} (n={group_n[g]})"] = valstr if len(vc) > 0 else "NA"
            # ì „ì²´ p
            p = np.nan; test_str = ""
            try:
                ct = pd.crosstab(valid[group_col], valid[var])
                if ct.shape == (2, 2):
                    _, p = stats.fisher_exact(ct); test_str = "Fisher"
                else:
                    _, p, _, _ = stats.chi2_contingency(ct); test_str = "Chi-square"
            except Exception:
                p = np.nan; test_str = "NA"
            row['Test'] = test_str
            row['p value'] = format_p(p)
            row['sub_p'] = ""
            result_rows.append(row)
        else:
            # í—¤ë” í–‰
            p = np.nan; test_str = ""
            try:
                ct = pd.crosstab(valid[group_col], valid[var])
                if ct.shape == (2, 2):
                    _, p = stats.fisher_exact(ct); test_str = "Fisher"
                else:
                    _, p, _, _ = stats.chi2_contingency(ct); test_str = "Chi-square"
            except Exception:
                p = np.nan; test_str = "NA"
            first_row = {'Characteristic': var}
            for g, g_name in zip(group_values, group_names):
                first_row[f"{g_name} (n={group_n[g]})"] = ""
            first_row['Test'] = test_str
            first_row['p value'] = format_p(p)
            first_row['sub_p'] = ""
            result_rows.append(first_row)

            for val in vlist:
                row = {'Characteristic': f"  {val}"}
                for g, g_name in zip(group_values, group_names):
                    cnt = valid[(valid[group_col] == g) & (valid[var] == val)].shape[0]
                    percent = (cnt/group_n[g]*100) if group_n[g] > 0 else 0
                    row[f"{g_name} (n={group_n[g]})"] = f"{cnt}({percent:.0f}%)"
                row['Test'] = ""
                row['p value'] = ""
                row['sub_p'] = calc_subgroup_p(valid, group_col, var, val, group_values)
                result_rows.append(row)

    return pd.DataFrame(result_rows)


# -------------------- [UI: Tab êµ¬ì„±] --------------------
df = st.session_state.get('df')
if df is not None:
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š Table1 ìë™í™”", "ğŸŸ¦ Cox íšŒê·€ë¶„ì„ (Univariate/Multivariate)", "ğŸŸ§ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ (Risk Factors)"])

    # ===== TAB1 =====
    with tab1:
        st.header("2ë‹¨ê³„: Table ìë™í™” (ê°’ ì§ì ‘ ì„ íƒ/ë¼ë²¨/í–‰ë¶„ë¦¬/ìš”ì•½ ì§€ì›)")
        st.info(
            "ğŸ“Œ ì—°ì†í˜•/ë²”ì£¼í˜• ìë™ ë¶„ë¥˜: ê³ ìœ ê°’ 20ê°œ ì´ˆê³¼ â†’ ì—°ì†í˜•, ì´í•˜ëŠ” ë²”ì£¼í˜•.\n"
            "ê²°ê³¼ê°€ ìƒì‹ê³¼ ë‹¤ë¥´ë©´ ì§ì ‘ ë³€ìˆ˜ íƒ€ì…ì„ í™•ì¸í•˜ì„¸ìš”."
        )

        candidate_cols = list(df.columns)
        group_col = st.selectbox("ë¶„ì„í•  ê·¸ë£¹ ë³€ìˆ˜ëª…ì„ ì„ íƒí•˜ì„¸ìš”", options=candidate_cols, key='group_col')
        value_map = {}

        if group_col and group_col in df.columns:
            unique_vals = list(df[group_col].dropna().unique())
            selected_vals = st.multiselect("ë¶„ì„í•  ê°’ì„ ì„ íƒí•˜ì„¸ìš”", unique_vals, default=unique_vals[:2], key='group_values')
            if selected_vals:
                col1, col2 = st.columns([2, 6])
                for val in selected_vals:
                    with col1:
                        st.write(f"ê°’: {val}")
                    with col2:
                        label = st.text_input(f"í•´ë‹¹ ê°’ì˜ í‘œì‹œ ë¼ë²¨", value=str(val), key=f'label_{val}')
                        value_map[val] = label

                if st.button("ë…¼ë¬¸ Table1 ìƒì„±", key='table1_generate'):
                    target_df = df.dropna(subset=[group_col])
                    result = analyze_table1_display(target_df, group_col, value_map, threshold=20)
                    st.dataframe(result, use_container_width=True)

                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine='openpyxl') as writer:
                        result.to_excel(writer, index=False)
                    st.download_button(
                        label="Table1 ì—‘ì…€ë¡œ ì €ì¥",
                        data=output.getvalue(),
                        file_name="ë…¼ë¬¸ìš©_Table1.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
            else:
                st.warning("ë¶„ì„í•  ê°’ì„ ìµœì†Œ 1ê°œ ì´ìƒ ì„ íƒí•´ ì£¼ì„¸ìš”.")
        elif group_col:
            st.error("í•´ë‹¹ ë³€ìˆ˜ëª…ì´ ë°ì´í„°ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # ===== TAB2 =====
    with tab2:
        st.header("ë…¼ë¬¸ Table: Factor / Subgroup / HR(95%CI) / p-value (Univariate & Multivariate)")

        time_col  = st.selectbox("ìƒì¡´ê¸°ê°„ ë³€ìˆ˜(time)", df.columns, key="cox_time_col")
        event_col = st.selectbox("Event ë³€ìˆ˜(ì˜ˆ: 0=ìƒì¡´, 1=ì‚¬ë§ ë“±)", df.columns, key="cox_event_col")

        temp_df = df.copy()
        if event_col:
            unique_events = list(df[event_col].dropna().unique())
            st.write(f"ì´ ë³€ìˆ˜ì˜ ì‹¤ì œ ê°’: {unique_events}")
            selected_event    = st.multiselect("ì´ë²¤íŠ¸(ì‚¬ê±´) ê°’", unique_events, key='selected_event_val')
            selected_censored = st.multiselect("ìƒì¡´/ê´€ì°°ì¢…ê²°(censored) ê°’", unique_events, key='selected_censored_val')
            st.caption("â€» ì‚¬ê±´ê°’ê³¼ ê²€ì—´ê°’ì€ ì„œë¡œ ê²¹ì¹˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.")
            temp_df["__event_for_cox"] = ensure_binary_event(temp_df[event_col], set(selected_event), set(selected_censored))
        else:
            temp_df["__event_for_cox"] = np.nan

        candidate_vars = [c for c in df.columns if c not in [time_col, event_col]]
        variables = st.multiselect("ë¶„ì„ í›„ë³´ ë³€ìˆ˜ ì„ íƒ", candidate_vars, key="cox_variables")

        c1, c2, c3, c4 = st.columns([1, 1, 1, 1])
        with c1:
            p_enter = st.number_input("ë‹¤ë³€ëŸ‰ í¬í•¨ ê¸°ì¤€ p-enter (â‰¤)", min_value=0.001, max_value=1.0, value=0.05, step=0.01)
        with c2:
            max_levels = st.number_input("ë²”ì£¼í˜• íŒì • ìµœëŒ€ ê³ ìœ ê°’", min_value=2, max_value=50, value=10, step=1)
        with c3:
            auto_penal = st.checkbox("penalizer ìë™ ì„ íƒ (CV, C-index)", value=False)  # === NEW
        with c4:
            cv_k = st.number_input("CV folds (K)", min_value=3, max_value=10, value=5, step=1, disabled=not auto_penal)  # === NEW

        penalizer = st.number_input("penalizer (ìˆ˜ë ´ ì•ˆì •í™”)", min_value=0.0, max_value=5.0, value=0.1, step=0.1, disabled=auto_penal)

        def basic_clean(df_in, time_col):
            out = df_in.copy()
            out[time_col] = clean_time(out[time_col])
            out = out[out[time_col] > 0]
            out = out.replace([np.inf, -np.inf], np.nan)
            return out

        if st.button("ë¶„ì„ ì‹¤í–‰"):
            # í•„ìˆ˜ ê²€ì¦
            if not selected_event or not selected_censored:
                st.error("ì‚¬ê±´ê°’ê³¼ ê²€ì—´ê°’ì„ ê°ê° ìµœì†Œ 1ê°œ ì´ìƒ ì„ íƒí•˜ì„¸ìš”.")
                st.stop()
            if set(selected_event) & set(selected_censored):
                st.error("ì‚¬ê±´ê°’ê³¼ ê²€ì—´ê°’ì´ ê²¹ì¹©ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•˜ì„¸ìš”.")
                st.stop()

            temp_df2 = basic_clean(temp_df, time_col).dropna(subset=[time_col, "__event_for_cox"])
            n_events = int(temp_df2["__event_for_cox"].sum())
            n_total  = temp_df2.shape[0]
            st.info(f"ì´ ê´€ì¸¡ì¹˜: {n_total}, ì´ë²¤íŠ¸ ìˆ˜: {n_events}")
            if n_events < 5:
                st.warning("ì´ë²¤íŠ¸ ìˆ˜ê°€ <5ë¡œ ë§¤ìš° ì ìŠµë‹ˆë‹¤. ì¶”ì •ì´ ë¶ˆì•ˆì •í•˜ê±°ë‚˜ ëª¨ë¸ì´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            # ---------- 1) Univariate ----------
            uni_sum_dict = {}
            uni_na_vars  = []
            cat_info     = {}

            for var in variables:
                try:
                    dat_raw = temp_df2[[time_col, "__event_for_cox", var]].copy()
                    dat_raw = dat_raw.dropna(subset=[var])
                    if dat_raw.empty:
                        uni_na_vars.append(var); continue

                    if (dat_raw[var].dtype == "object") or (dat_raw[var].nunique(dropna=True) <= max_levels):
                        lvls = ordered_levels(dat_raw[var])
                        if len(lvls) < 2:
                            uni_na_vars.append(var); continue
                        cat_info[var] = {"levels": lvls, "ref": lvls[0]}
                        dmy = make_dummies(dat_raw, var, lvls)
                        dat = pd.concat([dat_raw[[time_col, "__event_for_cox"]], dmy], axis=1)
                    else:
                        cat_info[var] = {"levels": None, "ref": None}
                        dat = dat_raw[[time_col, "__event_for_cox", var]].copy()
                        dat[var] = pd.to_numeric(dat[var], errors="coerce")

                    dat = dat.dropna()
                    dat = drop_constant_cols(dat)
                    if (dat.shape[0] < 3) or (dat["__event_for_cox"].sum() < 1) or (dat.shape[1] <= 2):
                        uni_na_vars.append(var); continue

                    cph = CoxPHFitter(penalizer=penalizer)  # UnivariateëŠ” ì…ë ¥ penalizer ì‚¬ìš©
                    cph.fit(dat, duration_col=time_col, event_col="__event_for_cox")
                    uni_sum_dict[var] = cph.summary.copy()
                except ConvergenceError:
                    uni_na_vars.append(var)
                except Exception:
                    uni_na_vars.append(var)

            # ë³€ìˆ˜ì„ íƒ
            univariate_pvals = {}
            for var, summ in uni_sum_dict.items():
                if cat_info[var]["levels"] is None:
                    if var in summ.index:
                        univariate_pvals[var] = float(summ.loc[var, "p"])
                else:
                    p_min = None
                    for _, row in summ.iterrows():
                        p = float(row["p"])
                        p_min = p if p_min is None else min(p_min, p)
                    if p_min is not None:
                        univariate_pvals[var] = p_min

            selected_vars = [v for v, p in univariate_pvals.items() if p <= p_enter]
            st.write(f"ë‹¤ë³€ëŸ‰ í›„ë³´ ë³€ìˆ˜(â‰¤ {p_enter:.3f}): {selected_vars if selected_vars else 'ì—†ìŒ'}")

            # ---------- 2) Multivariate ----------
            multi_sum = None
            multi_na_vars = []
            chosen_penalizer = penalizer  # ê¸°ë³¸ê°’

            if len(selected_vars) >= 1:
                try:
                    dat_base = temp_df2[[time_col, "__event_for_cox"]].copy()
                    X_list = []
                    for var in selected_vars:
                        if cat_info.get(var, {}).get("levels") is None:
                            xi = pd.to_numeric(temp_df2[var], errors="coerce").to_frame(var)
                        else:
                            lvls = cat_info[var]["levels"]
                            if len(lvls) < 2:
                                continue
                            xi = make_dummies(temp_df2[[var]], var, lvls)
                        X_list.append(xi)

                    if not X_list:
                        multi_na_vars = selected_vars
                    else:
                        X_all = pd.concat([dat_base] + X_list, axis=1).dropna()
                        X_all = drop_constant_predictors(X_all, time_col, "__event_for_cox")

                        # === NEW: Auto-CVë¡œ penalizer ì„ íƒ ===
                        if auto_penal and X_all["__event_for_cox"].sum() >= int(cv_k):
                            pen_grid = (0.0, 0.01, 0.05, 0.1, 0.2, 0.5)
                            best_pen, pen_scores = select_penalizer_by_cv(
                                X_all, time_col, "__event_for_cox",
                                grid=pen_grid, k=int(cv_k), seed=42
                            )
                            if best_pen is not None:
                                chosen_penalizer = float(best_pen)
                                st.success(f"Auto-CV ì„ íƒ penalizer = {chosen_penalizer} (í‰ê·  C-index ê¸°ì¤€)")
                                st.caption(f"Grid ì„±ëŠ¥: { {k: round(v,4) for k,v in pen_scores.items()} }")
                            else:
                                st.warning("CVë¡œ penalizerë¥¼ ê²°ì •í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì…ë ¥ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

                        if (X_all.shape[0] >= 3) and (X_all["__event_for_cox"].sum() >= 1) and (X_all.shape[1] > 2):
                            cph_multi = CoxPHFitter(penalizer=chosen_penalizer)
                            cph_multi.fit(X_all, duration_col=time_col, event_col="__event_for_cox")
                            multi_sum = cph_multi.summary.copy()
                        else:
                            multi_na_vars = selected_vars
                except ConvergenceError:
                    multi_na_vars = selected_vars
                except Exception:
                    multi_na_vars = selected_vars

            # ---------- 3) ì¶œë ¥ í…Œì´ë¸” ----------
            rows = []
            for var in variables:
                rows.append({
                    "Factor": var, "Subgroup": "",
                    "Univariate analysis HR (95% CI)": "", "Univariate analysis p-Value": "",
                    "Multivariate analysis HR (95% CI)": "", "Multivariate analysis p-Value": ""
                })

                # ì™„ì „ ì‹¤íŒ¨
                if (var in uni_na_vars) and ((multi_sum is None) or (var in multi_na_vars)):
                    rows.append({
                        "Factor": "", "Subgroup": "(insufficient / skipped)",
                        "Univariate analysis HR (95% CI)": "NA", "Univariate analysis p-Value": "NA",
                        "Multivariate analysis HR (95% CI)": "NA", "Multivariate analysis p-Value": "NA"
                    })
                    continue

                # ë²”ì£¼í˜•
                if cat_info.get(var, {}).get("levels") is not None:
                    lvls = cat_info[var]["levels"]; ref = cat_info[var]["ref"]
                    rows.append({
                        "Factor": "", "Subgroup": f"{ref} (Reference)",
                        "Univariate analysis HR (95% CI)": "Ref.", "Univariate analysis p-Value": "",
                        "Multivariate analysis HR (95% CI)": "Ref.", "Multivariate analysis p-Value": ""
                    })
                    for lv in lvls[1:]:
                        colname = dummy_colname(var, lv)
                        # Uni
                        if (var in uni_na_vars) or (var not in uni_sum_dict) or (colname not in uni_sum_dict[var].index):
                            hr_uni, p_uni = "NA", "NA"
                        else:
                            r = uni_sum_dict[var].loc[colname]
                            hr_uni = f"{r['exp(coef)']:.3f} ({r['exp(coef) lower 95%']:.3f}-{r['exp(coef) upper 95%']:.3f})"
                            p_uni = format_p(float(r['p']))
                        # Multi
                        if (multi_sum is None) or (var in multi_na_vars) or (colname not in (multi_sum.index if multi_sum is not None else [])):
                            hr_multi, p_multi = "NA", "NA"
                        else:
                            r = multi_sum.loc[colname]
                            hr_multi = f"{r['exp(coef)']:.3f} ({r['exp(coef) lower 95%']:.3f}-{r['exp(coef) upper 95%']:.3f})"
                            p_multi = format_p(float(r['p']))
                        rows.append({
                            "Factor": "", "Subgroup": str(lv),
                            "Univariate analysis HR (95% CI)": hr_uni, "Univariate analysis p-Value": p_uni,
                            "Multivariate analysis HR (95% CI)": hr_multi, "Multivariate analysis p-Value": p_multi
                        })

                # ì—°ì†í˜•
                else:
                    if (var not in uni_sum_dict) or (var in uni_na_vars) or (var not in uni_sum_dict[var].index if var in uni_sum_dict else True):
                        hr_uni, p_uni = "NA", "NA"
                    else:
                        r = uni_sum_dict[var].loc[var]
                        hr_uni = f"{r['exp(coef)']:.3f} ({r['exp(coef) lower 95%']:.3f}-{r['exp(coef) upper 95%']:.3f})"
                        p_uni = format_p(float(r['p']))

                    if (multi_sum is None) or (var in multi_na_vars) or (var not in (multi_sum.index if multi_sum is not None else [])):
                        hr_multi, p_multi = "NA", "NA"
                    else:
                        r = multi_sum.loc[var]
                        hr_multi = f"{r['exp(coef)']:.3f} ({r['exp(coef) lower 95%']:.3f}-{r['exp(coef) upper 95%']:.3f})"
                        p_multi = format_p(float(r['p']))

                    rows.append({
                        "Factor": "", "Subgroup": "",
                        "Univariate analysis HR (95% CI)": hr_uni, "Univariate analysis p-Value": p_uni,
                        "Multivariate analysis HR (95% CI)": hr_multi, "Multivariate analysis p-Value": p_multi
                    })

            result_table = pd.DataFrame(rows)
            st.write("**ë…¼ë¬¸ ì œì¶œìš© í…Œì´ë¸” (Univariate/Multivariate ë³‘ë ¬, Reference, Factor/ìˆ˜ì¤€êµ¬ì¡°)**")
            if auto_penal and len(selected_vars) >= 1:
                st.caption(f"*ë‹¤ë³€ëŸ‰ ìµœì¢… penalizer: {chosen_penalizer}*")
            st.dataframe(result_table, use_container_width=True)

            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                result_table.to_excel(writer, index=False)
            st.download_button(
                label="Cox ê²°ê³¼ ì—‘ì…€ë¡œ ì €ì¥",
                data=output.getvalue(),
                file_name="Cox_Regression_Results_Table.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

    # ===== TAB3: Logistic Regression (NEW) =====
    with tab3:
        # --- ì§€ì—­ ì„í¬íŠ¸: ìƒë‹¨ ì„í¬íŠ¸ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤ ---
        import statsmodels.api as sm
        from statsmodels.discrete.discrete_model import PerfectSeparationError

        st.header("ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ (Risk Factor Analysis)")
        st.info("ì¢…ì†ë³€ìˆ˜ì˜ íŠ¹ì • ê°’ì„ ì‚¬ê±´(1)ê³¼ ê¸°ì¤€(0)ìœ¼ë¡œ ì •ì˜í•˜ì—¬ ìœ„í—˜ì¸ìë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. Cox í…Œì´ë¸”ê³¼ ë™ì¼í•œ ë…¼ë¬¸ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë§Œë“­ë‹ˆë‹¤.")

        # ---- Hosmerâ€“Lemeshow ê²€ì • í•¨ìˆ˜ (íƒ­ ë‚´ë¶€ì— ì •ì˜; ì „ì—­ ì˜¤ì—¼ ë°©ì§€) ----
        def calculate_hosmer_lemeshow(y_true, y_prob, g=10):
            """Return (HL_stat, p_value, error_message). g=number of bins."""
            import numpy as np
            from scipy import stats as _stats
            try:
                y_true = np.asarray(y_true, dtype=float)
                y_prob = np.asarray(y_prob, dtype=float)
                mask = np.isfinite(y_true) & np.isfinite(y_prob)
                y_true, y_prob = y_true[mask], y_prob[mask]
                if y_true.size < 20:
                    return None, None, "í‘œë³¸ ìˆ˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤(<20)."
                # deciles by predicted probability
                try:
                    bins = pd.qcut(y_prob, q=min(g, max(2, np.unique(y_prob).size)), duplicates='drop')
                except Exception:
                    q = min(g, 5)
                    bins = pd.qcut(y_prob, q=q, duplicates='drop')
                df_hl = pd.DataFrame({'y': y_true, 'p': y_prob, 'bin': bins})
                grp = df_hl.groupby('bin', observed=True)
                O1 = grp['y'].sum()
                N  = grp['y'].count()
                Pbar = grp['p'].mean()
                E1 = N * Pbar
                O0 = N - O1
                E0 = N - E1
                eps = 1e-12
                HL = (((O1 - E1)**2)/(E1 + eps) + ((O0 - E0)**2)/(E0 + eps)).sum()
                df = max(1, len(grp) - 2)
                p  = 1 - _stats.chi2.cdf(HL, df)
                return float(HL), float(p), None
            except Exception as e:
                return None, None, f"HL ê³„ì‚° ì˜¤ë¥˜: {type(e).__name__}: {e}"

        # ---- UI ----
        dep_var = st.selectbox("ì¢…ì† ë³€ìˆ˜ (Y) ì„ íƒ", df.columns, key="logistic_dep_var")

        if dep_var:
            unique_outcomes = list(df[dep_var].dropna().unique())
            st.write(f"'{dep_var}' ë³€ìˆ˜ì˜ ê³ ìœ  ê°’: {unique_outcomes}")
            event_values   = st.multiselect("ì‚¬ê±´(Event=1)ì— í•´ë‹¹í•˜ëŠ” ê°’ ì„ íƒ", unique_outcomes, key="logistic_event")
            control_values = st.multiselect("ê¸°ì¤€(Control=0)ì— í•´ë‹¹í•˜ëŠ” ê°’ ì„ íƒ", unique_outcomes, key="logistic_control")
            st.caption("â€» ì‚¬ê±´ ê°’ê³¼ ê¸°ì¤€ ê°’ì€ ì„œë¡œ ê²¹ì¹˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.")
        else:
            event_values, control_values = [], []

        indep_vars = st.multiselect(
            "ë…ë¦½ ë³€ìˆ˜ (X) ì„ íƒ (ìœ„í—˜ì¸ì í›„ë³´)", [c for c in df.columns if c != dep_var], key="logistic_indep_vars"
        )

        c1_log, c2_log = st.columns(2)
        p_enter_logistic = c1_log.number_input(
            "ë‹¤ë³€ëŸ‰ í¬í•¨ ê¸°ì¤€ p-enter (â‰¤)", min_value=0.001, max_value=1.0, value=0.05, step=0.01, key='logistic_p_enter'
        )
        max_levels_logistic = c2_log.number_input(
            "ë²”ì£¼í˜• íŒì • ìµœëŒ€ ê³ ìœ ê°’", min_value=2, max_value=50, value=10, step=1, key="logistic_max_levels"
        )

        if st.button("ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ ì‹¤í–‰", key="run_logistic"):
            # ---- í•„ìˆ˜ ê²€ì¦ ----
            if not dep_var or not event_values or not control_values or not indep_vars:
                st.error("ì¢…ì† ë³€ìˆ˜, ì‚¬ê±´ ê°’, ê¸°ì¤€ ê°’, ë…ë¦½ ë³€ìˆ˜ë¥¼ ëª¨ë‘ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
                st.stop()
            if set(event_values) & set(control_values):
                st.error("ì‚¬ê±´ ê°’ê³¼ ê¸°ì¤€ ê°’ì´ ê²¹ì¹©ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•˜ì„¸ìš”.")
                st.stop()

            try:
                with st.spinner('ë¶„ì„ì„ ìˆ˜í–‰ ì¤‘ì…ë‹ˆë‹¤...'):
                    # ---- ë°ì´í„° ì¤€ë¹„: Y ì´ì§„í™” ----
                    cols_to_use = [dep_var] + indep_vars
                    df_model = df[cols_to_use].copy()
                    df_model['__y_bin'] = ensure_binary_event(df_model[dep_var], set(event_values), set(control_values))
                    df_model.dropna(subset=['__y_bin'], inplace=True)
                    df_model['__y_bin'] = df_model['__y_bin'].astype(int)

                    # ---- X ì „ì²˜ë¦¬ (ë²”ì£¼í˜• ë”ë¯¸, ì—°ì†í˜• ìˆ«ìí™”) ----
                    X_list, cat_info_logistic = [], {}
                    for var in indep_vars:
                        # ë²”ì£¼í˜• íŒì •: ê³ ìœ ê°’ â‰¤ max_levels OR dtype=object â†’ ë²”ì£¼í˜•
                        if (df_model[var].dtype == 'object') or (df_model[var].nunique(dropna=True) <= max_levels_logistic):
                            levels = ordered_levels(df_model[var])
                            if len(levels) < 2:
                                continue
                            cat_info_logistic[var] = {"levels": levels, "ref": levels[0]}
                            X_list.append(make_dummies(df_model[[var]], var, levels))  # drop_first=True ë‚´ë¶€ êµ¬í˜„
                        else:
                            cat_info_logistic[var] = {"levels": None, "ref": None}
                            X_list.append(pd.to_numeric(df_model[var], errors='coerce').rename(var))

                    if not X_list:
                        st.error("ìœ íš¨í•œ ë…ë¦½ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        st.stop()

                    X_processed = pd.concat(X_list, axis=1)
                    X_processed = X_processed.loc[:, X_processed.nunique(dropna=True) > 1]  # ìƒìˆ˜ ì—´ ì œê±°
                    model_data = pd.concat([df_model['__y_bin'], X_processed], axis=1).dropna()
                    y_final = model_data['__y_bin']
                    X_final = model_data.drop(columns=['__y_bin'])
                    X_final = sm.add_constant(X_final, has_constant='add')  # const ì¶”ê°€

                    if X_final.shape[1] <= 1 or len(np.unique(y_final)) < 2:
                        st.error("ë¶„ì„ì— ì‚¬ìš©í•  ìœ íš¨í•œ ë…ë¦½ ë³€ìˆ˜/ë°˜ì‘ê°’ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                        st.stop()

                    st.info(f"ë¶„ì„ì— ì‚¬ìš©ëœ ì´ ê´€ì¸¡ì¹˜: {int(len(y_final))}, ì‚¬ê±´ ìˆ˜: {int(y_final.sum())}")

                    # ---------- 1) Univariate ----------
                    uni_results = {}           # {var: fitted_result}
                    univariate_pmins = {}      # {var: min p among its terms}
                    uni_fail_reasons = {}      # {var: reason}

                    for var in indep_vars:
                        try:
                            # í•´ë‹¹ ë³€ìˆ˜ì˜ ëª¨ë“  ì»¬ëŸ¼(ì—°ì†í˜• 1ê°œ ë˜ëŠ” ë”ë¯¸ë“¤)ë§Œ ì¶”ì¶œ
                            var_cols = [c for c in X_final.columns if (c == var) or c.startswith(f"{var}=")]
                            if not var_cols:
                                continue
                            X_uni = X_final[['const'] + var_cols].copy()
                            y_uni = y_final.loc[X_uni.index]
                            # ìœ íš¨ì„±
                            if X_uni.shape[1] <= 1 or len(np.unique(y_uni)) < 2:
                                uni_fail_reasons[var] = "ìœ íš¨í•˜ì§€ ì•Šì€ ì„¤ê³„í–‰ë ¬/ë°˜ì‘ê°’"
                                continue
                            # ì í•© (newton â†’ ì‹¤íŒ¨ ì‹œ lbfgs ì¬ì‹œë„)
                            try:
                                res = sm.Logit(y_uni, X_uni).fit(method='newton', disp=0)
                            except Exception:
                                res = sm.Logit(y_uni, X_uni).fit(method='lbfgs', disp=0)
                            uni_results[var] = res
                            # min p (const ì œì™¸)
                            pvals = [res.pvalues[c] for c in res.pvalues.index if c != 'const']
                            if pvals:
                                univariate_pmins[var] = float(np.nanmin(pvals))
                        except PerfectSeparationError:
                            uni_fail_reasons[var] = "ë°ì´í„° ì™„ì „ ë¶„ë¦¬(Perfect Separation)"
                        except np.linalg.LinAlgError:
                            uni_fail_reasons[var] = "ë‹¤ì¤‘ê³µì„ ì„±(Singular Matrix)"
                        except Exception as e:
                            uni_fail_reasons[var] = f"ì˜¤ë¥˜: {type(e).__name__}: {e}"

                    if uni_fail_reasons:
                        with st.expander("ë‹¨ë³€ëŸ‰ ì í•© ì‹¤íŒ¨ ë³€ìˆ˜/ì‚¬ìœ  ë³´ê¸°"):
                            for v, r in uni_fail_reasons.items():
                                st.caption(f"- **{v}** â†’ {r}")

                    # ---------- 2) ë³€ìˆ˜ ì„ íƒ (p-enter ê¸°ì¤€) ----------
                    selected_vars_for_multi = [v for v, p in univariate_pmins.items() if p <= p_enter_logistic]
                    st.write(f"**ë‹¤ë³€ëŸ‰ ë¶„ì„ í¬í•¨ ë³€ìˆ˜ (p â‰¤ {p_enter_logistic:.3f})**: {selected_vars_for_multi if selected_vars_for_multi else 'ì—†ìŒ'}")

                    # ---------- 3) Multivariate ----------
                    result_multi = None
                    X_multi, y_multi = None, None
                    if selected_vars_for_multi:
                        multi_cols = ['const']
                        for var in selected_vars_for_multi:
                            multi_cols += [c for c in X_final.columns if (c == var) or c.startswith(f"{var}=")]
                        multi_cols = list(dict.fromkeys(multi_cols))  # ì¤‘ë³µ ì œê±° & ìˆœì„œ ìœ ì§€
                        X_multi = X_final[multi_cols].copy()
                        y_multi = y_final.loc[X_multi.index]
                        if X_multi.shape[1] > 1 and len(np.unique(y_multi)) > 1:
                            try:
                                result_multi = sm.Logit(y_multi, X_multi).fit(method='newton', disp=0)
                            except Exception:
                                result_multi = sm.Logit(y_multi, X_multi).fit(method='lbfgs', disp=0)

                    # ---------- 4) ë…¼ë¬¸ìš© í‘œ ìƒì„± (Cox í˜•ì‹ê³¼ ìœ ì‚¬) ----------
                    rows = []
                    for var in indep_vars:
                        # ì„¹ì…˜ í—¤ë”
                        rows.append({
                            'Factor': var, 'Subgroup': '',
                            'Univariate OR (95% CI)': '', 'Univariate p-Value': '',
                            'Multivariate OR (95% CI)': '', 'Multivariate p-Value': ''
                        })

                        is_cat = (cat_info_logistic.get(var, {}).get('levels') is not None)
                        if is_cat:
                            levels = cat_info_logistic[var]['levels']
                            ref    = cat_info_logistic[var]['ref']
                            # Reference ë¼ì¸
                            rows.append({
                                'Factor': '', 'Subgroup': f'{ref} (Reference)',
                                'Univariate OR (95% CI)': 'Ref.', 'Univariate p-Value': '',
                                'Multivariate OR (95% CI)': 'Ref.', 'Multivariate p-Value': ''
                            })
                            # ê° ë”ë¯¸ ë ˆë²¨
                            for lv in levels[1:]:
                                term = f"{var}={lv}"
                                # Univariate
                                if (var in uni_results) and (term in uni_results[var].params.index):
                                    r = uni_results[var]
                                    b = float(r.params[term]); p = float(r.pvalues[term])
                                    ci_low, ci_high = [float(x) for x in r.conf_int().loc[term].tolist()]
                                    or_txt = f"{np.exp(b):.3f} ({np.exp(ci_low):.3f}-{np.exp(ci_high):.3f})"
                                    p_txt  = format_p(p)
                                else:
                                    or_txt, p_txt = 'NA', 'NA'
                                # Multivariate
                                if (result_multi is not None) and (term in result_multi.params.index):
                                    r = result_multi
                                    b = float(r.params[term]); p = float(r.pvalues[term])
                                    ci_low, ci_high = [float(x) for x in r.conf_int().loc[term].tolist()]
                                    or_multi = f"{np.exp(b):.3f} ({np.exp(ci_low):.3f}-{np.exp(ci_high):.3f})"
                                    p_multi  = format_p(p)
                                else:
                                    or_multi, p_multi = 'NA', 'NA'
                                rows.append({
                                    'Factor': '', 'Subgroup': str(lv),
                                    'Univariate OR (95% CI)': or_txt, 'Univariate p-Value': p_txt,
                                    'Multivariate OR (95% CI)': or_multi, 'Multivariate p-Value': p_multi
                                })
                        else:
                            # ì—°ì†í˜• ë³€ìˆ˜ 1ì—´
                            if (var in uni_results) and (var in uni_results[var].params.index):
                                r = uni_results[var]
                                b = float(r.params[var]); p = float(r.pvalues[var])
                                ci_low, ci_high = [float(x) for x in r.conf_int().loc[var].tolist()]
                                or_txt = f"{np.exp(b):.3f} ({np.exp(ci_low):.3f}-{np.exp(ci_high):.3f})"
                                p_txt  = format_p(p)
                            else:
                                or_txt, p_txt = 'NA', 'NA'
                            if (result_multi is not None) and (var in result_multi.params.index):
                                r = result_multi
                                b = float(r.params[var]); p = float(r.pvalues[var])
                                ci_low, ci_high = [float(x) for x in r.conf_int().loc[var].tolist()]
                                or_multi = f"{np.exp(b):.3f} ({np.exp(ci_low):.3f}-{np.exp(ci_high):.3f})"
                                p_multi  = format_p(p)
                            else:
                                or_multi, p_multi = 'NA', 'NA'
                            rows.append({
                                'Factor': '', 'Subgroup': '',
                                'Univariate OR (95% CI)': or_txt, 'Univariate p-Value': p_txt,
                                'Multivariate OR (95% CI)': or_multi, 'Multivariate p-Value': p_multi
                            })

                    publication_df = pd.DataFrame(rows)
                    st.write("**ë…¼ë¬¸ ì œì¶œìš© í…Œì´ë¸” (Univariate/Multivariate ë³‘ë ¬, Reference, Factor/ìˆ˜ì¤€êµ¬ì¡°)**")
                    st.dataframe(publication_df, use_container_width=True)

                    # ---- HL ì í•©ë„ ----
                    if result_multi is not None:
                        st.write("---")
                        st.subheader("ëª¨ë¸ ì í•©ë„: Hosmerâ€“Lemeshow Test")
                        y_pred_prob = result_multi.predict(X_multi)
                        hl_stat, hl_p, hl_err = calculate_hosmer_lemeshow(y_multi, y_pred_prob)
                        if hl_err:
                            st.warning(f"í˜¸ìŠ¤ë¨¸â€“ë ˜ì‡¼ ê²€ì •ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {hl_err}")
                        else:
                            c1, c2 = st.columns(2)
                            c1.metric("Chi-squared statistic", f"{hl_stat:.3f}")
                            c2.metric("p-value", f"{hl_p:.3f}")
                            st.caption("â€» p-valueê°€ 0.05ë³´ë‹¤ í¬ë©´ ëª¨ë¸ì´ ë°ì´í„°ì— ì ì ˆíˆ ì í•©í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                    # ---- ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ----
                    output_logistic = io.BytesIO()
                    with pd.ExcelWriter(output_logistic, engine='openpyxl') as writer:
                        publication_df.to_excel(writer, index=False, sheet_name='Logistic Regression Results')
                    st.download_button(
                        label="ë¡œì§€ìŠ¤í‹± ê²°ê³¼ ì—‘ì…€ë¡œ ì €ì¥",
                        data=output_logistic.getvalue(),
                        file_name="Logistic_Regression_Publication_Table.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        key='download_logistic_publication'
                    )

            except Exception as e:
                st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {type(e).__name__}: {e}")

# ================== ë ==============================
