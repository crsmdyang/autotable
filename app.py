# -*- coding: utf-8 -*-
# ================== ìë™ Table & Cox & Logistic ë¶„ì„ê¸° (FINAL) ==================
# í•„ìš” íŒ¨í‚¤ì§€: pandas, numpy, scipy, lifelines, statsmodels, openpyxl, xlrd, streamlit
# ì„¤ì¹˜: pip install -U pandas numpy scipy lifelines statsmodels openpyxl xlrd streamlit

import streamlit as st
import pandas as pd
import numpy as np
from scipy import stats
import io
from lifelines import CoxPHFitter
from lifelines.exceptions import ConvergenceError
import statsmodels.api as sm  # Logistic Regression
from statsmodels.tools.sm_exceptions import PerfectSeparationError
import matplotlib.pyplot as plt

# ----- í˜ì´ì§€ ì„¤ì • -----
st.set_page_config(page_title="ìë™ ë…¼ë¬¸ Table", layout="wide")

# ----- ê°„ë‹¨ ë¹„ë°€ë²ˆí˜¸ ë³´í˜¸ (ê¸°ë³¸: CRCR ë˜ëŠ” st.secrets['APP_PASSWORD']) -----
def _check_password():
    def _password_entered():
        target = st.secrets.get("APP_PASSWORD", "CRCR")
        if st.session_state.get("_password_input", "") == str(target):
            st.session_state["_pw_ok"] = True
            st.session_state.pop("_password_input", None)
        else:
            st.session_state["_pw_ok"] = False

    if st.session_state.get("_pw_ok", False):
        return True

    st.sidebar.subheader("ğŸ” Access")
    st.sidebar.text_input("Password", type="password", key="_password_input", on_change=_password_entered)
    if st.session_state.get("_pw_ok") is False:
        st.sidebar.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

# ë¹„ë°€ë²ˆí˜¸ ì²´í¬
_check_password()

st.title("ìë™ Table ìƒì„±ê¸° (Table1 / Cox / Logistic)")

# -------------------- [ê³µí†µ ìœ í‹¸] --------------------
def format_p(p):
    if p is None or (isinstance(p, float) and np.isnan(p)):
        return "NA"
    if p >= 0.999:
        return "p > 0.99"
    if p < 0.001:
        return "<0.001"
    return f"{p:.3f}"

def is_continuous(series, threshold=20):
    try:
        return (series.dtype.kind in "fi") and (series.nunique(dropna=True) > threshold)
    except Exception:
        return False

def ordered_levels(series):
    # ë¬¸ìì—´/ìˆ«ì í˜¼í•©ì„ ì•ˆì „í•˜ê²Œ ì •ë ¬
    unique_strings = pd.Series(series.dropna().unique()).astype(str).unique().tolist()
    try:
        unique_strings.sort(key=float)
    except ValueError:
        unique_strings.sort()
    return unique_strings

def make_dummies(df_in, var, levels):
    # "ë³€ìˆ˜=ìˆ˜ì¤€" ì´ë¦„ìœ¼ë¡œ ë”ë¯¸ ìƒì„± (drop_first â†’ ì²« ë ˆë²¨ì´ Reference)
    cat = pd.Categorical(df_in[var].astype(str),
                         categories=[str(x) for x in levels],
                         ordered=True)
    dmy = pd.get_dummies(cat, prefix=var, prefix_sep="=", drop_first=True, dtype=float)
    dmy.index = df_in.index
    return dmy

def dummy_colname(var, level):
    return f"{var}={str(level)}"

# --- ì¤‘ìš”: ë¡œì§€ìŠ¤í‹±ì—ì„œ í•„ìš”í•œ 'const'ëŠ” ë³´ì¡´ ---
def drop_constant_cols(X):
    """
    ìƒìˆ˜ì—´(ê³ ìœ ê°’ 1ê°œ) ì œê±°. ë‹¨, 'const' ì—´ì€ íšŒê·€ëª¨í˜•ì— í•„ìš”í•˜ë¯€ë¡œ ìœ ì§€.
    """
    cols_to_keep = []
    for col in X.columns:
        if col == 'const' or X[col].nunique(dropna=True) > 1:
            cols_to_keep.append(col)
    return X[cols_to_keep]

def drop_constant_predictors(X, time_col, event_col):
    pred_cols = [c for c in X.columns if c not in [time_col, event_col]]
    keep = [c for c in pred_cols if X[c].nunique(dropna=True) > 1]
    return X[[time_col, event_col] + keep]

def clean_time(s):
    s = pd.to_numeric(s, errors="coerce")
    s = s.replace([np.inf, -np.inf], np.nan)
    return s

def ensure_binary_event(col, events, censored):
    def _map(x):
        if x in events: return 1
        if x in censored: return 0
        return np.nan
    return col.apply(_map).astype(float)

def calculate_hosmer_lemeshow(y_true, y_pred_prob, n_groups=10):
    """
    Hosmer-Lemeshow ì í•©ë„ ê²€ì •
    """
    data = pd.DataFrame({'y_true': y_true, 'y_pred_prob': y_pred_prob})
    try:
        data['group'] = pd.qcut(data['y_pred_prob'], q=n_groups, duplicates='drop')
    except ValueError:
        n_groups = data['y_pred_prob'].nunique()
        if n_groups < 2:
            return np.nan, np.nan, "Not enough unique probabilities for the test."
        data['group'] = pd.qcut(data['y_pred_prob'], q=n_groups, duplicates='drop')

    summary = data.groupby('group', observed=False).agg(
        total_count=('y_true', 'count'),
        observed_events=('y_true', 'sum'),
        expected_events=('y_pred_prob', 'sum')
    )
    summary['observed_non_events'] = summary['total_count'] - summary['observed_events']
    summary['expected_non_events'] = summary['total_count'] - summary['expected_events']

    if (summary['expected_events'] < 1e-6).any() or (summary['expected_non_events'] < 1e-6).any():
        return np.nan, np.nan, "Test failed due to zero or near-zero expected frequencies in some groups."

    hl_stat = (
        ((summary['observed_events'] - summary['expected_events'])**2 / summary['expected_events']) +
        ((summary['observed_non_events'] - summary['expected_non_events'])**2 / summary['expected_non_events'])
    ).sum()

    df_hl = len(summary) - 2
    if df_hl <= 0:
        return np.nan, np.nan, "Not enough groups to calculate p-value (degrees of freedom <= 0)."

    p_value = stats.chi2.sf(hl_stat, df_hl)
    return hl_stat, p_value, None

def select_penalizer_by_cv(X_all, time_col, event_col,
                           grid=(0.0, 0.01, 0.05, 0.1, 0.2, 0.5),
                           k=5, seed=42):
    """
    lifelines CoxPHFitterìš© penalizerë¥¼ K-fold CVë¡œ ì„ íƒ (C-index ìµœëŒ€í™”)
    """
    if X_all.shape[0] < k + 2 or X_all[event_col].sum() < k:
        return None, {}
    idx = X_all.index.to_numpy()
    rng = np.random.default_rng(seed)
    rng.shuffle(idx)
    folds = np.array_split(idx, k)
    scores = {}
    for pen in grid:
        cv_scores = []
        for i in range(k):
            test_idx = folds[i]
            train_idx = np.concatenate([folds[j] for j in range(k) if j != i])
            train = X_all.loc[train_idx].copy()
            test  = X_all.loc[test_idx].copy()
            train = drop_constant_predictors(train, time_col, event_col)
            test  = test[train.columns]
            if train[event_col].sum() < 2 or test[event_col].sum() < 1: 
                continue
            if train.shape[1] <= 2 or train.shape[0] < 5:
                continue
            try:
                cph = CoxPHFitter(penalizer=pen)
                cph.fit(train, duration_col=time_col, event_col=event_col)
                s = float(cph.score(test, scoring_method="concordance_index"))
                if np.isfinite(s):
                    cv_scores.append(s)
            except Exception:
                continue
        if cv_scores:
            scores[pen] = float(np.mean(cv_scores))
    if not scores:
        return None, {}
    best_pen = sorted(scores.items(), key=lambda x: (-x[1], x[0]))[0][0]
    return best_pen, scores

# -------------------- [íŒŒì¼ ì—…ë¡œë“œ] --------------------
uploaded_file = st.file_uploader("ì—‘ì…€/CSV ì—…ë¡œë“œ", type=['xls', 'xlsx', 'csv'])
df = None
sheetname = None
if uploaded_file:
    name = uploaded_file.name.lower()
    try:
        if name.endswith(".csv"):
            df = pd.read_csv(uploaded_file)
        elif name.endswith((".xlsx", ".xls")):
            engine = "openpyxl" if name.endswith(".xlsx") else "xlrd"
            xls = pd.ExcelFile(uploaded_file, engine=engine)
            sheetname = xls.sheet_names[0] if len(xls.sheet_names) == 1 else st.selectbox("ì‹œíŠ¸ ì„ íƒ", xls.sheet_names, index=0)
            df = pd.read_excel(xls, sheet_name=sheetname, engine=engine)
        else:
            st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. csv/xls/xlsxë§Œ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            st.stop()

        # ì»¬ëŸ¼ ì •ë¦¬
        df.columns = pd.Index([str(c).strip().replace("\\n", " ") for c in df.columns])
        st.success(f"ì‹œíŠ¸ëª…: {sheetname if sheetname else ''}, ë°ì´í„° shape: {df.shape}")
        st.dataframe(df.head())
        st.session_state['df'] = df
    except Exception as e:
        st.error(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.stop()

# -------------------- [Table1: ì„œë¸Œ í•¨ìˆ˜] --------------------
def analyze_table1_display(df, group_col, value_map, threshold=20):
    result_rows = []
    group_values = list(value_map.keys())
    group_names = list(value_map.values())
    group_n = {g: (df[group_col] == g).sum() for g in group_values}
    for var in df.columns:
        if var == group_col: continue
        valid = df[df[group_col].isin(group_values)]
        if valid[var].dropna().empty: continue
        # ì—°ì†í˜•
        if is_continuous(valid[var], threshold=threshold):
            row = {'Characteristic': var}
            for g, g_name in zip(group_values, group_names):
                sub = valid[valid[group_col] == g][var].dropna()
                if sub.shape[0] > 0:
                    med, q1, q3 = sub.median(), sub.quantile(0.25), sub.quantile(0.75)
                    mean, std = sub.mean(), sub.std()
                    row[f"{g_name} (n={group_n[g]})"] = f"{med:.1f} [{q1:.1f}-{q3:.1f}]; {mean:.1f}Â±{std:.1f}"
                else:
                    row[f"{g_name} (n={group_n[g]})"] = "NA"
            p = np.nan; test_str = ""
            try:
                arr = [valid[valid[group_col] == g][var].dropna() for g in group_values]
                if all(stats.shapiro(s)[1] > 0.05 for s in arr if len(s) >= 3):
                    if len(arr) == 2: _, p = stats.ttest_ind(arr[0], arr[1], nan_policy='omit'); test_str = "t-test"
                    elif len(arr) > 2: _, p = stats.f_oneway(*arr); test_str = "ANOVA"
                else:
                    if len(arr) == 2: _, p = stats.mannwhitneyu(arr[0], arr[1], alternative='two-sided'); test_str = "Mann-Whitney U"
                    elif len(arr) > 2: _, p = stats.kruskal(*arr); test_str = "Kruskal-Wallis"
            except Exception:
                p = np.nan; test_str = "NA"
            row['Test'] = test_str; row['p value'] = format_p(p); row['sub_p'] = ""
            result_rows.append(row)
        # ë²”ì£¼í˜•
        else:
            p = np.nan; test_str = ""
            try:
                ct = pd.crosstab(valid[group_col], valid[var])
                if ct.shape == (2, 2): _, p = stats.fisher_exact(ct); test_str = "Fisher"
                else: _, p, _, _ = stats.chi2_contingency(ct); test_str = "Chi-square"
            except Exception:
                p = np.nan; test_str = "NA"
            result_rows.append({'Characteristic': var, **{f"{g_name} (n={group_n[g]})": "" for g_name in group_names}, 'Test': test_str, 'p value': format_p(p), 'sub_p': ""})
            for val in ordered_levels(valid[var]):
                row = {'Characteristic': f"  {val}"}
                for g, g_name in zip(group_values, group_names):
                    cnt = valid[(valid[group_col] == g) & (valid[var] == val)].shape[0]
                    percent = (cnt/group_n[g]*100) if group_n[g] > 0 else 0
                    row[f"{g_name} (n={group_n[g]})"] = f"{cnt}({percent:.0f}%)"
                row['Test'] = ""
                row['p value'] = ""
                p_sub = np.nan
                try:
                    table = np.array([[(valid[(valid[group_col] == g) & (valid[var] == val)]).shape[0], (valid[(valid[group_col] == g) & (valid[var] != val)]).shape[0]] for g in group_values])
                    if table.shape == (2, 2): _, p_sub = stats.fisher_exact(table)
                    else: _, p_sub, _, _ = stats.chi2_contingency(table)
                except: 
                    pass
                row['sub_p'] = format_p(p_sub)
                result_rows.append(row)
    return pd.DataFrame(result_rows)

# -------------------- [UI: Tab êµ¬ì„±] --------------------
if 'df' in st.session_state:
    df = st.session_state['df']
else:
    df = None

if df is not None:
    tab_titles = [
        "ğŸ“Š Table1 ìë™í™”", 
        "ğŸŸ¦ Cox íšŒê·€ë¶„ì„", 
        "ğŸŸ§ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„"
    ]
    tab1, tab2, tab3 = st.tabs(tab_titles)

    # ===== TAB1: Table 1 =====
    with tab1:
        st.header("Table 1 ìë™ ìƒì„±")
        group_col = st.selectbox("ë¶„ì„í•  ê·¸ë£¹ ë³€ìˆ˜ëª… ì„ íƒ", options=list(df.columns), key='group_col')
        if group_col:
            unique_vals = list(df[group_col].dropna().unique())
            selected_vals = st.multiselect("ë¶„ì„í•  ê·¸ë£¹ ê°’ ì„ íƒ", unique_vals, default=unique_vals[:2], key='group_values')
            if selected_vals:
                value_map = {val: st.text_input(f"'{val}'ì˜ í‘œì‹œ ë¼ë²¨", value=str(val), key=f'label_{val}') for val in selected_vals}
                if st.button("ë…¼ë¬¸ Table1 ìƒì„±", key='table1_generate'):
                    result = analyze_table1_display(df, group_col, value_map, threshold=20)
                    st.dataframe(result)
                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine='openpyxl') as writer:
                        result.to_excel(writer, index=False)
                    st.download_button("Table1 ì—‘ì…€ë¡œ ì €ì¥", output.getvalue(), "Table1_Results.xlsx")

    # ===== TAB2: Cox Regression (ì™„ì „ êµ¬í˜„) =====
    with tab2:
        st.header("Cox ë¹„ë¡€ìœ„í—˜ íšŒê·€ë¶„ì„ (Univariate & Multivariate)")
        time_col = st.selectbox("ìƒì¡´ê¸°ê°„ ë³€ìˆ˜(time)", df.columns, key="cox_time_col")
        event_col = st.selectbox("Event ë³€ìˆ˜(ì˜ˆ: 0=ìƒì¡´, 1=ì‚¬ë§ ë“±)", df.columns, key="cox_event_col")

        temp_df = df.copy()
        if event_col:
            unique_events = list(df[event_col].dropna().unique())
            st.write(f"ì´ ë³€ìˆ˜ì˜ ì‹¤ì œ ê°’: {unique_events}")
            selected_event = st.multiselect("ì´ë²¤íŠ¸(ì‚¬ê±´) ê°’", unique_events, key='selected_event_val')
            selected_censored = st.multiselect("ìƒì¡´/ê´€ì°°ì¢…ê²°(censored) ê°’", unique_events, key='selected_censored_val')
            st.caption("â€» ì‚¬ê±´ê°’ê³¼ ê²€ì—´ê°’ì€ ì„œë¡œ ê²¹ì¹˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.")
            temp_df["__event_for_cox"] = ensure_binary_event(temp_df[event_col], set(selected_event), set(selected_censored))
        else:
            temp_df["__event_for_cox"] = np.nan

        candidate_vars = [c for c in df.columns if c not in [time_col, event_col]]
        variables = st.multiselect("ë¶„ì„ í›„ë³´ ë³€ìˆ˜ ì„ íƒ", candidate_vars, key="cox_variables")

        c1, c2, c3, c4 = st.columns([1,1,1,1])
        with c1:
            p_enter = st.number_input("ë‹¤ë³€ëŸ‰ í¬í•¨ ê¸°ì¤€ p-enter (â‰¤)", min_value=0.001, max_value=1.0, value=0.05, step=0.01)
        with c2:
            max_levels = st.number_input("ë²”ì£¼í˜• íŒì • ìµœëŒ€ ê³ ìœ ê°’", min_value=2, max_value=50, value=10, step=1)
        with c3:
            auto_penal = st.checkbox("penalizer ìë™ ì„ íƒ (CV, C-index)", value=False)
        with c4:
            cv_k = st.number_input("CV folds (K)", min_value=3, max_value=10, value=5, step=1, disabled=not auto_penal)

        penalizer = st.number_input("penalizer (ìˆ˜ë ´ ì•ˆì •í™”)", min_value=0.0, max_value=5.0, value=0.1, step=0.1, disabled=auto_penal)

        def basic_clean(df_in, time_col):
            out = df_in.copy()
            out[time_col] = clean_time(out[time_col])
            out = out[out[time_col] > 0]
            out = out.replace([np.inf, -np.inf], np.nan)
            return out

        if st.button("Cox íšŒê·€ë¶„ì„ ì‹¤í–‰"):
            # í•„ìˆ˜ ê²€ì¦
            if not selected_event or not selected_censored:
                st.error("ì‚¬ê±´ê°’ê³¼ ê²€ì—´ê°’ì„ ê°ê° ìµœì†Œ 1ê°œ ì´ìƒ ì„ íƒí•˜ì„¸ìš”.")
                st.stop()
            if set(selected_event) & set(selected_censored):
                st.error("ì‚¬ê±´ê°’ê³¼ ê²€ì—´ê°’ì´ ê²¹ì¹©ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•˜ì„¸ìš”.")
                st.stop()

            temp_df2 = basic_clean(temp_df, time_col).dropna(subset=[time_col, "__event_for_cox"])
            n_events = int(temp_df2["__event_for_cox"].sum())
            n_total = temp_df2.shape[0]
            st.info(f"ì´ ê´€ì¸¡ì¹˜: {n_total}, ì´ë²¤íŠ¸ ìˆ˜: {n_events}")
            if n_events < 5:
                st.warning("ì´ë²¤íŠ¸ ìˆ˜ê°€ <5ë¡œ ë§¤ìš° ì ìŠµë‹ˆë‹¤. ì¶”ì •ì´ ë¶ˆì•ˆì •í•˜ê±°ë‚˜ ëª¨ë¸ì´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            # ---------- 1) Univariate ----------
            uni_sum_dict = {}
            uni_na_vars = []
            cat_info = {}

            for var in variables:
                try:
                    dat_raw = temp_df2[[time_col, "__event_for_cox", var]].copy()
                    dat_raw = dat_raw.dropna(subset=[var])
                    if dat_raw.empty:
                        uni_na_vars.append(var); continue

                    # ë²”ì£¼í˜•/ì—°ì†í˜• ë¶„ê¸°
                    if (dat_raw[var].dtype == "object") or (dat_raw[var].nunique(dropna=True) <= max_levels):
                        lvls = ordered_levels(dat_raw[var])
                        cat_info[var] = {"levels": lvls, "ref": lvls[0]}
                        dmy = make_dummies(dat_raw, var, lvls)
                        dat = pd.concat([dat_raw[[time_col, "__event_for_cox"]], dmy], axis=1)
                    else:
                        cat_info[var] = {"levels": None, "ref": None}
                        dat = dat_raw[[time_col, "__event_for_cox", var]].copy()
                        dat[var] = pd.to_numeric(dat[var], errors="coerce")

                    dat = dat.dropna()
                    # Coxì—ì„œëŠ” const ë¶ˆí•„ìš” â†’ ì¼ë°˜ ìƒìˆ˜ì—´ë§Œ ì œê±°
                    dat = dat.loc[:, [c for c in dat.columns if dat[c].nunique(dropna=True) > 1]]

                    if (dat.shape[0] < 3) or (dat["__event_for_cox"].sum() < 1) or (dat.shape[1] <= 2):
                        uni_na_vars.append(var); continue

                    cph = CoxPHFitter(penalizer=penalizer)  # UnivariateëŠ” ì…ë ¥ penalizer ì‚¬ìš©
                    cph.fit(dat, duration_col=time_col, event_col="__event_for_cox")
                    uni_sum_dict[var] = cph.summary.copy()
                except ConvergenceError:
                    uni_na_vars.append(var)
                except Exception:
                    uni_na_vars.append(var)

            # ë³€ìˆ˜ì„ íƒ: ìµœì†Œ p
            univariate_pvals = {}
            for var, summ in uni_sum_dict.items():
                if cat_info[var]["levels"] is None:
                    if var in summ.index:
                        univariate_pvals[var] = float(summ.loc[var, "p"])
                else:
                    p_min = None
                    for _, row in summ.iterrows():
                        p = float(row["p"])
                        p_min = p if p_min is None else min(p_min, p)
                    if p_min is not None:
                        univariate_pvals[var] = p_min

            selected_vars = [v for v, p in univariate_pvals.items() if p <= p_enter]
            st.write(f"ë‹¤ë³€ëŸ‰ í›„ë³´ ë³€ìˆ˜(â‰¤ {p_enter:.3f}): {selected_vars if selected_vars else 'ì—†ìŒ'}")

            # ---------- 2) Multivariate ----------
            multi_sum = None
            multi_na_vars = []
            chosen_penalizer = penalizer  # ê¸°ë³¸ê°’

            if len(selected_vars) >= 1:
                try:
                    dat_base = temp_df2[[time_col, "__event_for_cox"]].copy()
                    X_list = []
                    for var in selected_vars:
                        if cat_info.get(var, {}).get("levels") is None:
                            xi = pd.to_numeric(temp_df2[var], errors="coerce").to_frame(var)
                        else:
                            lvls = cat_info[var]["levels"]
                            xi = make_dummies(temp_df2[[var]], var, lvls)
                        X_list.append(xi)
                    X_all = pd.concat([dat_base] + X_list, axis=1).dropna()
                    X_all = drop_constant_predictors(X_all, time_col, "__event_for_cox")

                    # Auto-CV
                    if auto_penal and X_all["__event_for_cox"].sum() >= cv_k:
                        pen_grid = (0.0, 0.01, 0.05, 0.1, 0.2, 0.5)
                        best_pen, pen_scores = select_penalizer_by_cv(
                            X_all, time_col, "__event_for_cox",
                            grid=pen_grid, k=int(cv_k), seed=42
                        )
                        if best_pen is not None:
                            chosen_penalizer = float(best_pen)
                            st.success(f"Auto-CV ì„ íƒ penalizer = {chosen_penalizer} (í‰ê·  C-index ê¸°ì¤€)")
                            st.caption(f"Grid ì„±ëŠ¥: { {k: round(v,4) for k,v in pen_scores.items()} }")
                        else:
                            st.warning("CVë¡œ penalizerë¥¼ ê²°ì •í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì…ë ¥ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

                    if (X_all.shape[0] >= 3) and (X_all["__event_for_cox"].sum() >= 1) and (X_all.shape[1] > 2):
                        cph_multi = CoxPHFitter(penalizer=chosen_penalizer)
                        cph_multi.fit(X_all, duration_col=time_col, event_col="__event_for_cox")
                        multi_sum = cph_multi.summary.copy()
                    else:
                        multi_na_vars = selected_vars
                except ConvergenceError:
                    multi_na_vars = selected_vars
                except Exception:
                    multi_na_vars = selected_vars

            # ---------- 3) ì¶œë ¥ í…Œì´ë¸” ----------
            rows = []
            for var in variables:
                rows.append({
                    "Factor": var, "Subgroup": "",
                    "Univariate analysis HR (95% CI)": "", "Univariate analysis p-Value": "",
                    "Multivariate analysis HR (95% CI)": "", "Multivariate analysis p-Value": ""
                })

                if (var in uni_na_vars) and ((multi_sum is None) or (var in multi_na_vars)):
                    rows.append({
                        "Factor": "", "Subgroup": "(insufficient / skipped)",
                        "Univariate analysis HR (95% CI)": "NA", "Univariate analysis p-Value": "NA",
                        "Multivariate analysis HR (95% CI)": "NA", "Multivariate analysis p-Value": "NA"
                    })
                    continue

                # ë²”ì£¼í˜•
                if cat_info.get(var, {}).get("levels") is not None:
                    lvls = cat_info[var]["levels"]; ref = cat_info[var]["ref"]
                    rows.append({
                        "Factor": "", "Subgroup": f"{ref} (Reference)",
                        "Univariate analysis HR (95% CI)": "Ref.", "Univariate analysis p-Value": "",
                        "Multivariate analysis HR (95% CI)": "Ref.", "Multivariate analysis p-Value": ""
                    })
                    for lv in lvls[1:]:
                        colname = dummy_colname(var, lv)
                        # Uni
                        if (var in uni_na_vars) or (var not in uni_sum_dict) or (colname not in uni_sum_dict[var].index):
                            hr_uni, p_uni = "NA", "NA"
                        else:
                            r = uni_sum_dict[var].loc[colname]
                            hr_uni = f"{r['exp(coef)']:.3f} ({r['exp(coef) lower 95%']:.3f}-{r['exp(coef) upper 95%']:.3f})"
                            p_uni = format_p(float(r['p']))
                        # Multi
                        if (multi_sum is None) or (var in multi_na_vars) or (colname not in (multi_sum.index if multi_sum is not None else [])):
                            hr_multi, p_multi = "NA", "NA"
                        else:
                            r = multi_sum.loc[colname]
                            hr_multi = f"{r['exp(coef)']:.3f} ({r['exp(coef) lower 95%']:.3f}-{r['exp(coef) upper 95%']:.3f})"
                            p_multi = format_p(float(r['p']))
                        rows.append({
                            "Factor": "", "Subgroup": str(lv),
                            "Univariate analysis HR (95% CI)": hr_uni, "Univariate analysis p-Value": p_uni,
                            "Multivariate analysis HR (95% CI)": hr_multi, "Multivariate analysis p-Value": p_multi
                        })
                # ì—°ì†í˜•
                else:
                    if (var not in uni_sum_dict) or (var in uni_na_vars) or (var not in uni_sum_dict[var].index if var in uni_sum_dict else True):
                        hr_uni, p_uni = "NA", "NA"
                    else:
                        r = uni_sum_dict[var].loc[var]
                        hr_uni = f"{r['exp(coef)']:.3f} ({r['exp(coef) lower 95%']:.3f}-{r['exp(coef) upper 95%']:.3f})"
                        p_uni = format_p(float(r['p']))
                    if (multi_sum is None) or (var in multi_na_vars) or (var not in (multi_sum.index if multi_sum is not None else [])):
                        hr_multi, p_multi = "NA", "NA"
                    else:
                        r = multi_sum.loc[var]
                        hr_multi = f"{r['exp(coef)']:.3f} ({r['exp(coef) lower 95%']:.3f}-{r['exp(coef) upper 95%']:.3f})"
                        p_multi = format_p(float(r['p']))
                    rows.append({
                        "Factor": "", "Subgroup": "",
                        "Univariate analysis HR (95% CI)": hr_uni, "Univariate analysis p-Value": p_uni,
                        "Multivariate analysis HR (95% CI)": hr_multi, "Multivariate analysis p-Value": p_multi
                    })

            result_table = pd.DataFrame(rows)
            st.write("**ë…¼ë¬¸ ì œì¶œìš© í…Œì´ë¸” (Univariate/Multivariate ë³‘ë ¬, Reference, Factor/ìˆ˜ì¤€êµ¬ì¡°)**")
            if auto_penal and len(selected_vars) >= 1:
                st.caption(f"*ë‹¤ë³€ëŸ‰ ìµœì¢… penalizer: {chosen_penalizer}*")
            st.dataframe(result_table)

            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                result_table.to_excel(writer, index=False)
            st.download_button(
                label="Cox ê²°ê³¼ ì—‘ì…€ë¡œ ì €ì¥",
                data=output.getvalue(),
                file_name="Cox_Regression_Results_Table.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

    # ===== TAB3: Logistic Regression =====
    with tab3:
        st.header("ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ (Risk Factor Analysis)")
        st.info("ì¢…ì†ë³€ìˆ˜ì˜ íŠ¹ì • ê°’ì„ ì‚¬ê±´(1)ê³¼ ê¸°ì¤€(0)ìœ¼ë¡œ ì •ì˜í•˜ì—¬ ìœ„í—˜ì¸ìë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

        dep_var = st.selectbox("ì¢…ì† ë³€ìˆ˜ (Y) ì„ íƒ", df.columns, key="logistic_dep_var")

        if dep_var:
            unique_outcomes = list(df[dep_var].dropna().unique())
            st.write(f"'{dep_var}' ë³€ìˆ˜ì˜ ê³ ìœ  ê°’: {unique_outcomes}")

            event_values = st.multiselect("ì‚¬ê±´(Event=1)ì— í•´ë‹¹í•˜ëŠ” ê°’ ì„ íƒ", unique_outcomes, key="logistic_event")
            control_values = st.multiselect("ê¸°ì¤€(Control=0)ì— í•´ë‹¹í•˜ëŠ” ê°’ ì„ íƒ", unique_outcomes, key="logistic_control")
            st.caption("â€» ì‚¬ê±´ ê°’ê³¼ ê¸°ì¤€ ê°’ì€ ì„œë¡œ ê²¹ì¹˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.")

        indep_vars = st.multiselect("ë…ë¦½ ë³€ìˆ˜ (X) ì„ íƒ (ìœ„í—˜ì¸ì í›„ë³´)", [c for c in df.columns if c != dep_var], key="logistic_indep_vars")

        c1_log, c2_log = st.columns(2)
        p_enter_logistic = c1_log.number_input("ë‹¤ë³€ëŸ‰ í¬í•¨ ê¸°ì¤€ p-enter (â‰¤)", 0.001, 1.0, 0.05, 0.01, key='logistic_p_enter')
        max_levels_logistic = c2_log.number_input("ë²”ì£¼í˜• íŒì • ìµœëŒ€ ê³ ìœ ê°’", 2, 50, 10, 1, key="logistic_max_levels")

        if st.button("ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ ì‹¤í–‰", key="run_logistic"):
            if not dep_var or not event_values or not control_values or not indep_vars:
                st.error("ì¢…ì† ë³€ìˆ˜, ì‚¬ê±´ ê°’, ê¸°ì¤€ ê°’, ë…ë¦½ ë³€ìˆ˜ë¥¼ ëª¨ë‘ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
                st.stop()
            if set(event_values) & set(control_values):
                st.error("ì‚¬ê±´ ê°’ê³¼ ê¸°ì¤€ ê°’ì´ ê²¹ì¹©ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•˜ì„¸ìš”.")
                st.stop()

            try:
                with st.spinner('ë¶„ì„ì„ ìˆ˜í–‰ ì¤‘ì…ë‹ˆë‹¤...'):
                    cols_to_use = [dep_var] + indep_vars
                    df_model = df[cols_to_use].copy()

                    df_model['__dependent_var_binary'] = ensure_binary_event(df_model[dep_var], set(event_values), set(control_values))
                    df_model.dropna(subset=['__dependent_var_binary'], inplace=True)
                    df_model['__dependent_var_binary'] = df_model['__dependent_var_binary'].astype(int)

                    y = df_model['__dependent_var_binary']
                    X_list, cat_info_logistic = [], {}
                    for var in indep_vars:
                        if not is_continuous(df_model[var], threshold=max_levels_logistic):
                            levels = ordered_levels(df_model[var])
                            cat_info_logistic[var] = {"levels": levels, "ref": levels[0]}
                            X_list.append(make_dummies(df_model[[var]], var, levels))
                        else:
                            cat_info_logistic[var] = {"levels": None, "ref": None}
                            X_list.append(pd.to_numeric(df_model[var], errors='coerce').rename(var))

                    if not X_list:
                        st.error("ìœ íš¨í•œ ë…ë¦½ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤."); st.stop()

                    X_processed = pd.concat(X_list, axis=1)
                    model_data = pd.concat([y, X_processed], axis=1).dropna()
                    y_final = model_data[y.name]
                    X_final_no_const = model_data.drop(columns=[y.name])

                    # Add constant and then drop any constant predictors (but keep the added 'const')
                    X_final_with_const = sm.add_constant(X_final_no_const, has_constant='add')
                    X_final = drop_constant_cols(X_final_with_const)

                    if X_final.shape[1] <= 1 or 'const' not in X_final.columns:
                        st.error("ë¶„ì„ì— ì‚¬ìš©í•  ìœ íš¨í•œ ë…ë¦½ ë³€ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."); st.stop()
                    st.info(f"ë¶„ì„ì— ì‚¬ìš©ëœ ì´ ê´€ì¸¡ì¹˜: {len(y_final)}, ì‚¬ê±´ ìˆ˜: {y_final.sum()}")

                    uni_results = {}
                    univariate_pvals = {}
                    failed_uni_vars = {}

                    for var in indep_vars:
                        try:
                            var_cols = [c for c in X_final.columns if c == var or c.startswith(f"{var}=")]
                            if not var_cols: continue
                            X_uni_cols = ['const'] + var_cols
                            X_uni_cols_exist = [c for c in X_uni_cols if c in X_final.columns]
                            X_uni = X_final[X_uni_cols_exist]
                            y_uni = y_final.loc[X_uni.index]
                            if len(y_uni.unique()) > 1 and X_uni.shape[1] > 1:
                                res = sm.Logit(y_uni, X_uni).fit(method='newton', disp=0)
                                uni_results[var] = res
                                pvals = [res.pvalues[c] for c in res.pvalues.index if c != 'const']
                                if pvals:
                                    univariate_pvals[var] = min(pvals)
                        except PerfectSeparationError:
                            failed_uni_vars[var] = "ë°ì´í„° ì™„ì „ ë¶„ë¦¬(Perfect Separation)ë¡œ ë¶„ì„ ì‹¤íŒ¨"
                        except np.linalg.LinAlgError:
                            failed_uni_vars[var] = "ë‹¤ì¤‘ê³µì„ ì„±(Singular Matrix) ë¬¸ì œë¡œ ë¶„ì„ ì‹¤íŒ¨"
                        except Exception as e:
                            failed_uni_vars[var] = f"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ë¡œ ë¶„ì„ ì‹¤íŒ¨ ({type(e).__name__}: {e})"

                    if failed_uni_vars:
                        st.warning("ì¼ë¶€ ë³€ìˆ˜ì— ëŒ€í•œ ë‹¨ë³€ëŸ‰ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤:")
                        for var, reason in failed_uni_vars.items():
                            st.caption(f"- **{var}**: {reason}")

                    selected_vars_for_multi = [v for v, p in univariate_pvals.items() if p <= p_enter_logistic]
                    st.write(f"**ë‹¤ë³€ëŸ‰ ë¶„ì„ í¬í•¨ ë³€ìˆ˜ (p â‰¤ {p_enter_logistic})**: {selected_vars_for_multi if selected_vars_for_multi else 'ì—†ìŒ'}")

                    result_multi = None
                    X_multi, y_multi = None, None
                    if selected_vars_for_multi:
                        multi_cols_to_keep = ['const']
                        for var in selected_vars_for_multi:
                            multi_cols_to_keep.extend([c for c in X_final.columns if c == var or c.startswith(f"{var}=")])
                        X_multi = X_final[list(dict.fromkeys(multi_cols_to_keep))]
                        y_multi = y_final.loc[X_multi.index]
                        if X_multi.shape[1] > 1:
                            try:
                                result_multi = sm.Logit(y_multi, X_multi).fit(method='newton', disp=0)
                            except Exception as e:
                                st.error(f"ë‹¤ë³€ëŸ‰ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

                    output_rows = []
                    for var in indep_vars:
                        is_cat = cat_info_logistic.get(var, {}).get('levels') is not None

                        if is_cat:
                            output_rows.append({'Factor': var, 'Subgroup': '', 'Univariate OR (95% CI)': '', 'p-value (Uni)': '', 'Multivariate OR (95% CI)': '', 'p-value (Multi)': ''})
                            levels = cat_info_logistic[var]['levels']
                            output_rows.append({'Factor': '', 'Subgroup': f"{levels[0]} (Reference)", 'Univariate OR (95% CI)': '1.0', 'p-value (Uni)': '', 'Multivariate OR (95% CI)': '1.0', 'p-value (Multi)': ''})
                            for level in levels[1:]:
                                dummy_name = f"{var}={level}"
                                row_data = {'Factor': '', 'Subgroup': str(level)}
                                res_uni = uni_results.get(var)
                                if res_uni and dummy_name in res_uni.params:
                                    param, pval, conf = res_uni.params[dummy_name], res_uni.pvalues[dummy_name], res_uni.conf_int().loc[dummy_name]
                                    row_data['Univariate OR (95% CI)'] = f"{np.exp(param):.3f} ({np.exp(conf[0]):.3f}-{np.exp(conf[1]):.3f})"
                                    row_data['p-value (Uni)'] = format_p(pval)
                                else:
                                    row_data['Univariate OR (95% CI)'] = 'NA'
                                    row_data['p-value (Uni)'] = 'NA'

                                if result_multi and var in selected_vars_for_multi and dummy_name in result_multi.params:
                                    param, pval, conf = result_multi.params[dummy_name], result_multi.pvalues[dummy_name], result_multi.conf_int().loc[dummy_name]
                                    row_data['Multivariate OR (95% CI)'] = f"{np.exp(param):.3f} ({np.exp(conf[0]):.3f}-{np.exp(conf[1]):.3f})"
                                    row_data['p-value (Multi)'] = format_p(pval)
                                output_rows.append(row_data)
                        else:
                            row_data = {'Factor': var, 'Subgroup': ''}
                            res_uni = uni_results.get(var)
                            if res_uni and var in res_uni.params:
                                param, pval, conf = res_uni.params[var], res_uni.pvalues[var], res_uni.conf_int().loc[var]
                                row_data['Univariate OR (95% CI)'] = f"{np.exp(param):.3f} ({np.exp(conf[0]):.3f}-{np.exp(conf[1]):.3f})"
                                row_data['p-value (Uni)'] = format_p(pval)
                            else:
                                row_data['Univariate OR (95% CI)'] = 'NA'
                                row_data['p-value (Uni)'] = 'NA'

                            if result_multi and var in selected_vars_for_multi and var in result_multi.params:
                                param, pval, conf = result_multi.params[var], result_multi.pvalues[var], result_multi.conf_int().loc[var]
                                row_data['Multivariate OR (95% CI)'] = f"{np.exp(param):.3f} ({np.exp(conf[0]):.3f}-{np.exp(conf[1]):.3f})"
                                row_data['p-value (Multi)'] = format_p(pval)
                            output_rows.append(row_data)

                    publication_df = pd.DataFrame(output_rows).fillna('')
                    st.write("### ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ ê²°ê³¼ (ë…¼ë¬¸ í˜•ì‹)")
                    st.dataframe(publication_df)

                    if result_multi is not None:
                        st.write("---")
                        st.write("### ëª¨ë¸ ì í•©ë„ ê²€ì • (Hosmer-Lemeshow Test)")
                        y_pred_prob = result_multi.predict(X_multi)
                        hl_stat, p_value_hl, hl_error = calculate_hosmer_lemeshow(y_multi, y_pred_prob)
                        if hl_error:
                            st.warning(f"í˜¸ìŠ¤ë¨¸-ë ˜ì‡¼ ê²€ì •ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {hl_error}")
                        else:
                            col1, col2 = st.columns(2)
                            col1.metric("Chi-squared statistic", f"{hl_stat:.3f}")
                            col2.metric("p-value", f"{p_value_hl:.3f}")
                            st.caption("â€» p-valueê°€ 0.05ë³´ë‹¤ í¬ë©´ ëª¨ë¸ì´ ë°ì´í„°ì— ì˜ ì í•©í•œë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                    output_logistic = io.BytesIO()
                    with pd.ExcelWriter(output_logistic, engine='openpyxl') as writer:
                        publication_df.to_excel(writer, index=False, sheet_name='Logistic Regression Results')
                    st.download_button(
                        label="ë¶„ì„ ê²°ê³¼ ì—‘ì…€ë¡œ ì €ì¥",
                        data=output_logistic.getvalue(),
                        file_name="Logistic_Regression_Publication_Table.xlsx",
                        mime="application/vnd.openxmlformats-officedocument-spreadsheetml.sheet",
                        key='download_logistic_publication'
                    )
            except Exception as e:
                st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
