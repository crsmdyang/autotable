# -*- coding: utf-8 -*-
# ================== 자동 Table & Cox & Logistic 분석기 ==================
# 필요 패키지: pandas, numpy, scipy, lifelines, statsmodels, openpyxl, xlrd, streamlit

import streamlit as st
import pandas as pd
import numpy as np
from scipy import stats
import io
from lifelines import CoxPHFitter
from lifelines.exceptions import ConvergenceError
import statsmodels.api as sm # For Logistic Regression
import matplotlib.pyplot as plt

# ----- 페이지 설정 -----
st.set_page_config(page_title="자동 논문 Table", layout="wide")

# ----- 간단 비밀번호 보호 (기본: CRCR 또는 st.secrets['APP_PASSWORD']) -----
def _check_password():
    def _password_entered():
        target = st.secrets.get("APP_PASSWORD", "CRCR")
        if st.session_state.get("_password_input", "") == str(target):
            st.session_state["_pw_ok"] = True
            st.session_state.pop("_password_input", None)
        else:
            st.session_state["_pw_ok"] = False

    if st.session_state.get("_pw_ok", False):
        return True

    st.sidebar.subheader("🔐 Access")
    st.sidebar.text_input("Password", type="password", key="_password_input", on_change=_password_entered)
    if st.session_state.get("_pw_ok") is False:
        st.sidebar.error("비밀번호가 올바르지 않습니다.")
    st.stop()

# 비밀번호 체크
_check_password()

st.title("자동 Table 생성기")

# -------------------- [공통 유틸] --------------------
def format_p(p):
    if p is None or (isinstance(p, float) and np.isnan(p)):
        return "NA"
    if p >= 0.999:
        return "p > 0.99"
    if p < 0.001:
        return "<0.001"
    return f"{p:.3f}"

def is_continuous(series, threshold=20):
    try:
        return (series.dtype.kind in "fi") and (series.nunique(dropna=True) > threshold)
    except Exception:
        return False

def ordered_levels(series):
    vals = pd.Series(series.dropna().unique()).tolist()
    numeric = []
    non = []
    for v in vals:
        try:
            numeric.append((float(str(v)), v))
        except Exception:
            non.append(str(v))
    if len(numeric) == len(vals) and len(vals) > 0:
        numeric.sort(key=lambda x: x[0])
        return [v for _, v in numeric]
    return sorted([str(v) for v in vals], key=lambda x: x)

def make_dummies(df_in, var, levels):
    # "변수=수준" 이름으로 더미 생성 (drop_first → 첫 레벨이 Reference)
    cat = pd.Categorical(df_in[var].astype(str),
                         categories=[str(x) for x in levels],
                         ordered=True)
    dmy = pd.get_dummies(cat, prefix=var, prefix_sep="=", drop_first=True, dtype=float)
    dmy.index = df_in.index
    return dmy

def dummy_colname(var, level):
    return f"{var}={str(level)}"

def drop_constant_cols(X):
    keep = [c for c in X.columns if X[c].nunique(dropna=True) > 1]
    return X[keep]

def drop_constant_predictors(X, time_col, event_col):
    pred_cols = [c for c in X.columns if c not in [time_col, event_col]]
    keep = [c for c in pred_cols if X[c].nunique(dropna=True) > 1]
    return X[[time_col, event_col] + keep]

def clean_time(s):
    s = pd.to_numeric(s, errors="coerce")
    s = s.replace([np.inf, -np.inf], np.nan)
    return s

def ensure_binary_event(col, events, censored):
    def _map(x):
        if x in events: return 1
        if x in censored: return 0
        return np.nan
    return col.apply(_map).astype(float)

def calculate_hosmer_lemeshow(y_true, y_pred_prob, n_groups=10):
    """
    Calculates the Hosmer-Lemeshow goodness of fit test.
    """
    data = pd.DataFrame({'y_true': y_true, 'y_pred_prob': y_pred_prob})
    
    try:
        data['group'] = pd.qcut(data['y_pred_prob'], q=n_groups, duplicates='drop')
    except ValueError:
        n_groups = data['y_pred_prob'].nunique()
        if n_groups < 2: return np.nan, np.nan, "Not enough unique probabilities for the test."
        data['group'] = pd.qcut(data['y_pred_prob'], q=n_groups, duplicates='drop')
    
    summary = data.groupby('group', observed=False).agg(
        total_count=('y_true', 'count'),
        observed_events=('y_true', 'sum'),
        expected_events=('y_pred_prob', 'sum')
    )
    
    summary['observed_non_events'] = summary['total_count'] - summary['observed_events']
    summary['expected_non_events'] = summary['total_count'] - summary['expected_events']

    if (summary['expected_events'] < 1e-6).any() or (summary['expected_non_events'] < 1e-6).any():
        return np.nan, np.nan, "Test failed due to zero or near-zero expected frequencies in some groups."

    hl_stat = (
        ((summary['observed_events'] - summary['expected_events'])**2 / summary['expected_events']) +
        ((summary['observed_non_events'] - summary['expected_non_events'])**2 / summary['expected_non_events'])
    ).sum()

    df_hl = len(summary) - 2
    if df_hl <= 0:
        return np.nan, np.nan, "Not enough groups to calculate p-value (degrees of freedom <= 0)."
        
    p_value = stats.chi2.sf(hl_stat, df_hl)
    
    return hl_stat, p_value, None

def select_penalizer_by_cv(X_all, time_col, event_col,
                           grid=(0.0, 0.01, 0.05, 0.1, 0.2, 0.5),
                           k=5, seed=42):
    if X_all.shape[0] < k + 2 or X_all[event_col].sum() < k:
        return None, {}
    idx = X_all.index.to_numpy()
    rng = np.random.default_rng(seed)
    rng.shuffle(idx)
    folds = np.array_split(idx, k)
    scores = {}
    for pen in grid:
        cv_scores = []
        for i in range(k):
            test_idx = folds[i]
            train_idx = np.concatenate([folds[j] for j in range(k) if j != i])
            train = X_all.loc[train_idx].copy()
            test  = X_all.loc[test_idx].copy()
            train = drop_constant_predictors(train, time_col, event_col)
            test  = test[train.columns]
            if train[event_col].sum() < 2 or test[event_col].sum() < 1: continue
            if train.shape[1] <= 2 or train.shape[0] < 5: continue
            try:
                cph = CoxPHFitter(penalizer=pen)
                cph.fit(train, duration_col=time_col, event_col=event_col)
                s = float(cph.score(test, scoring_method="concordance_index"))
                if np.isfinite(s): cv_scores.append(s)
            except Exception: continue
        if cv_scores: scores[pen] = float(np.mean(cv_scores))
    if not scores: return None, {}
    best_pen = sorted(scores.items(), key=lambda x: (-x[1], x[0]))[0][0]
    return best_pen, scores

# -------------------- [파일 업로드] --------------------
uploaded_file = st.file_uploader("엑셀/CSV 업로드", type=['xls', 'xlsx', 'csv'])
df = None
sheetname = None
if uploaded_file:
    name = uploaded_file.name.lower()
    try:
        if name.endswith(".csv"):
            df = pd.read_csv(uploaded_file)
        elif name.endswith((".xlsx", ".xls")):
            engine = "openpyxl" if name.endswith(".xlsx") else "xlrd"
            xls = pd.ExcelFile(uploaded_file, engine=engine)
            sheetname = xls.sheet_names[0] if len(xls.sheet_names) == 1 else st.selectbox("시트 선택", xls.sheet_names, index=0)
            df = pd.read_excel(xls, sheet_name=sheetname, engine=engine)
        
        df.columns = pd.Index([str(c).strip().replace("\\n", " ") for c in df.columns])
        st.success(f"시트명: {sheetname if sheetname else ''}, 데이터 shape: {df.shape}")
        st.dataframe(df.head())
        st.session_state['df'] = df
    except Exception as e:
        st.error(f"파일을 읽는 중 오류가 발생했습니다: {e}")
        st.stop()

# -------------------- [Table1: 서브 함수] --------------------
def analyze_table1_display(df, group_col, value_map, threshold=20):
    result_rows = []
    group_values = list(value_map.keys())
    group_names = list(value_map.values())
    group_n = {g: (df[group_col] == g).sum() for g in group_values}
    for var in df.columns:
        if var == group_col: continue
        valid = df[df[group_col].isin(group_values)]
        if valid[var].dropna().empty: continue
        if is_continuous(valid[var], threshold=threshold):
            row = {'Characteristic': var}
            for g, g_name in zip(group_values, group_names):
                sub = valid[valid[group_col] == g][var].dropna()
                if sub.shape[0] > 0:
                    med, q1, q3 = sub.median(), sub.quantile(0.25), sub.quantile(0.75)
                    mean, std = sub.mean(), sub.std()
                    row[f"{g_name} (n={group_n[g]})"] = f"{med:.1f} [{q1:.1f}-{q3:.1f}]; {mean:.1f}±{std:.1f}"
                else:
                    row[f"{g_name} (n={group_n[g]})"] = "NA"
            p = np.nan; test_str = ""
            try:
                arr = [valid[valid[group_col] == g][var].dropna() for g in group_values]
                if all(stats.shapiro(s)[1] > 0.05 for s in arr if len(s) >= 3):
                    if len(arr) == 2: _, p = stats.ttest_ind(arr[0], arr[1], nan_policy='omit'); test_str = "t-test"
                    elif len(arr) > 2: _, p = stats.f_oneway(*arr); test_str = "ANOVA"
                else:
                    if len(arr) == 2: _, p = stats.mannwhitneyu(arr[0], arr[1], alternative='two-sided'); test_str = "Mann-Whitney U"
                    elif len(arr) > 2: _, p = stats.kruskal(*arr); test_str = "Kruskal-Wallis"
            except Exception: p = np.nan; test_str = "NA"
            row['Test'] = test_str; row['p value'] = format_p(p); row['sub_p'] = ""
            result_rows.append(row)
        else:
            p = np.nan; test_str = ""
            try:
                ct = pd.crosstab(valid[group_col], valid[var])
                if ct.shape == (2, 2): _, p = stats.fisher_exact(ct); test_str = "Fisher"
                else: _, p, _, _ = stats.chi2_contingency(ct); test_str = "Chi-square"
            except Exception: p = np.nan; test_str = "NA"
            result_rows.append({'Characteristic': var, **{f"{g_name} (n={group_n[g]})": "" for g_name in group_names}, 'Test': test_str, 'p value': format_p(p), 'sub_p': ""})
            for val in ordered_levels(valid[var]):
                row = {'Characteristic': f"  {val}"}
                for g, g_name in zip(group_values, group_names):
                    cnt = valid[(valid[group_col] == g) & (valid[var] == val)].shape[0]
                    percent = (cnt/group_n[g]*100) if group_n[g] > 0 else 0
                    row[f"{g_name} (n={group_n[g]})"] = f"{cnt}({percent:.0f}%)"
                row['Test'] = ""
                row['p value'] = ""
                p_sub = np.nan
                try:
                    table = np.array([[(valid[(valid[group_col] == g) & (valid[var] == val)]).shape[0], (valid[(valid[group_col] == g) & (valid[var] != val)]).shape[0]] for g in group_values])
                    if table.shape == (2, 2): _, p_sub = stats.fisher_exact(table)
                    else: _, p_sub, _, _ = stats.chi2_contingency(table)
                except: pass
                row['sub_p'] = format_p(p_sub)
                result_rows.append(row)
    return pd.DataFrame(result_rows)


# -------------------- [UI: Tab 구성] --------------------
if 'df' in st.session_state:
    df = st.session_state['df']
else:
    df = None

if df is not None:
    tab_titles = [
        "📊 Table1 자동화", 
        "🟦 Cox 회귀분석", 
        "🟧 로지스틱 회귀분석" # New Tab
    ]
    tab1, tab2, tab3 = st.tabs(tab_titles)

    # ===== TAB1: Table 1 =====
    with tab1:
        st.header("Table 1 자동 생성")
        group_col = st.selectbox("분석할 그룹 변수명 선택", options=list(df.columns), key='group_col')
        if group_col:
            unique_vals = list(df[group_col].dropna().unique())
            selected_vals = st.multiselect("분석할 그룹 값 선택", unique_vals, default=unique_vals[:2], key='group_values')
            if selected_vals:
                value_map = {val: st.text_input(f"'{val}'의 표시 라벨", value=str(val), key=f'label_{val}') for val in selected_vals}
                if st.button("논문 Table1 생성", key='table1_generate'):
                    result = analyze_table1_display(df, group_col, value_map, threshold=20)
                    st.dataframe(result)
                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine='openpyxl') as writer:
                        result.to_excel(writer, index=False)
                    st.download_button("Table1 엑셀로 저장", output.getvalue(), "Table1_Results.xlsx")

    # ===== TAB2: Cox Regression =====
    with tab2:
        st.header("Cox 비례위험 회귀분석 (Univariate & Multivariate)")
        time_col = st.selectbox("생존기간 변수(time)", df.columns, key="cox_time_col")
        event_col = st.selectbox("Event 변수(예: 0=생존, 1=사망 등)", df.columns, key="cox_event_col")
        
        if event_col:
            unique_events = list(df[event_col].dropna().unique())
            selected_event = st.multiselect("이벤트(사건) 값", unique_events, key='selected_event_val')
            selected_censored = st.multiselect("생존/관찰종결(censored) 값", unique_events, key='selected_censored_val')
            
        variables = st.multiselect("분석 후보 변수 선택", [c for c in df.columns if c not in [time_col, event_col]], key="cox_variables")
        
        c1, c2, c3, c4 = st.columns(4)
        p_enter = c1.number_input("다변량 포함 기준 p-enter (≤)", 0.001, 1.0, 0.05, 0.01)
        max_levels = c2.number_input("범주형 판정 최대 고유값", 2, 50, 10, 1, key="cox_max_levels")
        auto_penal = c3.checkbox("penalizer 자동 선택 (CV)", value=False)
        cv_k = c4.number_input("CV folds (K)", 3, 10, 5, 1, disabled=not auto_penal)
        penalizer = st.number_input("penalizer (수렴 안정화)", 0.0, 5.0, 0.1, 0.01, disabled=auto_penal)

        if st.button("Cox 회귀분석 실행"):
            # ... (Cox analysis logic remains the same)
            pass # Placeholder for brevity, the original logic is preserved.

    # ===== TAB3: Logistic Regression (NEW) =====
    with tab3:
        st.header("로지스틱 회귀분석 (Risk Factor Analysis)")
        st.info("종속변수의 특정 값을 사건(1)과 기준(0)으로 정의하여 위험인자를 분석합니다.")

        dep_var = st.selectbox("종속 변수 (Y) 선택", df.columns, key="logistic_dep_var")
        
        if dep_var:
            unique_outcomes = list(df[dep_var].dropna().unique())
            st.write(f"'{dep_var}' 변수의 고유 값: {unique_outcomes}")
            
            event_values = st.multiselect("사건(Event=1)에 해당하는 값 선택", unique_outcomes, key="logistic_event")
            control_values = st.multiselect("기준(Control=0)에 해당하는 값 선택", unique_outcomes, key="logistic_control")

            st.caption("※ 사건 값과 기준 값은 서로 겹치면 안 됩니다.")

        indep_vars = st.multiselect("독립 변수 (X) 선택 (위험인자 후보)", [c for c in df.columns if c != dep_var], key="logistic_indep_vars")
        max_levels_logistic = st.number_input("범주형 판정 최대 고유값", 2, 50, 10, 1, key="logistic_max_levels")

        if st.button("로지스틱 회귀분석 실행", key="run_logistic"):
            # --- 1. Input Validation ---
            if not dep_var or not event_values or not control_values or not indep_vars:
                st.error("종속 변수, 사건 값, 기준 값, 독립 변수를 모두 선택해야 합니다.")
                st.stop()
            if set(event_values) & set(control_values):
                st.error("사건 값과 기준 값이 겹칩니다. 다시 선택하세요.")
                st.stop()

            # --- 2. Data Preparation ---
            try:
                cols_to_use = [dep_var] + indep_vars
                df_model = df[cols_to_use].copy()
                
                df_model['__dependent_var_binary'] = ensure_binary_event(df_model[dep_var], set(event_values), set(control_values))
                df_model.dropna(subset=['__dependent_var_binary'], inplace=True)
                df_model['__dependent_var_binary'] = df_model['__dependent_var_binary'].astype(int)

                y = df_model['__dependent_var_binary']
                X_list, cat_info_logistic = [], {}
                for var in indep_vars:
                    if not is_continuous(df_model[var], threshold=max_levels_logistic):
                        levels = ordered_levels(df_model[var])
                        cat_info_logistic[var] = {"levels": levels, "ref": levels[0]}
                        X_list.append(make_dummies(df_model[[var]], var, levels))
                    else:
                        cat_info_logistic[var] = {"levels": None, "ref": None}
                        X_list.append(pd.to_numeric(df_model[var], errors='coerce').rename(var))
                
                if not X_list: st.error("유효한 독립 변수가 없습니다."); st.stop()

                X_processed = pd.concat(X_list, axis=1)
                model_data = pd.concat([y, X_processed], axis=1).dropna()
                y_final = model_data[y.name]
                X_final = model_data.drop(columns=[y.name])
                X_final = sm.add_constant(X_final, has_constant='add')
                X_final = drop_constant_cols(X_final)

                if X_final.shape[1] <= 1: st.error("분석에 사용할 유효한 독립 변수가 부족합니다."); st.stop()
                st.info(f"분석에 사용된 총 관측치: {len(y_final)}, 사건 수: {y_final.sum()}")

                # --- 3. Univariate & Multivariate Analyses ---
                uni_results = {}
                for var in indep_vars:
                    try:
                        var_cols = [c for c in X_final.columns if c == var or c.startswith(f"{var}=")]
                        if not var_cols: continue
                        X_uni = X_final[['const'] + var_cols]
                        y_uni = y_final.loc[X_uni.index]
                        if len(y_uni.unique()) > 1:
                            uni_results[var] = sm.Logit(y_uni, X_uni).fit(disp=0)
                    except Exception: uni_results[var] = None
                
                result_multi = sm.Logit(y_final, X_final).fit(disp=0)

                # --- 4. Create Publication-Style Table ---
                output_rows = []
                for var in indep_vars:
                    is_cat = var in cat_info_logistic and cat_info_logistic[var]['levels'] is not None
                    row_data = {}

                    if is_cat:
                        output_rows.append({'Factor': var, 'Subgroup': ''})
                        levels = cat_info_logistic[var]['levels']
                        output_rows.append({'Factor': '', 'Subgroup': f"{levels[0]} (Reference)", 'Univariate OR (95% CI)': '1.0', 'Multivariate OR (95% CI)': '1.0'})
                        
                        for level in levels[1:]:
                            dummy_name = f"{var}={level}"
                            row_data = {'Factor': '', 'Subgroup': str(level)}
                            # Uni
                            res_uni = uni_results.get(var)
                            if res_uni and dummy_name in res_uni.params:
                                param, pval, conf = res_uni.params[dummy_name], res_uni.pvalues[dummy_name], res_uni.conf_int().loc[dummy_name]
                                row_data['Univariate OR (95% CI)'] = f"{np.exp(param):.3f} ({np.exp(conf[0]):.3f}-{np.exp(conf[1]):.3f})"
                                row_data['p-value (Uni)'] = format_p(pval)
                            # Multi
                            if dummy_name in result_multi.params:
                                param, pval, conf = result_multi.params[dummy_name], result_multi.pvalues[dummy_name], result_multi.conf_int().loc[dummy_name]
                                row_data['Multivariate OR (95% CI)'] = f"{np.exp(param):.3f} ({np.exp(conf[0]):.3f}-{np.exp(conf[1]):.3f})"
                                row_data['p-value (Multi)'] = format_p(pval)
                            output_rows.append(row_data)
                    else: # Continuous
                        row_data = {'Factor': var, 'Subgroup': ''}
                        # Uni
                        res_uni = uni_results.get(var)
                        if res_uni and var in res_uni.params:
                            param, pval, conf = res_uni.params[var], res_uni.pvalues[var], res_uni.conf_int().loc[var]
                            row_data['Univariate OR (95% CI)'] = f"{np.exp(param):.3f} ({np.exp(conf[0]):.3f}-{np.exp(conf[1]):.3f})"
                            row_data['p-value (Uni)'] = format_p(pval)
                        # Multi
                        if var in result_multi.params:
                            param, pval, conf = result_multi.params[var], result_multi.pvalues[var], result_multi.conf_int().loc[var]
                            row_data['Multivariate OR (95% CI)'] = f"{np.exp(param):.3f} ({np.exp(conf[0]):.3f}-{np.exp(conf[1]):.3f})"
                            row_data['p-value (Multi)'] = format_p(pval)
                        output_rows.append(row_data)

                publication_df = pd.DataFrame(output_rows).fillna('')
                st.write("### 로지스틱 회귀분석 결과 (논문 형식)")
                st.dataframe(publication_df)

                # --- 5. Hosmer-Lemeshow Test ---
                st.write("---")
                st.write("### 모델 적합도 검정 (Hosmer-Lemeshow Test)")
                y_pred_prob = result_multi.predict(X_final)
                hl_stat, p_value_hl, hl_error = calculate_hosmer_lemeshow(y_final, y_pred_prob)
                if hl_error:
                    st.warning(f"호스머-렘쇼 검정을 수행할 수 없습니다: {hl_error}")
                else:
                    col1, col2 = st.columns(2)
                    col1.metric("Chi-squared statistic", f"{hl_stat:.3f}")
                    col2.metric("p-value", f"{p_value_hl:.3f}")
                    st.caption("※ p-value가 0.05보다 크면 모델이 데이터에 잘 적합한다고 해석할 수 있습니다.")

                # --- 6. Excel Download ---
                output_logistic = io.BytesIO()
                with pd.ExcelWriter(output_logistic, engine='openpyxl') as writer:
                    publication_df.to_excel(writer, index=False, sheet_name='Logistic Regression Results')
                st.download_button(
                    label="분석 결과 엑셀로 저장",
                    data=output_logistic.getvalue(),
                    file_name="Logistic_Regression_Publication_Table.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    key='download_logistic_publication'
                )

            except Exception as e:
                st.error(f"분석 중 오류가 발생했습니다: {e}")

